<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>butterfly主题</title>
      <link href="/2023/08/24/butterfly/"/>
      <url>/2023/08/24/butterfly/</url>
      
        <content type="html"><![CDATA[<h2 id="基本配置">基本配置</h2><h3 id="安装主题">安装主题</h3><p><code>npm install hexo-theme-butterfly</code></p><h3 id="应用主题">应用主题</h3><p>修改 Hexo 根目录下的 _config.yml，把主題改为butterfly<br><code>theme: butterfly</code></p><h3 id="安装插件">安装插件</h3><p>如果沒有 pug 以及 stylus 的渲染器，請下載安裝：<br><code>npm install hexo-renderer-pug hexo-renderer-stylus --save</code></p><h3 id="升级建议">升级建议</h3><p>在 hexo 的根目錄創建一個文件 <code>_config.butterfly.yml</code>，並把主題目錄的 <code>_config.yml</code> 內容複製到 <code>_config.butterfly.yml</code> 去。 注意: 複製的是主題的 <code>_config.yml</code> ，而不是 hexo 的 <code>_config.yml</code></p><h2 id="页面配置">页面配置</h2><h3 id="标签页">标签页</h3><p>进入根目录，输入如下命令<br><code>hexo new page tags</code><br>于是得到了文件<code>source/tags/index.md</code><br>添加<code>type: &quot;tags&quot;</code></p><h3 id="分类页">分类页</h3><p>进入根目录，输入如下命令<br><code>hexo new page categories</code><br>于是得到了文件<code>source/categories/index.md</code><br>添加<code>type: &quot;categories&quot;</code></p><h3 id="友情链接">友情链接</h3><p>进入根目录，输入如下命令<br><code>hexo new page link</code><br>于是得到了文件<code>source/link/index.md</code><br>添加<code>type: &quot;link&quot;</code><br>在Hexo博客目錄中的 <code>source/_data</code>（如果沒有 _data 文件夾，請自行創建），創建一個文件 <code>link.yml</code></p><h3 id="404页面">404页面</h3><p>主題內置了一個簡單的 404 頁面，可在設置中開啟<br>在主题的配置页面中找到如下配置：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">error<span class="emphasis">_404:</span></span><br><span class="line"><span class="emphasis">  enable: false</span></span><br><span class="line"><span class="emphasis">  subtitle: &quot;頁面沒有找到&quot;</span></span><br><span class="line"><span class="emphasis">  background: </span></span><br></pre></td></tr></table></figure><p>将<code>false</code>改为<code>true</code>即可</p><h2 id="网站配置">网站配置</h2><h3 id="导航栏配置如下：">导航栏配置如下：</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">menu:</span><br><span class="line">  主页: / || fas fa-home</span><br><span class="line">  归档: /archives/ || fas fa-archive</span><br><span class="line">  标签: /tags/ || fas fa-tags</span><br><span class="line">  分类: /categories/ || fas fa-folder-open</span><br><span class="line">  链接||fas fa-list:</span><br><span class="line"><span class="code">    友情链接: /link/ || fas fa-link</span></span><br><span class="line"><span class="code">    关于: /about/ || fas fa-heart</span></span><br><span class="line"><span class="code">    音乐: /music/ || fas fa-music</span></span><br><span class="line"><span class="code">    视频: /movies/ || fas fa-video</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello Hexo</title>
      <link href="/2023/08/23/hello-world/"/>
      <url>/2023/08/23/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Hexo博客搭建">Hexo博客搭建</h2><h3 id="Hexo安装">Hexo安装</h3><p><code>npm install hexo-cli -g</code></p><h3 id="初始化博客">初始化博客</h3><p><code>hexo init [文件夹名字]</code></p><h3 id="安装依赖">安装依赖</h3><p><code>npm install</code></p><h3 id="预览">预览</h3><p><code>hexo s</code></p><h2 id="部署到github">部署到github</h2><h3 id="配置git">配置git</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global user.name <span class="string">&quot;chenyv118&quot;</span></span><br><span class="line">git config --global user.email <span class="string">&quot;3241218997@qq.com&quot;</span></span><br></pre></td></tr></table></figure><h3 id="本地配置">本地配置</h3><p>打开项目根目录中的_config.yml文件，找到deploy</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  <span class="built_in">type</span>: <span class="string">&#x27;git&#x27;</span></span><br><span class="line">  <span class="comment">### 使用https就可以不用配置ssh密钥</span></span><br><span class="line">  repo: <span class="string">&#x27;https://github.com/chenyv118/chenyv118.github.io.git&#x27;</span></span><br><span class="line">  <span class="comment">### repo: &#x27;git@github.com:chenyv118/chenyv118.github.io.git&#x27;</span></span><br><span class="line">  branch: main</span><br></pre></td></tr></table></figure><p>右下角可以切换换行符为<code>CRLF</code>或<code>LF</code></p><h3 id="安装部署插件">安装部署插件</h3><p><code>npm install hexo-deployer-git --save</code></p><h2 id="设置模板">设置模板</h2><h3 id="安装">安装</h3><p><code>npm install hexo-butterfly-cli -g</code></p><h3 id="使用">使用</h3><p><code>butterfly-cli init [projectName]</code></p><h3 id="安装依赖-2">安装依赖</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd [projectName]</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><h2 id="Butterfly主题">Butterfly主题</h2><h3 id="安装主题">安装主题</h3><p><code>npm install hexo-theme-butterfly</code></p><h3 id="应用主题">应用主题</h3><p>修改 Hexo 根目录下的 _config.yml，把主題改为butterfly<br><code>theme: butterfly</code></p><h3 id="安装插件">安装插件</h3><p>如果沒有 pug 以及 stylus 的渲染器，請下載安裝：<br><code>npm install hexo-renderer-pug hexo-renderer-stylus --save</code></p><h3 id="升级建议">升级建议</h3><p>在 hexo 的根目錄創建一個文件 _config.butterfly.yml，並把主題目錄的 _config.yml 內容複製到 _config.butterfly.yml 去。( 注意: 複製的是主題的 _config.yml ，而不是 hexo 的 _config.yml</p><h2 id="版本控制">版本控制</h2><p>备份博客，便于电脑出故障也能够保证博客不会丢失。</p><h3 id="git初始化">git初始化</h3><p><code>git init</code></p><h3 id="远程推送">远程推送</h3><p>把所有文件添加到本地列表<br><code>git add .</code><br>提交到本地仓库<br><code>git commit -m &quot;提交名称&quot;</code><br>推送到远程<br><code>git branch -M main</code><br><code>git remote add origin https://github.com/chenyv118/hexo-blog.git</code><br><code>git push -u origin main</code></p><h3 id="仓库恢复">仓库恢复</h3><p>本地丢失项目后，可在github上使用<code>git clone</code>恢复，如：<br><code>git clone https://github.com/chenyv118/hexo-blog.git</code><br>再安装依赖<br><code>npm install</code></p><h2 id="Quick-Start">Quick Start</h2><h3 id="Create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>NTK-PINN</title>
      <link href="/2023/08/21/NTK-PINN/"/>
      <url>/2023/08/21/NTK-PINN/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>权重调整</title>
      <link href="/2023/08/09/WeightAdapt/"/>
      <url>/2023/08/09/WeightAdapt/</url>
      
        <content type="html"><![CDATA[<h1>多目标优化（MOO）的权重调整</h1><p>不同损失项之间的不平衡引起边界条件中的梯度刚性。Multi-Objective optimisation，多目标优化，是指在优化过程中需要同时优化多个目标函数，在PINN中，就是指有监督（Data_loss和BC_loss）和无监督（PDE_Loss）等不同目标的优化。这些目标函数可能是相互矛盾的，因此需要在它们之间进行权衡和平衡，以找到最优解。在多目标优化中，通常会得到一组Pareto最优解，这些解表示在给定的目标函数之间的最优权衡。</p><h2 id="SoftAdapt（基于损失统计信息）">SoftAdapt（基于损失统计信息）</h2><p>SoftAdapt与GradNorm类似，也利用了相对进展等式（在这里）来平衡损失项，不过作者只考虑了前一个时间步$\mathcal{L}_i(t)$，并用时间步之间的差值来替代除法，从而放松了这一限制，然后用softmax对权重参数进行归一化处理：<br>$$<br>\lambda_i(t)=\frac{\exp(\mathcal{T}(\mathcal{L}_i(t)-\mathcal{L}<em>i(t-1)))}{\sum</em>{j=1}^k\exp(\mathcal{T}(\mathcal{L}_j(t)-\mathcal{L}_j(t-1)))}，i\in{1,…k}<br>$$<br>其中$L_i(t)$是第$i$项在步骤$t$的损失，SoftAdapt与GradNorm的主要区别在于它不需要梯度统计，因此无需对每个目标进行单独的反向传播，相反它利用了梯度的大小直接取决于损失函数中项的大小这一事实，因此只需通过损失统计来实现平衡。令$\mathcal{T}(\mathcal{L}_i(t)-\mathcal{L}_i(t-1))=\beta s_i$，其中$\beta$为一个超参数，$\beta&gt;0$时2将会为损失中表现最差（最大变化率）的项分配更大的权重，$\beta&lt;0$时有利于表现最佳的损失（最负的变化率）；$s_i$为损失变化的梯度。</p><p>重点从SoftAdapt展开来看，SoftAdapt有三种变体：</p><ol><li>Original Variant：用softmax计算第$i$个周期的第$k$个损失项的损失权重：$\alpha_k^i=\frac{\exp{\beta s_k^i}}{\sum_{j=1}^n \exp{\beta s_k^i}}$</li><li>Loss Weights:将损失值$f_k^i$加入计算中：$\alpha_k^i=\frac{\exp{f_k^i \beta s_k^i}}{\sum_{j=1}^n \exp{f_k^i \beta s_k^i}}$</li><li>在计算前将$\sum_{j=1}^ns_j^i$归一化。</li></ol><p>进行1w次训练的运行结果：</p><ol><li>Origin<img src="WeightAdapt/Origin.png" alt="Oringin"></li><li>SoftAdapt<img src="WeightAdapt/SoftAdapt_0.1.png" alt="Origin"></li><li>LossWeightsSoftAdapt<img src="./WeightAdapt/LossWeight_SoftAdapt_0.1.png" alt="LossWeights"></li><li>NormalizedSoftAdapt<img src="./WeightAdapt/Normalize_softAdapt_0.1.png" alt="Normalized"></li></ol><p>在估计参数方面，三种方法的表现都差不多，都不太行，跟不加没什么区别。</p><p>在模型预测方面：</p><ol><li>SoftAapt可能有些提升，但是提升不多。调试发现，最初训练时损失权重会有一些变化，然而很快就会收敛到每一项损失权重都相等，对于softAdapt而言可能是因为损失的变化梯度数量级过小以至于经过softmax处理后结果非常接近。</li><li>Normalize方法结果与SoftAdapt近似，Normalize方法会将$s_k$归一化，理论上权重的最大差别也仅仅只相差了2.7倍，受超参数$beta=0.1$的影响，权重的差距更加小，对于数量级相差较大的损失项来说意义不大（私以为）。尝试调整超参数$beta$,放大数量级后,取$beta=1,-1,10,-10$，仅$beta=-10$的物理模型有所好转，如下：<img src="./WeightAdapt/Normalized-10.png" alt="Normalized">(因为不喜欢在边界处由于仿真和观察值不同导致的突变所以把可观测变量的边界条件去掉了，导致边界处有状态变量乱走跑到负值，应该无伤大雅)</li><li>LossWeight方法的表现很差，是因为使用LossWeight方法会将损失值也作为权重的一部分乘上，这导致了数量级大的损失项的权重更大，数量级小的损失项的权重更小，反而加重了损失计算的不平衡。</li></ol><p>固定部分参数防止多解实验：<br>从论文中提到的不需要重点估计的部分参数中选择，结合LSA曲线选择较为平缓的参数：7，-6，-10，4，对应matlab中的15，10，18，5。<br>为了防止去掉参数后再次导致不满秩的问题，需要看移除参数后是否会有式子中没有可估计参数。<br>参数15影响式子10，13，16；<br>参数10影响式子9，13；<br>参数18影响式子8，9，14；<br>参数5影响式子2，9。<br>共影响式子：2，8，9，10，13，14，16</p><p>式子2中有参数3，4，5，6，安全；<br>式子8中有参数13，14，17，18，安全<br>式子9中有参数1，2，5，6等，安全<br>式子10中有参数3，4，13，14等，安全<br>式子13中有参数9，10，15，16，安全<br>式子14中有参数17，18，安全<br>式子16中有参数15，11，12，安全</p><h2 id="GradNorm-基于梯度统计信息">GradNorm(基于梯度统计信息)</h2><p>所有项相对于初始损失的改善率相同，并有一个单独的优化器执行，与其他项相比，一个项从训练开始时就以更高的速度提升了，那么它就会得到一个更弱的缩放，直到所有项都取得相同的进步。<br>$$<br>\mathcal{L}(t;\lambda)=\sum_{i=1}^k|G_\theta^{(i)}(t)-\bar{G}<em>\theta(t)\times[r_i(t)]^\alpha|<em>1<br>$$<br>其中$G</em>\theta^{(t)}=|\nabla</em>\theta\lambda_i\mathcal{L}<em>i(t)|<em>2$是目标$i\in{1,…k}$的缩放损失的网络参数的梯度$L_2$范式，$\bar{G}</em>\theta(t)=\frac1k\sum</em>{i=1}^kG_\theta^(i)(t)$梯度标准的平均值，$r_i(t)=\mathcal{L}<em>i(t)/(\mathcal{L}<em>i(0)\cdot\mathcal{L}(t))$定义了第$i$个周期截至目前的改进速度。$\alpha$是一个代表将任务拉回共同训练速率的恢复力的强度的超参数。$\bar{G}</em>\theta(t)\times[r_i(t)]^\alpha$是$G</em>\theta^(i)(t)$的理想值，因此必须防止梯度流过这个表达式。用于更新网络参数的最终损失函数是使用之前更新的权重参数进行线性标量化的结果（即权重参数$\lambda$用于给损失项加权进行优化）：<br>$$<br>\mathcal{L}(t;\theta)=\sum_{i=1}^k\lambda_i(t)\mathcal{L}_i(t)<br>$$<br>尽管它解决了学习率退火法的一些问题，但是它仍然需要对每个任务单独进行向后处理，k越大时它的成本就会越高。此外它在每一步都依赖于两轮单独的优化：一轮用于调整权重参数$\lambda_i$，一轮用于更新权重$\theta$。GradNorm可以通过标量化多目标优化来表示：s<br>$$<br>\mathcal{L}(t)=(\mathcal{L}(t;\theta),\mathcal{L}(t,;\lambda))^T<br>$$<br>这又需要对超参数，如之前提到的学习率和初始化等进行经验性调整以保持系统平衡，这正是我们试图通过自适应损失平衡方案来解决的问题。</p><h2 id="Learning-Rate-Annealing">Learning Rate Annealing</h2><p>使用梯度统计来自适应缩放损失。公式如下：<br>$$<br>\hat{\lambda}<em>i(t)=\frac{\max {|\nabla</em>\theta \mathcal{L}<em>\Omega(t)|}}{\overline{|\nabla</em>\theta\mathcal{L}<em>{(\Gamma,\Upsilon)<em>i}(t)|}},i\in{1,…k}\<br>\lambda_i(t)=\alpha\lambda_i(t-1)+(1-\alpha)\hat{\lambda}<em>i(t)\<br>\theta^{(t+1)}=\theta-\eta\nabla</em>\theta\left(\mathcal{L</em>{\Omega}(t)+\sum</em>{i=1}^k\lambda_i(t)\mathcal{L}<em>{(\Gamma,\Upsilon)<em>i}(t)}\right)<br>$$<br>其中 $\overline{|\nabla</em>\theta\mathcal{L}</em>{\Gamma_i}(t)|}$ 是关于参数 $\theta$的梯度的均值, $\alpha$是一个超参数，作者建议改为0.9。<br>无论何时 $|\nabla_\theta\mathcal{L}<em>\Omega(t)|$ 的最大值比 $|\nabla</em>\theta\mathcal{L}_{(\Gamma,\Upsilon)_i}(t)|$ 的平均值大得多的时候，缩放 $\lambda_i(t)$ 校正该差异，使得所有梯度具有相似的幅度。此外没使用指数衰减一边平滑平衡并且避免优化步骤之间的损耗空间的剧烈变化。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>tips-GAN</title>
      <link href="/2023/06/15/tips-GAN/"/>
      <url>/2023/06/15/tips-GAN/</url>
      
        <content type="html"><![CDATA[<h2 id="1-GAN">1. GAN</h2>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>PINN日志</title>
      <link href="/2023/04/20/biodiesel_log/"/>
      <url>/2023/04/20/biodiesel_log/</url>
      
        <content type="html"><![CDATA[<h2 id="biodiesel-PINN代码部分">biodiesel-PINN代码部分</h2><h3 id="特征层">特征层</h3><p>LuLu的代码中添加了特征层，特征层的作用是提取数据的随时间变化的曲线特征，具体是$e^t$或$\sin(nt)$，但是如果在数据的曲线上看不到这样的特征，又该如何选择呢？<br>这里如果将LuLu在特征层添加数学公式的行为认定为手工设计的特征,那么想要提取随时间变化的特征，或者说时序特征，为什么不将特征层直接设计为Cov层或LSTM层呢？不过这种层的训练方式和DeepXDE的训练方式不一样，添加进去可能比较麻烦，甚至需要重新设计训练方式。</p><h3 id="代码行为">代码行为</h3><p>为什么k全部都要初始化为0？不是应该随机初始化的吗？k在PINN中是怎么存在的呢？</p><p>训练前可以有预训练部分，需要将ode_weight置为0，先训练data，可以更好找到数据的锚点。</p><p>callback:监控神经网络的训练过程，然后实时进行修改，例如改变学习率，通常是一个很好的策略。</p><h3 id="代码问题日志">代码问题日志</h3><p>conda安装后环境变量不生效显示conda命令不存在，需要重启电脑。<br>conda安装后需要运行conda config才能有.condarc文件，以此才能配置镜像。</p><p><strong>错误：</strong><br>ODEintWarning: Excess work done on this call (perhaps wrong Dfun type). Run with full_output = 1 to get quantitative information.<br><strong>已解决:</strong><br>检查后发现模型内的参数错误。</p><p><strong>错误：</strong><br>2023/02/12  22:36<br>损失函数出现nan。<br>校对了模型参数，没有发现问题。<br>数据集中也没有nan值。<br>有一点奇怪：体积从没有超过$Vreactf$，检查发现数据y出现负值。<br>在酵解模型中，y也出现负值，可能不是负值的问题。<br>酵解模型的torch代码有问题，葡萄糖模型不会出现负值。<br><strong>参考:</strong><br>葡萄糖SBINN的ODE中：t[0]=665,对应y[0]=[41.1668,71.4790,10846.3035,18.4421,18.3821,18.4204],数据集中y[665]=[104.46920566,   151.28505095,14260.69246984,68.36861391,59.52390049,61.16265854]，数据也不相等，但是不同，可能做了预处理？不过肯定不会出现负值。<br><strong>注意:</strong><br>如果变量在神经网络中会改变,那么模型里对于$V&lt;Vreactf$的判断就不准确了。<br>预训练的时候就已经出现了nan,说明与ODE模型无关。<br>预训练的时候OED损失的权值置0，却依然出现了nan，还是ODE模型的问题。<br>2023/02/27 18:33<br><strong>发现原因：</strong><br>deepxde中的sbinn里的ODE函数中的参数t与y的含义与ode_func中的不同，ode_func中的y是数据集的y，生成的数据用于data_loss；ODE函数返回的是神经网络的损失项中的ODE_loss，其中神经网络中的ODE损失是无监督学习，其返回的损失值由AutoGrad与神经网络forward的结果进行loss_function。神经网络在最开始拟合的时候状态变量可能会出现负值，而系统中的状态变量全为正值，拟合的过程中可能会经过0，系统中存在对状态变量的除法，这时候就会出现nan。<br><strong>解决方案：</strong></p><ol><li>改变初始化方法，使得权重矩阵中的值全为正值。<ol><li>Deepxde中没有对相关初始化方法如Kaiming、Orthoogonal等进行封装，需要自己实现。</li><li>限制了神经网络从负数域中拟合函数的可能性。</li></ol></li><li>更改模型，采用优化前的无除号的模型。<br>最终采用方案2。</li></ol><p><strong>注意：</strong><br>PINN中尽量避免状态变量的除法！！！</p><p><strong>错误：</strong><br>参数估计总出现负值。</p><p><strong>分析：</strong><br>代码输出的结果为负值并不是什么问题，因为代码输出的结果是模型中的Variable值，而Variable并不是真正的参数值，模型中的Variable经过非线性层放缩后得到的参数值才是真正的参数值，因此需要将Variable值转换为参数值，然后再进行分析。<br>在酵解模型中，LuLu的legacy代码中利用指数变换将参数值放缩到$(0,+\infty)$,而在deepxde版本的代码中，是利用softplus函数放缩参数，保证Variable为负时参数接近0，variable为正时参数接近variable本身。<br>而在分析葡萄糖代码时，LuLu将参数值限制在置信区间内。<br>我研究的生物柴油系统的参数的置信区间如下：<br><img src="./biodiesel_log/confidence_interval.png"><br><strong>注意：</strong> 图中k10的数量级为-2而不是2</p><p><strong>问题：</strong><br>LSA中由于扩展了状态变量，从17扩展到357，出现了显存不足的情况，即在dde.jacobian时</p><h3 id="代码运行结果">代码运行结果</h3><h4 id="参数估计部分">参数估计部分</h4><p>loss_history:<br><img src="./biodiesel_log/loss.png"></p><p>预期效果：<br><img src="./biodiesel_log/fact.png"/></p><p>实际效果：<br><img src="./biodiesel_log/prediction.jpg"/></p><p>去掉DirichletBC：<br><img src="./biodiesel_log/prediction2.png"></p><p>加入特征层：<br>全$\exp^{\lambda t}$特征层:<br><img src="./biodiesel_log/pred_feature_e.jpg"><br>$\exp^{\lambda t}+\sin{\lambda t}$<br><img src="./biodiesel_log/pred_feature_e&sin.jpg"></p><p>在边界上全部收敛为一个值，一定是边界条件出现问题，因此需要修改边界条件：</p><img src="./biodiesel_log/AdjustBC.png"><p>调整loss_weight:<br>方法1：loss内部权重调整：<br><img src="./biodiesel_log/lossWeight.png"><br>方法2：loss之间权重调整：</p><p><a href="https://chenyu.sochiji.tech:35424/s/rNSR">代码备份</a></p><p>加入噪声（weight初始化为全1）：<br><img src="./biodiesel_log/pred_noise&no_weight.jpg"><br>有两个状态变量拟合效果很差，推断应该是由于ode_weight不够大，因此无法观测到的数据不能被拟合。</p><p>加入噪声对loss之间权重进行调整：<br><img src="./biodiesel_log/pred_noise&weight.png"><br>加入std=0.1的噪声后，由于噪声较大，因此将data_weight的权重降低一个数量级，将ode_weight和bc_weight的权重提高一个数量级，使得data_loss对loss的影响较小，使得模型更加关注于ODE的拟合。</p><p><strong>分析</strong>：结果并不理想，ode的weight相比于data的weight足足大了两个数量级，可能导致对于每一个局部可以保证ode正确，而整体的data是有问题的。</p><p>在此基础上将所有loss_weight重新置为1训练<br><img src="./biodiesel_log/pred_noise&weight&no_weight.png"></p><p><strong>思考</strong>：进行ode_weight的选择时选择的策略是先预训练1000次，再根据训练得到的损失均衡数量级，然而由于初始条件的随机性，每次预训练得到的数量级差别很大，可能不具备参考性，那么是否我们应该考虑从数值层面对考虑数量级的均衡？<br>再者，如果一个状态变量的底数部分为9,另一个状态变量的底数部分为1，那么在平衡数量级时，指数部分相同也会导致数据相差几乎一个数量级，是否考虑将数据归一化？</p><p>模型拟合后，对于未观测到的状态变量，许多无突变的量也会出现突变，对比原模型模型刚开始部分曲线有垂直突变，部分曲线平滑，模型是否有问题？<br>模型没有问题，是因为刚性的原因。</p><p><strong>参数缩放：</strong><br>参数估计最终目的是为了估计出正确的参数，然而从全范围中搜索将会使得参数难以收敛在置信区间内，因此需要对参数进行缩放，使得参数在置信区间内，然后再进行参数估计。</p><p>权值置为$[10]*17+[1]*22$训练18w次：<br><img src="./biodiesel_log/tmp.png"><br>权值置为$[100]*17+[1]*22$继续训练2w次：<br><img src="./biodiesel_log/tmp3.png"><br>权值置为$[1000]*17+[1]*22$训练2w次：<br><img src="./biodiesel_log/tmp4.png"><br>权值重新置为$[100]<em>17</em>22$训练2w次：<br><img src="./biodiesel_log/tmp5.png"><br>继续训练10w次：<br><img src="./biodiesel_log/tmp6.png"></p><p>放缩variables区间后保存的模型保存在<a href="https://chenyu.sochiji.tech:35424/s/PECd">此链接</a></p><p>参数估计的结果：<br>[-0.3067, 8.0753, -2.2338, -0.0038, -5.3847, -6.0993, 5.6386, 8.5027, 8.2021, -0.0150, -1.0453, 8.6405, -2.3672, 0.6468, 0.4204, -6.3345, -0.9533, -8.6001, -6.7509, 0.0099]<br>重新放缩：<br>[4.20631328125, 4.6472487449646, 1.96, 1.63000009765625, 1.5503068, 1.7566206, 3.412207, 1.4759207, 1.3600091, 1.950825, 8.30996328125, 4.14001375, 9.409943, 4.632402420043945, 2.8400001953125, 2.66, 2.85, 1.66000109375, 3.374575823545456, 4.0750482003204525]</p><p>按照这样的趋势训练50w次：<br><img src="./biodiesel_log/tmp7.png"></p><p>采用估计出的参数重新仿真：<br><img src="./biodiesel_log/fact.png"></p><p>这样的结果是不行的，首先网络中就没有完全拟合，然而重新仿真的结果却和原模型完全没有区别，这是因为在拿结果推过程，之前的将参数缩放在置信区间内的操作是不允许的。</p><p>将参数初始化为已知参数,训练25w次（在24w次时收敛）：<br><img src="./biodiesel_log/init_variable_as_k.png"></p><p>将估计出的参数重新带回模型仿真。<br><img src="./biodiesel_log/odefunc.png"></p><p>模型完全不一样，但是对比葡萄糖模型的结果：<br><img src="./biodiesel_log/glucose.png"></p><p>葡萄糖模型带回去仿真模型也不一样，直接采用目前的参数做LSA和不确定也可，因为模型不再是ODE模型而是数据驱动的网络代理模型。</p><p>固定参数训练10w次，再训练50w次：<br><img src="./biodiesel_log/freeze-50w.png"><br>96w次：<br><img src="./biodiesel_log/freeze-96w.png"></p><p>用各种方法训练得到的参数不同，然而将它们带回去仿真，结果的形式却非常相像：</p><img src="./biodiesel_log/compare.png"><p>这是由于参数之间的相关性很大，从而使得一个参数的影响可以由其他参数的改变来弥补。</p><p>当我们设置权重时，考虑到ODE权重和数据权重分别代表网络的不同侧重点，因此可能导致估计出的参数在代理模型即神经网络中可以正确地拟合状态变量，而带回ODE物理模型中却无法出现，我们尝试不同的权重，代理模型的预测结果以及将其重新代回物理模型的结果如下：</p><p>ode_weight:data_weight=100:1<br><img src="./biodiesel_log/pred1e2.png"><br><img src="./biodiesel_log/ode1e2.png"></p><p>ode_weight:data_weight=10:1</p><img src="./biodiesel_log/pred1e1.png"><img src="./biodiesel_log/ode1e1.png"><p>ode_weight:data_weight=1:1</p><img src="./biodiesel_log/pred1e0.png"><img src="./biodiesel_log/ode1e0.png"><p>ode_weight:data_weight=1:10</p><img src="./biodiesel_log/pred1e-1.png"><img src="./biodiesel_log/ode1e-1.png"><p>ode_weight:data_weight=1:100<br><img src="./biodiesel_log/pred1e-2.png"><br><img src="./biodiesel_log/ode1e-2.png"></p><p>代理模型中，数据权重越大，抖动现象越明显，说明受噪声影响越严重，而ODE权重越大，曲线越平滑，然而曲线整体趋势并不正确。<br>物理模型中，并不能看出明显的差异。</p><p><strong>意外结果:</strong><br>参数值全为1，训练10w次：<br><img src="./biodiesel_log/accident_10w.png"><br>代理模型完全无法拟合而物理模型出奇地好。<br>var_list(k_)：<br>4.6280, 1.3338, 1.5049, 3.0367, 0.1557, 9.0786, 5.7904, 1.8945, 2.1055, 2.9502, 6.6698, 0.6709, 1.8122, 0.1182, 1.5712, 4.4811, 4.9589, 2.8555, 1.8663, 5.5000<br>参数值：<br>46377.27, 1567665.1, 17054212.0, 3083577.0, 7740244.5, 90787.14, 5793452.5, 20346.041, 22204.209, 0.03001206, 6.6710677, 10838.359, 19634560.0, 7539925.5, 175999.69, 449235.72, 4.965896, 2911433.2, 201013.64, 0.00055041</p><p>继续训练30w次：<br><img src="./biodiesel_log/accident_40w.png"><br>var_list(k_)：<br>3.8527, -0.8101, -4.2528, 1.7563, -6.9256, 7.3110, -3.9997, 0.8384, 0.4251, 2.4893, 6.6971, -3.7184, 0.1844, -6.2049, 0.4461, -3.0726, 4.6793, 0.2734, 0.5878, 5.8350<br>参数值：<br>38737.004, 367980.3, 141241.34, 1915594.0, 9818.313, 73116.68, 18155.324, 11977.526, 9281.179, 0.02569005, 6.6983337, 239.82874, 7895916.0, 20174.744, 94086.92, 4526.0728, 4.6885424, 839161.7, 102962.805, 0.00058379<br>损失变化如下：<br><img src="./biodiesel_log/accident_loss.png"><br>这非常奇怪，只能归结为PINN的随机性太大了。</p><p>用LuLu的方法调ode_weight：<br>其他权重调过的基础上ode_weight全1：<br>代理模型：<br><img src="./biodiesel_log/ode_without_adjust_proxy.png"><br>物理模型：<br><img src="./biodiesel_log/ode_without_adjust_physical.png"><br>物理模型对比标称模型：<br><img src="./biodiesel_log/ode_without_adjust_compare_standard.png"></p><p>得到ode_loss：<br>[8.30e-07, 5.26e-06, 1.04e-06, 9.35e-07, 4.25e-06, 2.93e-07, 9.29e-07, 4.83e-07, 3.00e-07, 2.80e-06, 2.59e-06, 1.39e-07, 8.62e-08, 1.98e-18, 1.31e-08, 1.25e-09, 3.98e-06]<br>以及data_loss:<br>[4.62e-05, 5.33e-04, 1.24e-04, 5.58e-05, 3.00e-04]<br>根据数量级差异得到的ode_weight:<br>[1e2, 1e1, 1e1, 1e2, 1e1, 1e2, 1e2, 1e2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1e4, 1e1]<br>进行下一步，将少反应量的物质权重减小。<br>代理模型：<br><img src="./biodiesel_log/../biodiesel_log/ignore_E_proxy.jpg"><br>物理模型：<br><img src="./biodiesel_log/../biodiesel_log/ignore_E_physical.png"></p><p>硬约束：<br>将边界条件硬编码进网络，原先软约束的边界损失项就可以被替换掉，从而不再需要考虑边界损失项的权重，并且损失项的减少也可以加速网络训练。</p><p>训练的过程中，一边训练一边调整ODE的损失权重，使得损失项的数量级相近。</p><p>调整100w次时训练结果如下：<br><img src="./biodiesel_log/hard_BC.png"></p><p>结果并没有很理想，然而物理模型和代理模型非常接近，与标称模型的区别在于标称模型的突变无法表示出来，这是因为标称模型的突变是在边界处发生的，而边界处的损失项已经被硬编码进网络，因此网络很难学习到这种突变。</p><p><a href="https://chenyu.sochiji.tech:35424/s/qrUp">模型以及参数保存在此</a></p><p>系统种V只受Fa影响，而Fa只受V与t影响，因此V完全是一个可观测的变量。在浓度方程中，V参与其他各个状态变量的计算，然而在物质的量的方程中V与其他参数没有任何关系，因此V可以视作是正问题的一部分，然而V每次的学习效果都很差，需要对其额外增加权重才会好转，一方面说明权重的重要性，另一方面这种独立于参数之外的变量对网络的学习以及参数的估计是否会有影响或干扰并不清楚，不过既然V可观测，也应该加入约束中去，约束越多越可能估计出参数。</p><p>加入V作为观测变量后，网络的学习效率有所提升，10w次就有大致趋势，但是物理模型仍有很大的差距，尤其是很难学到标称模型的突变部分，然而代理模型却几乎与标称模型一致，可能是由于学习太少，参数从标称值出发并没有来得及偏移标称值使得问题发生的不明显。训练结果如下图：<br><img src="./biodiesel_log/hard_BC_add_V.png"></p><p>然而在未将边界条件硬编码的软约束中，加入V观测值的约束后收敛变快了许多，10w次的训练已经可以大致估计出正问题，然而参数依然很难估计出来。<br>10w次训练的软约束与硬约束的结果分别如下：<br>从左往右分别是代理模型、物理模型、标称模型，<br>第一行为观测变量，第二行为不可观测变量。<br><img src="./biodiesel_log/soft_bc_10w.png"><br><img src="./biodiesel_log/hard_bc_10w.png"></p><p>用实验数据训练的结果如下：<br><img src="./biodiesel_log/pred20230610.png"><br>从左到右依次是代理模型、物理模型、标称模型，其中标称模型用标称值仿真得到，代理模型能够拟合其大致趋势，不论是观测值还是不可观测值，而估计出的参数带回物理模型依然不正确。</p><p>数据部分其实也有问题，matlab的ode15s仿真的数据和python的odeint仿真的数据不一致，而且odeint仿真的数据与实验数据也不一致，对于训练所带来的问题就是边界条件如何确定，训练数据用的是实验数据，然而实验数据的边界只有初始值，边界处不可观测变量的边界条件还未知，实验数据的边界条件与matlab仿真的边界条件与python仿真的边界条件都不一样。再者就是衡量实验结果的准确性应该用matlab还是python的仿真结果比较好。</p><h3 id="化学系统分析">化学系统分析</h3><p>17个状态变量对应17个式子，估计20个参数本就不满秩，但是实际上也就10个速率方程，秩数为10，所以俞博的论文里重点研究的就是10个参数，先试试无视可逆反应的参数，令所有可逆反应的参数值为0。<br>同时照对mailab代码使用sympy替换找到了python代码中长久以来没有发现的表达式错误，现在使用matlab仿真与python仿真的结果没有区别了。错误修正后以全1的权重训练30w次后结果如下：<br><img src="./biodiesel_log/fix_error.png"></p><p>尝试增加网络结构的复杂度，结果并没有什么区别，以下分别为3x128,4x128,5x1283x256,4x256的结果：<br><img src="./biodiesel_log/addNN.gif"></p><p>同时减小lr的做法也没效果，以lr=1e-3学习20w次损失基本收敛（小范围内震荡），令lr=1e-4继续训练模型曲线也不会产生什么特别的变化。</p><p>尝试将学习任务从简单到复杂：</p><h4 id="LSA部分">LSA部分</h4><p>参数灵敏度相对于时间的导数可以从 ODE 的偏微分获得:<br>$$<br>\frac{d}{d t} \frac{\partial \mathbf{X}}{\partial \boldsymbol{\theta}}=\frac{\partial \mathbf{f}}{\partial \mathbf{X}} \frac{\partial \mathbf{X}}{\partial \boldsymbol{\theta}}+\frac{\partial \mathbf{f}}{\partial \boldsymbol{\theta}}=\mathbf{J} \cdot \mathbf{S}+\mathbf{F}<br>$$<br>推导过程如下：<br>$$<br>\left{<br>\begin{array}{l}<br>\dot{X}(t)=f(X(t),\theta),X(t_0)=X_0\<br>Y=h(X(t),\theta)+\xi(t)<br>\end{array}\right.\<br>\Downarrow\<br>\frac{\partial}{\partial\theta}\frac{dX}{dt}=\frac{\partial}{\partial\theta}f(X(t),\theta)\<br>\Downarrow\<br>\frac{d}{dt}\frac{\partial X}{\partial\theta}=\frac{\partial f}{\partial X}\frac{dX}{d\theta}+\frac{\partial f}{\partial\theta}\<br>$$<br>其中$S=\frac{\partial X}{\partial\theta}$，$J=\frac{\partial f}{\partial X}$，$F=\frac{\partial f}{\partial\theta}$，将系统ODE方程与参数灵敏度方程组合成耦合微分方程：<br>$$<br>\begin{array}{l}<br>\dot{\mathbf{X}}(t)=\mathbf{f}(\mathbf{X}(t), \boldsymbol{\theta}), \mathbf{X}\left(t_{0}\right)=\mathbf{X}<em>{0} \<br>\dot{\mathbf{S}}(t)=\mathbf{J} \cdot \mathbf{S}(t)+\mathbf{F}, \mathbf{S}\left(t</em>{0}\right)=\mathbf{S}_{0}<br>\end{array}<br>$$</p><p>求$J,F$需要分别求导$17<em>17、17</em>20$次，手动求导不可取，使用sympy求导。</p><p>LSA不可行的原因：</p><ol><li>添加了LSA相关的ODE方程后，自动微分所占空间非常大，目前 20Para$\times$17Var 的情况已经需要20G的空间了，对于真正的复杂系统来说，几千的参数与几万的状态变量更无法实现。</li><li>添加LSA的ODE方程为软约束，使模型更加复杂且难以优化。</li><li>LSA受刚性的影响十分严重，系统初始处于一种十分混沌的状态，使其受到的干扰很大。</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>paper</title>
      <link href="/2023/03/13/paper/"/>
      <url>/2023/03/13/paper/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction">Introduction</h2><p>【研究背景】**生物系统通常使用常微分方程 (ODE) 系统建模，用以描述化学物质的量随时间的动态变化。**在过去，由于实验成本和人力成本的高昂，人们希望通过仿真模拟实验来代替实际实验。然而，对于复杂系统而言，其先验知识通常很少或很复杂，从而难以构建系统，同时数据量过大也使得仿真方法难以实现。为了解决这些问题，我们可以考虑利用数据驱动方法，如响应曲面法或神经网络建立代理模型，这些方法可以解决高维复杂系统中维数灾难的问题，并且可以利用大量的数据来发现系统中的隐藏状态。然而，纯数据驱动方法完全没有考虑到对先验知识的利用，并且冗余数据则也会导致训练结果过拟合，因此，我们需要将物理先验知识与数据驱动方法结合起来。同时，我们传统做仿真时，一些实验设计或系统辨识的方法可以通过数据驱动的方式结合神经网络实现，因此可以将PINN作为出发点。在PINN中，先验信息和数据信息的需求量成反比关系。这种方法可以适用于从模型比较清晰、数据量较少的简单系统一直到高维、数据量很大的复杂系统。</p><p>【研究问题】<strong>生物系统通常会引入一些未知参数来表示反应的速率，我们需要对这些参数进行有效的估计</strong>【继续扩展，得到有效的模型，合理地进行系统预测以及有效的控制】，参数值的推断需要使得模型预测值足够拟合系统观测值，然而在一个复杂系统中，由于技术或成本限制，我们通常只能观察到系统变量的部分状态或只能观察到部分系统变量的状态，我们仅能利用这些观测数据来推断系统中的未知参数。【motivation】本文所研究的生物柴油系统（为什么生物柴油系统需要用PINN？试验技术成本高，模型辨识困难，仿真困难，可以利用神经网络，可以利用PINN并且对于复杂系统已有PINN的解决方案）</p><p>【研究目的】本文将结合神经网络和物理先验知识，利用PINN估计生物柴油系统中的未知参数【以及不确定性分析、对实验设计方法的展望】。</p><p>【研究方法】</p><ol><li>首先利用仿真方法对现有的ODE模型生成系统的状态变量随时间的变化，作为实验数据部分。</li><li>建立PINN网络，对可观测数据进行拟合，同时将ODE方程作为物理先验知识，以软约束的形式添加到损失函数中，进行参数估计。</li><li>利用估计出的参数重新仿真，对估计结果的有效性进行分析。</li></ol><p>【研究贡献】</p><h2 id="Related-Work">Related Work</h2><p>在使用PINN解决正向问题时，我们有现成的控制方程的约束，只需要给定边界条件和初始量就可以预测整个系统的状态变化，【如某某某的某项工作】而对于逆问题，我们可以利用现成的模型结构和少量的观察值推断模型中的参数【如某某某的某项工作】，</p><h2 id="可识别性分析">可识别性分析</h2><h2 id="参数估计">参数估计</h2><h2 id="结果分析">结果分析</h2><h2 id="LSA">LSA</h2><h2 id="实验设计">实验设计</h2><h2 id="结论">结论</h2><h2 id="Reference">Reference</h2><p>以下是使用PINN实现LSA的一个简单样例：</p><p>考虑一个简单的扩散方程：</p><p>$$<br>\frac{\partial u}{\partial t} = D \frac{\partial^2 u}{\partial x^2}<br>$$</p><p>其中 $u$ 是扩散物质的浓度，$D$ 是扩散系数。</p><p>我们想要分析这个方程的稳定性，即确定参数 $D$ 的取值范围，使得系统稳定。</p><p>使用PINN实现LSA的步骤如下：</p><p>使用神经网络来近似系统的响应，即求解稳态解 $u(x)$。我们可以使用一个含有多个隐藏层的神经网络来近似 $u(x)$，并利用批量梯度下降法来训练网络。</p><p>对稳态解进行微小扰动，即 $u(x,t) = u(x) + \epsilon e^{\lambda t} \phi(x)$，其中 $\epsilon$ 是一个小的扰动系数，$\lambda$ 是特征值，$\phi(x)$ 是特征向量。</p><p>将扰动解代入原方程中，得到线性化方程 $\lambda \epsilon \phi(x) = D \frac{\partial^2}{\partial x^2} (\epsilon \phi(x))$。</p><p>对线性化方程进行求解，得到特征值 $\lambda$ 和特征向量 $\phi(x)$。</p><p>分析特征值的实部，确定系统的稳定性。如果特征值的实部均为负数，则系统是稳定的。</p><p>在这个简单的样例中，我们可以使用PINN来求解稳态解 $u(x)$，然后使用微小扰动法来求解特征值和特征向量。最终，我们可以确定参数 $D$ 的取值范围，使得系统是稳定的。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>tips</title>
      <link href="/2023/01/30/tips/"/>
      <url>/2023/01/30/tips/</url>
      
        <content type="html"><![CDATA[<h2 id="tips">tips</h2><p><a href="https://www.bilibili.com/video/BV1YB4y177bx/?vd_source=8ec8136ec38f748a5cc0e7844ed542db">bilibili视频</a>：<br>nips2022有篇文章不是说pinn将方程作为正则项导致了loss函数难以优化，鞍点太多<br>NIPS 2021：Characterizing possible failure modes in physics-informed neural networks</p><p><a href="https://www.bilibili.com/video/BV12P4y1V7sz/?vd_source=8ec8136ec38f748a5cc0e7844ed542db">bilibili视频</a><br>pinn与学习频率:<br>pinn方法学习到的频率信息是高频和低频一起学习到的，而传统的方法是先低频后高频。<br>gpinn RAR 方法:<br>这两种方法结合 能明显提高收敛效率。<br>额外的约束条件:<br>有些约束条件可能看起来是没有用的，但是因为神经网络是有误差的，不是完全满足除看起来没有用的条件的其他条件，所以看起来没有用的神经网络可能是有用的。<br>硬约束:<br>pdeloss系数缓慢增大，并加入拉格朗日乘子项。和之前拉格朗日方法是相似的，以尽可能满足pde约束。<br>拓扑优化问题:<br>固体如何分布，最小化目标函数。<br>积分算子:<br>通过离散求积分算子<br>stochastic pde:<br>通过KL展开处理  ，变为高维pde问题。<br>xde应用:<br>可以解决复杂边界，因为是无网格的方法。</p><p><a href="https://www.bilibili.com/video/BV1xD4y1775W/?spm_id_from=333.788&amp;vd_source=8ec8136ec38f748a5cc0e7844ed542db">bilibili视频</a><br>对于某些在界面上存在一些突变的系统，激活函数可以选择ReLu，对于连续光滑的偏微分，可以选择tanh。</p><p>PINN的软方式与硬方式：<br>软方式：**软方式是指将物理约束条件作为额外的损失函数添加到神经网络的训练过程中。**这些额外的损失函数通常是偏微分方程的残差或边界条件的误差，可以使用梯度下降等优化算法进行最小化。软方式的优点是实现方便，可以在现有的神经网络框架中直接添加物理约束条件，同时具有较强的鲁棒性和泛化能力。缺点是需要手动选择损失函数的权重和优化算法的超参数，调整过程较为繁琐。<br>硬方式：**硬方式是指将物理约束条件直接嵌入到神经网络模型的结构中。**这些物理约束条件可以通过添加层或神经元的方式实现，例如添加偏微分方程的算子作为一层或一组神经元。硬方式的优点是可以确保物理约束条件的精确满足，同时避免了手动选择损失函数的权重和优化算法的过程。缺点是实现较为复杂，需要对神经网络结构进行修改，同时具有较弱的泛化能力和鲁棒性。</p><p>结构可能无法识别：部分参数对模型输出影响很小；一些参数对模型预测的影响可以通过其他参数来补偿</p><p>SBINN:在损失函数中强制执行方程会在学习过程中增加额外的约束，这导致了所提出算法的几个优点：首先，一旦神经网络被训练，我们就能够推断出 ODE 系统的未知参数；其次，我们可以使用一些可观察量的极简数据来推断动力学和未知参数；第三，方程的执行增加了正则化效果，使算法对噪声具有鲁棒性</p><h2 id="总步骤">总步骤</h2><p><span id="fig1"></span></p><center><img src="./tips/step.jpg"></center>最下面的一行是整个辨识的一个流程。<h3 id="step-A">step A</h3><p>跑通基本的PINN代码，看能不能提高精度，减小error，比如网络的深度，宽度，优化器，同时思考一个问题，LSA怎么算（$\frac{\partial x}{\partial \theta}$）。有两种可能：第一个，链式法则，看能不能推出来；第二个，就得加额外的结构了。<br>正常来说的话，后面需要对模型参数做分析，uncertainty和LSA。<br>那篇uncertainty quantification要好好看，里面的bootstrap直接求解theta的上下界，就是step A.3。</p><h2 id="PINN论文">PINN论文</h2><h3 id="DeepXDE-A-Deep-Learning-Library-for-Solving-Differential-Equations">DeepXDE: A Deep Learning Library for Solving Differential Equations</h3><p>从实现的角度来看，PINN 解决逆向问题与解决正向问题一样容易。DeepXDE 可以解决给定初始和边界条件的正向问题，以及给定一些额外测量的逆向问题。<br>对于正向问题，我们令损失函数如下：<br>$$\mathcal{L}(\theta ;T)=w_{f}\mathcal{L}<em>{f}(\theta ;T</em>{f})+w_{b}\mathcal{L}<em>{b}(\theta ;T</em>{b}),$$<br>其中：<br>$$<br>\begin{aligned}<br>\mathcal{L}<em>{f}(\theta ;\mathcal{T}</em>{f})&amp;= \frac{1}{|\mathcal{T}<em>{f}|}\sum</em>{x \in T_{f}}\left|f(x; \frac{\partial \widehat{u}}{\partial x_{1}}, \ldots , \frac{\partial \widehat{u}}{\partial x_{d}}; \frac{\partial ^{2}\widehat{u}}{\partial x_{1}\partial x_{1}}, \ldots , \frac{\partial ^{2}\widehat{u}}{\partial x_{1}\partial x_{d}}; \ldots ; \lambda)\right|<em>{2}^{2}\<br>\mathcal{L}</em>{b}(\theta ;\mathcal{T}<em>{b})&amp;= \frac{1}{|\mathcal{T}</em>{b}|}\sum_{x \in T_{b}}\left|\mathcal{B}(\widehat{u},x)\right|<em>{2}^{2}\end{aligned}$$<br>解决正向和逆向问题的唯一区别是我们在 (2.2) 中添加了一个额外的损失项：$$ \mathcal{L}(\theta , \lambda ; \tau)=w</em>{f}\mathcal{L}<em>{f}(\theta , \lambda ;\mathcal{T}</em>{f})+w_{b}\mathcal{L}<em>{b}(\theta , \lambda ;\mathcal{T}</em>{b})+w_{i}\mathcal{L}<em>{i}(\theta , \lambda ;\mathcal{T}</em>{i})$$<br>其中：<br>$$\mathcal{L}<em>{i}(\theta , \lambda ;\mathcal{T}</em>{i})= \frac{1}{|\mathcal{T}<em>{i}|}\sum <em>{x \in \mathcal{T}</em>{i}}||\mathcal{T}(\widehat{u},x)||</em>{2}^{2}.$$<br>通过使用 DeepXDE，从实现的角度来看，只需定义初始条件，就可以像求解稳态一样轻松地求解瞬态 PDE。除了 DeepXDE 的主要工作流程之外，用户还可以通过回调函数轻松监控和修改求解过程。</p><p>网络部分：神经网络代表一个组合函数；考虑了足以解决大多数 PDE 问题的 FNN，以及残差神经网络 (ResNet) ，这更容易训练深度网络。</p><p>自动微分：在深度学习中，导数是使用反向传播进行评估的，这是一种专门的 AD 技术。AD 重复应用链式法则来计算导数。 AD 中有两个步骤：一个前向传递计算所有变量的值，一个反向传递计算导数。无论输入维度是多少，AD 只需要一次前向传递和一次反向传递就可以计算出所有的偏导数，当输入维度较高时，AD 比有限差分更有效。</p><p>PINN解PDE：边界条件可以是 Dirichlet、Neumann、Robin 或周期性边界条件。对于时间相关问题，我们将时间 t 视为 x 的一个特殊分量，初始条件可以简单地视为时空域上的一种特殊类型的 Dirichlet 边界条件。PINN 不能保证唯一解，因为 PINN 解是通过求解非凸优化问题获得的，而这些问题通常没有唯一解。</p><p>对于平滑的 PDE 解决方案，L-BFGS 可以找到比 Adam 更少迭代次数的好解决方案，因为 L-BFGS 使用损失函数的二阶导数，而 Adam 仅依赖于一阶导数.对于刚性解决方案，L-BFGS 更有可能陷入糟糕的局部最小值。所需的迭代次数在很大程度上取决于问题（例如，解决方案的平滑度），并且要检查网络是否收敛，我们可以使用回调函数监控损失函数或 PDE 残差。</p><p>RAR，这是一种在训练过程中自适应选择残差点的有效方法。</p><p>我们可以将残差点分成小批次而不是使用所有残差点，并且在每次迭代中我们只使用一个批次来计算损失和更新模型参数；这就是所谓的“小批量”梯度下降。</p><p>对于函数逼近，神经网络从低频到高频学习目标函数，但PINN 的学习模式因高阶导数的存在而不同。使用 PINN 求解 PDE 比使用神经网络逼近函数更快。具有足够神经元的前馈神经网络 (FNN) 可以同时一致地逼近任何函数及其偏导数。然而，神经网络在实践中的规模有限。</p><p>优化误差源于损失函数的复杂性和优化设置，例如学习率和迭代次数。然而，目前还没有针对 PINN 的误差估计，甚至量化监督学习的三个误差仍然是一个开放的研究问题。</p><h3 id="Systems-biology-informed-deep-learning-for-inferring-parameters-and-hidden-dynamics">Systems biology informed deep learning for inferring parameters and hidden dynamics</h3><p>系统级生物模型通常会引入一些未知参数，需要准确高效地进行估计。因此，这些系统的计算建模的一个核心挑战是模型参数的估计和模型动力学的预测。</p><p>然而，由于技术限制，生物反应网络通常只能部分观察到。通常，考虑到模型的大小，实验数据不足，这会导致参数无法识别 [15] 或只能在置信区间内识别。此外，系统生物学中的一大类模型对分布在多个数量级上的参数值很敏感。</p><p>系统信息神经网络，基于物理信息神经网络 [22、23] 的方法，以推断实验未观察到的物种的隐藏动力学以及方程组中的未知参数。通过将 ODE 系统纳入神经网络（通过将方程的残差添加到损失函数），我们有效地向优化算法添加了约束，</p><h3 id="Coupled-Graph-ODE-for-Learning-Interacting-System-Dynamics">Coupled Graph ODE for Learning Interacting System Dynamics</h3><p>图ODE：一种潜在常微分方程 (ODE) 生成模型，它以连续的方式使用基于图神经网络 (GNN) 的 ODE 学习节点和边的耦合动力学。</p><p>一种新的基于常微分方程 (ODE) 的生成模型：耦合图 ODE，用于通过联合考虑节点和边的演化来预测节点特征的动态。</p><p>边 ODE，广泛使用的生成过程假设新边完全由源节点和目标节点的特征决定，此论文添加了一个额外的项来模拟边缘的自我进化。这种自我进化在许多现实世界的系统中被广泛观察到。</p><p>使用 ODE 学习连续系统动力学，一个根本的挑战在于如何估计整个系统的潜在初始状态。基于 VAE 的潜在 ODE 模型来估计具有不确定性的潜在初始状态。由于对象在交互系统中高度耦合，提出了一种新颖的 GNN 作为编码器，它可以同时推断所有对象的潜在初始状态。</p><p>模型有三个部分：<br>(1) 一个编码器，可以同时推断所有对象和边缘的潜在初始状态，同时考虑它们的相互作用。<br>(2) 由两个耦合 ODE 参数化的生成模型，分别学习边和节点的演化模式。<br>(3) 节点和边的两个解码器分别将节点和边的潜在状态投射到原始输入空间。</p><p>设计:<br>对于一个具有$N$个相互作用对象的动力系统,输入包括这些对象的轨迹（特征）和它们之间随时间变化的有向加权交互图。交互图的快照:$G= \left{ G^{1},G^{2}, \ldots ,G^{T}\right} ,$其中$G^{t}=(V, \varepsilon ^{t})$是时间戳$t$的交互图,$V$表示$N$个交互对象的集合,$\varepsilon ^{t}$是有向加权边的集合。对于每对连接的节点$i,j\in V$在时间戳$t,w^t_{i\rightarrow j}\in \mathbb{R}$表示连接它们的有向边的权重,边权重不对称。$A={A^1,A^2,…,A^T}$表示加权邻接矩阵序列。$X={X^1,X^2,…,X^T}$表示节点轨迹序列,其中$X^t$是所有对象在时间戳$t$的特征向量。</p><p>目标:基于观察到的动态系统的耦合轨迹即$(X,A)$,学习建立在节点$z_i^t\in \mathbb{R}^d$和边$z_{i\rightarrow j}^t\in \mathbb{R}^d$的潜在表示之上的基础动力学，并利用它们来预测未来的轨迹$X^t(t&gt;T)$。</p><h3 id="自适应权重">自适应权重</h3><p>假设我们使用如下对数方差$s={s_f,s_b,s_i,s_d}，s:log \varepsilon^2$表示lbPINNs的损失函数：<br>$$<br>L(s;\theta;N)=\frac12 \exp(-s_f)L_{PDE}(\theta;N_f)+\frac12 \exp(-s_b)L_{BC}(\theta;N_b)+\frac12 \exp(-s_i)L_{IC}(\theta;N_i)+\frac12\exp(-s_d)L_{data}(\theta;N_{data})+s_f+s_b+s_i+s_d<br>$$</p><p>自适应权值的流程：<br>全连接神经网络定义一个PINN<br>通过自动微分计算神经网络残差，定义损失项<br>定义自适应权重集合<br>用PINNs的输出和自适应权重集合e给出的均值构建高斯概率模型。<br>使用K次梯度下降更新权重参数合网络参数</p><ol><li>基于最大似然估计定义自适应权重方法</li><li>通过Adam优化器最大化满足约束的似然，以调整自适应权重</li><li>通过Adam优化器更新网络权重</li></ol><p>lbPINN 对初始自适应权重不敏感,收敛速度明显快于PINN,性能优于 PINN。</p><h3 id="Characterizing-possible-failure-modes-in-physics-informed-neural-networks">Characterizing possible failure modes in physics-informed neural networks</h3><p><strong>问题：</strong> 涉及基于 PDE 的微分算子的 PINN 中的软正则化会引入许多微妙的问题，包括使问题更加病态。虽然将物理约束添加为软正则化可能更容易使用现有的无约束优化方法进行部署和优化，但这种方法确实需要权衡取舍，包括在许多情况下优化问题变得更难解决.<br><strong>原因：</strong> 重要的是，我们表明这些可能的故障模式不是由于 NN 架构缺乏表现力，而是 PINN 的设置使得损失情况很难优化。<br><strong>方法：</strong> 第一种方法是使用正则化，其中 PINN 的损失项从简单的 PDE 正则化开始，并随着 NN 的训练逐渐变得复杂。第二种方法是将问题视为序列到序列的学习任务，而不是学习一次预测整个时空。</p><p>添加/增加基于 PDE 的软约束正则化会使其更加复杂且难以优化，而减少正则化参数可以帮助减轻损失情况的复杂性，但这反过来会导致不满足 PDE/约束的具有高误差的不良解决方案。</p><p>PINN存在的问题：不适用于描述化学动力学的刚性常微分方程、某些异质介质问题、某些流体流动问题。</p><p>课程正则化：<br>对于具有较高 β/ρ 的情况，我们不是训练 PINN 立即学习解决方案，而是首先在较低 β/ρ 上训练 PINN（PINN 更容易学习），然后逐渐转向在较高 β/ρ 上训练 PINN分别为 ρ。</p><h3 id="Exploiting-the-bootstrap-method-for-uantifying-parameter-confidence-intervals-in-dynamical-systems">Exploiting the bootstrap method for uantifying parameter confidence intervals in dynamical systems</h3><p>通过应用bootstrap程序方法，可以获得参数的（可能）不对称置信区间的更好近似值。</p><p>分析估计参数准确性的方法很少。通常，使用以下方法:<br>参数$p_j$的方差$\sigma^2_{p_j}$由下式给出：<br>$$<br>\sigma^2_{p_j}=(F^{-1}<em>{ij})<br>$$<br>$F$是Fisher信息矩阵，$p_j$的置信区间基于$p_j$的估计值$\hat{p_j}$给出：<br>$$<br>\hat{p_j}-\sigma</em>{p_j}\cdot t^v_{\alpha/2}&lt;p_j&lt;\hat{p_j}+\sigma_{p_j}\cdot t^v_{\alpha/2}<br>$$<br>其中$t^v_{\alpha/2}$由Student’s t分布给出，$\alpha$是置信水平。$v$是自由度，$\alpha$和$1-\alpha$是自己选择的置信区间。</p><p>通常方法的缺点：<br>$\sigma^2_{p_j}=(F^{-1}_{ij})$只是方差的下限，置信区间以估计参数为中心并对称，这是由于状态变量相对于参数的线性近似。非线性系统以不同的方式表现：它们表现出非正态分布，通常伴随着偏差。</p><p>bootstrap方法通过使用有限样本的数据评估统计中的不确定性来克服理论局限性。与蒙特卡洛方法一样，bootstrap方法使用随机元素和重复模拟来分析所考虑系统的属性。应用实验设计表明，使用bootstrap方法计算的置信区间显着降低。</p><p>参数估计：<br>使用最小二乘法，在matlab中使用Levenberg–Marquardt方法来最小化测量值$x^M_i$和仿真值$x_i$的均方误差。<br>对于k个离散时间点，n个状态变量的系统，有以下目标函数：<br>$$<br>\Theta = \sum <em>{i=1}^{n}\Theta</em>{i}= \sum <em>{i=1}^{n}\sum</em>{t_{k}}(\frac{x_{i}(t_{k})-x_{i}^{M}(t_{k})}{\sigma_{x_i}})^{2}<br>$$<br>模型精度评价：<br>测量数据的标准偏差通常仅基于少量测量，因此对于每个状态的F检验考虑两个$\chi^2$分布变量的比率，$\Theta_i$和自由度$df_{1i}=K-l_i$（其中$l_i$是影响状态$i$的参个数），并且$\sigma_{x_i}$和自由度$df_2$更合适。在<br>$$<br>F= \frac{\theta <em>{i}/df</em>{1i}}{\sigma <em>{xi}}<br>$$<br>中，如果对于所有状态变量，满足：<br>$$<br>F</em>{\alpha /2,df_{11},df_{2}}&lt; \theta <em>{i}&lt;F</em>{1- \alpha /2,df_{11},df_{2}}<br>$$<br>则模型是准确的</p><p>Bootstrap方法：<br>bootstrap 方法是一种基于数据的统计推断模拟方法。该方法的主要应用是计算非参数分布的置信区间。为了执行分析，一组初始实验数据 S 用作数据库。执行参数估计产生第一组参数。由于测量误差，实验的重复导致一组略有不同的数据$\mathbf{S}^<em>_1$，因此产生一组不同的估计参数。bootstrap方法使用大量$B$次复制的实验数据$\mathbf{S}^</em>_1,\mathbf{S}^<em>_2,\mathbf{S}^</em>_3,…,\mathbf{S}^*_B$来计算重新估计的参数集的结果分布的统计特性。实际实验不可能重复过多次实验，因此使用蒙特卡洛方法来模拟数据。</p><p>bootstrap置信区间是根据$p_j$的分布（数密度函数）计算其置信限度，分布通常由直方图表示，直方图中的bin数由Freedman–Diaconis规则确定。</p><p>重复实验数据集$\mathbf{S}^<em>_1,\mathbf{S}^</em><em>2,\mathbf{S}^<em>_3,…,\mathbf{S}^</em><em>B$可重复生成重估计参数$p^<em>_1,p^</em><em>2,p^<em>_3,…,p^</em><em>B$，参数的置信区间由百分位数方法计算：<br>令$\hat{p}^{*(\alpha)}$表示$B$次重复bootstrap的$(1-\alpha)$百分位，预期的覆盖范围的百分位区间$(\bar{p</em>{lo}},\bar{p</em>{up}})$表示为：<br>$$<br>(\bar{p}</em>{lo}, \bar{p}</em>{up})=(\widehat{p}^{<em>(\alpha /2)}, \widehat{p}^{</em>(1- \alpha /2)})<br>$$<br>置信区间$(\bar{p}<em>{lo}, \bar{p}</em>{up})$的长度$L$和形状$sh$基于均值$\bar{p}$计算：<br>$$L= \bar{p}<em>{up}- \bar{p}</em>{lo}\<br>sh= \frac{\bar{p}<em>{up}-\bar{p}}{\bar{p}- \bar{p}</em>{lo}}$$<br>正态分布中的置信区间关于$\bar{p}$对称，形状sh=1.0，如果长度L基于参数$\bar{p}$的当前值，则使用符号$%L$:<br>$$<br>%L=100*\frac{L}{\bar{p}}<br>$$</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>现代控制理论</title>
      <link href="/2022/12/07/ModernControlTheory/"/>
      <url>/2022/12/07/ModernControlTheory/</url>
      
        <content type="html"><![CDATA[<h2 id="center-引言-center"><center>引言</center></h2><p>现代控制理论是一门广泛的学科，旨在研究如何通过控制系统来满足特定目标。它包括许多不同的方法和技术，用于设计、分析和实现控制系统。现代控制理论涉及到许多不同的领域，包括自动化、机械工程、电气工程、航空航天工程、生物医学工程等。本文关于现代控制理论课程的知识点进行综述，主要介绍了现代控制的基本概念，并对现代控制中的重要理论进行了推导与总结。本文分为七个章节，具体工作如下：</p><ol><li>在第一章里首先普及了现代控制理论中相关的基本概念。</li><li>在第二章里介绍了传递函数，这对后面系统的辨识具有重要的意义。</li><li>在第三章里介绍了约旦标准型矩阵，对于系统的计算优化提供了便利。</li><li>在第四章里我们对线性定常系统的状态方程求解，以便于有效地分析和控制系统。</li><li>在第五章里我们研究系统的能控型和能观性，并且在第六章里我们研究了李雅普诺夫的稳定性的判别，为系统的设计和控制提供理论依据，帮助我们了解系统的性能限制和潜在问题。</li><li>第七章我们介绍了线性反馈控制系统，这对系统的优化具有指导意义。</li></ol><h2 id="center-目录-center"><center>目录</center></h2><p><a href="#1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">第一章 基本概念</a></p><p><a href="#2-%E4%BC%A0%E9%80%92%E5%87%BD%E6%95%B0">第二章 传递函数</a></p><p><a href="#3-%E7%BA%A6%E6%97%A6%E6%A0%87%E5%87%86%E5%9E%8B%E7%9F%A9%E9%98%B5">第三章 约旦标准型矩阵</a></p><p><a href="#4-%E7%BA%BF%E6%80%A7%E5%AE%9A%E5%B8%B8%E7%B3%BB%E7%BB%9F">第四章 线性定常系统</a></p><p><a href="#5-%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%83%BD%E6%8E%A7%E6%80%A7%E5%92%8C%E8%83%BD%E8%A7%82%E6%80%A7">第五章 系统的能控性和能观性</a></p><p><a href="#6-%E6%9D%8E%E9%9B%85%E6%99%AE%E8%AF%BA%E5%A4%AB%E7%9A%84%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%88%A4%E5%88%AB">第六章 李雅普诺夫的稳定性判别</a></p><p><a href="#7-%E7%BA%BF%E6%80%A7%E5%8F%8D%E9%A6%88%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F">第七章 线性反馈控制系统</a></p><h2 id="1-基本概念">1 基本概念</h2><h3 id="1-1-系统表达">1.1 系统表达</h3><h4 id="1-1-1-状态变量">1.1.1 状态变量</h4><p>定义；足以完全确定系统运动状态的个数最小的一组变量。<br>性质： 已知$t=t_0$时刻的值，在给定$t&gt;t_0$时刻的输入$I_t$的作用下，便能完全确定系统在任何$t&gt;t_0$时刻的行为。<br>函数表示：$x_t=f(x_{t_0},I_t)$</p><h4 id="1-1-2-状态矢量">1.1.2 状态矢量</h4><p>将一组数量为$n$的状态变量用$x_1(t),x_2(t),…,x_n(t)$表示，则可用一个状态矢量$x(t)=\begin{pmatrix}<br>x_1(t)\x_2(t)\\vdots\x_n(t)<br>\end{pmatrix}$表示这一组状态变量</p><h4 id="1-1-3-状态空间">1.1.3 状态空间</h4><p>以一组数量为$n$的状态变量$x_1(t),x_2(t),…,x_n(t)$作为坐标轴表示的$n$维空间称为状态空间。</p><h4 id="1-1-4-状态方程">1.1.4 状态方程</h4><p>由系统的状态变量构成的一阶微分方程组称为系统的状态方程。</p><h4 id="1-1-5-输出方程">1.1.5 输出方程</h4><p>在指定系统输出的情况下，该输出与状态变量间的函数关系式称为系统的输出方程。</p><h4 id="1-1-6-状态空间表达式">1.1.6 状态空间表达式</h4><p>系统的状态空间表达式是一种表示系统动态行为的数学方法，它是用来描述系统的状态变化的一组方程。通常，状态空间表达式由三个部分组成：状态方程、输出方程和输入方程。状态方程描述系统的内部状态变化，输出方程描述系统的输出，输入方程描述系统的输入。</p><p>对于一个单输入单输出定常系统，其状态变量为$x_1,x_2,…,x_n$，则状态的一般方程的一般形式为：<br>$$<br>\dot{x_1}=a_{11}x_1+a_{12}x_2+…+a_{1n}x_n+b_1u\<br>\dot{x_2}=a_{21}x_1+a_{22}x_2+…+a_{2n}x_n+b_2u\<br>\vdots\<br>\dot{x_n}=a_{n1}x_1+a_{n2}x_2+…+a_{nn}x_n+b_nu\<br>$$<br>输出方程如下：$y=c_1x_1+c_2x_2+…+c_nx_n$<br>矩阵表示为：<br>$$\left{\begin{aligned}<br>\dot x&amp;=Ax+bu\<br>y&amp;=cx<br>\end{aligned}\right.<br>$$</p><p>多输入多输出的系统空间表达式为<br>$$\left{\begin{aligned}<br>\dot x&amp;=Ax+Bu\<br>y&amp;=Cx+Du<br>\end{aligned}\right.<br>$$<br>其中$x$为$n$维状态矢量，$A$为$n\times n$的系统矩阵，$u=\begin{bmatrix}<br>u_1\u_2\\vdots\u_r<br>\end{bmatrix}$为$r$维输入矢量，$y=\begin{bmatrix}<br>y_1\y_2\\vdots\y_m<br>\end{bmatrix}$为$m$维输出矢量。<br>$B$为输入矩阵，$C$为输出矩阵，$D$为直接传递矩阵。<br>除了将多输入向量化为$B$外，还需要考虑输入矢量$U$的直接传递和状态变量的组合，因此会额外多一个$D$矩阵。</p><h4 id="1-1-7-状态结构图">1.1.7 状态结构图</h4><p><span id="fig1"></span></p><center><img src="./ModernControlTheory/状态结构图.png">图1</center><p><a href="#fig1">图1</a>的上半部分是单变量单输入系统$\left{\begin{array}{l}<br>\dot x=Ax+bu\<br>y=cx<br>\end{array}\right.$的状态结构图，下半部分是多输入多输出系统$\left{\begin{array}{l}<br>\dot x=Ax+Bu\<br>y=Cx+Du<br>\end{array}\right.$的状态结构图。</p><h5 id="绘制步骤">绘制步骤</h5><ol><li>用积分器的输出表示相应的某个状态变量。</li><li>根据所给的状态方程和输出方程，画出相应的加法器和比例器。</li><li>最后用箭头连接所有的元件。</li></ol><h5 id="举例">举例</h5><p>现有一个单输入单输出系统，其状态空间表达式为<br>$$\left{\begin{array}{l}<br>\dot x_1=x_2\<br>\dot x_2=x_3\<br>\dot x_3=-6x_1-3x_2-2x_3+u\<br>y=x_1+x_2<br>\end{array}\right.<br>$$<br>其模拟结构图如<a href="#fig2">图2</a>所示。<br><span id="fig2"></span></p><center><img src="./ModernControlTheory/单输入单输出.png">图2</center><p>对于一个多输入多输出变量系统，其状态空间表达式为<br>$$\left{\begin{array}{l}<br>\dot x_1=a_{11}x_1+a_{12}x_2+b_{11}u_1+b_{12}u_2\<br>\dot x_2=a_{21}x_1+a_{22}x_2+b_{21}u_1+b_{22}u_2\<br>y_1=c_{11}x_1+c_{12}x_2\<br>y_2=c_{21}x_1+c_{22}x_2<br>\end{array}\right.<br>$$<br>其模拟结构图如<a href="#fig3">图3</a>所示。<br><span id="fig3"></span></p><center><img src="./ModernControlTheory/多输入多输出.png">图3</center><h3 id="1-2-非唯一性">1.2 非唯一性</h3><p>对于一个给定的定常系统，可以选取许多种状态变量，相应地有许多种状态空间表达式描述同一系统。因此，系统的结构可以有多种形式，并且取到的各种状态矢量在本质上是一种矢量的线性变换。</p><p>对于系统$\left{\begin{array}{l}<br>\dot x=Ax+Bu,\quad x(0)=x_0\<br>y=Cx+Du<br>\end{array}\right.$<br>我们总能找到任意一个非奇异矩阵$T$将原状态矢量$x$做线性变换，得到另一个状态矢量$z$，其中变换关系为$x=Tz$，相应的$z=T^{-1}x$。带入原系统，可得新的状态空间表达式<br>$$<br>\left{<br>\begin{array}{l}<br>\dot z=T^{-1}ATz+T^{-1}Bu,\quad z(0)=T^{-1}x(0)=T^{-1}x_0\<br>y=cTz+Du<br>\end{array}<br>\right.<br>$$<br>我们将$T$称为变换矩阵。</p><h3 id="1-3-系统不变量">1.3 系统不变量</h3><h4 id="1-3-1-特征值">1.3.1 特征值</h4><p>系统的特征值就是系统矩阵$A$的特征值，即特征方程$|\lambda I-A|=0$的根。实际的物理系统中，系统矩阵$A$为方阵，对于一个$n$阶方阵$A$有$n$个特征值，其特征值为实数或共轭复数。额外的，如果$A$为实对称方阵，那么$A$的特征值都是实数。<br>对于$n$维矢量$p_i$，若其经过以$A$作为变换阵的变换得到一个新的矢量$\tilde{p}<em>{i}=\lambda</em>{i} p_{i}$，若变换后矢量$p_i$方向不变，仅长度变化$\lambda_i$倍，则称$p_i$为对应于特征值$\lambda_i$的特征向量。此时有$Ap_i=\lambda_ip_i$。</p><h4 id="1-3-2-系统不变量">1.3.2 系统不变量</h4><p>同一系统经过非奇异变换后，得$<br>\left{<br>\begin{array}{l}<br>\dot z=T^{-1}ATz+T^{-1}Bu\<br>y=cTz+Du<br>\end{array}<br>\right.<br>$<br>其对应特征方程为$|\lambda I-T^{-1}AT|=0$。我们可以证明：<br>$$<br>\begin{aligned}<br>\left|\lambda I-T^{-1} A T\right| &amp; =\left|\lambda T^{-1} T-T^{-1} A T\right|=\left|T^{-1} \lambda T-T^{-1} A T\right| \<br>&amp; =\left|T^{-1}(\lambda I-A) T\right|=\left|T^{-1}\right| \lambda I-A|T| \<br>&amp; =\left|T^{-1} T\right| \lambda I-A|=| \lambda I-A \mid<br>\end{aligned}<br>$$<br>虽然系统经过非奇异变换，特征方程形式不同，但是其特征值依然是不变的。将$|\lambda I-A|$展开，得到一个关于$\lambda$的多项式$$\lambda^n+a_{n-1}\lambda^{n-1}+…+a_1\lambda+a_0=0$$<br>特征值由多项式系数$a_{n-1}，a_{n-2},…,a_0$确定，特征值是不变的，因此这些系数也是不变的，因此特征多项式的系数$a_{n-1}，a_{n-2},…,a_0$称为系统的不变量。</p><h2 id="2-传递函数">2 传递函数</h2><p>传递函数是一种用于表示系统动态行为的数学模型。它描述了一个系统输入信号与输出信号之间的关系，并能够用来分析系统的频率响应特性。传递函数在控制工程、信号处理和电子工程等领域中都有广泛的应用。</p><h3 id="2-1-单输入单输出中的传递函数">2.1 单输入单输出中的传递函数</h3><p>空间状态表达式$\left{\begin{array}{}<br>\dot x=Ax+bu\<br>y=cx+du<br>\end{array}\right.$，进行拉氏变换，并且假设初始为零，可以得到<br>$$\left{\begin{array}{l}<br>X(s)=(sI-A)^{-1}bU(s)\Y(s)=cX(s)+dU(s)<br>\end{array}\right.\Rightarrow<br>\begin{aligned}<br>Y(s)&amp;=c(sI-A)^{-1}bU(s)+dU(s)\<br>&amp;=[c(sI-A)^{-1}+d]U(s)<br>\end{aligned}<br>$$<br>$U$-$X$之间的传递函数阵为$W_{ux}(s)=\frac{X(s)}{U(s)}=(s I-A)^{-1} b$<br>$U$-$Y$之间的传递函数阵为$W(s)=\frac{Y(s)}{U(s)}=c(s I-A)^{-1} b+d$</p><h3 id="2-2-多输入多输出中的传递函数">2.2 多输入多输出中的传递函数</h3><p>空间状态表达式$\left{\begin{array}{}<br>\dot x=Ax+Bu\<br>y=Cx+Du<br>\end{array}\right.$，进行拉氏变换，并且假设初始为零，可以得到<br>$$\left{\begin{array}{l}<br>X(s)=(sI-A)^{-1}BU(s)\Y(s)=CX(s)+DU(s)<br>\end{array}\right.\Rightarrow<br>\begin{aligned}<br>Y(s)&amp;=C(sI-A)^{-1}BU(s)+dU(s)\<br>&amp;=[C(sI-A)^{-1}+D]U(s)<br>\end{aligned}<br>$$<br>$U$-$X$之间的传递函数阵为$W_{ux}(s)=\frac{X(s)}{U(s)}=(sI-A)^{-1} B$<br>$U$-$Y$之间的传递函数阵为$W(s)=\frac{Y(s)}{U(s)}=C(s I-A)^{-1} B+D$</p><p>值得注意的是，同一系统中尽管状态空间表达式可以作各种非奇异变换以至于状态空间表达式不唯一，但是它的传递函数是不变的。</p><h3 id="2-3-子系统连结的传递函数阵">2.3 子系统连结的传递函数阵</h3><p>令子系统1为$\left{\begin{array}{}<br>\dot x=A_1x_1+B_1u_1\<br>y=C_1x_1+D_1u_1<br>\end{array}\right.$，记为$\Sigma_1$<br>令子系统2为$\left{\begin{array}{}<br>\dot x=A_2x_2+B_2u_2\<br>y=C_2x_2+D_2u_2<br>\end{array}\right.$，记为$\Sigma_2$</p><h4 id="2-3-1-并联">2.3.1 并联</h4><p>系统结构如<a href="#fig4">图4</a>所示。<br><span id="fig4"></span></p><center><img src="./ModernControlTheory/并联.png"/><br>图4</center><p>将子系统的输入结合，得到其矩阵形式：<br>$$<br>\left{\begin{array}{l}<br>\dot{x}<em>{1}=A</em>{1} x_{1}+B_{1} u_{1} \<br>\dot{x}<em>{2}=A</em>{2} x_{2}+B_{2} u_{2}<br>\end{array} \Rightarrow\left[\begin{array}{l}<br>\dot{x}<em>{1} \<br>\dot{x}</em>{2}<br>\end{array}\right]=\left[\begin{array}{cc}<br>A_{1} &amp; 0 \<br>0 &amp; A_{2}<br>\end{array}\right] \cdot\left[\begin{array}{l}<br>x_{1} \<br>x_{2}<br>\end{array}\right]+\left[\begin{array}{l}<br>B_{1} \<br>B_{2}<br>\end{array}\right] u\right.<br>$$<br>对子系统的输出进行同样的操作：<br>$$<br>\left{\begin{array}{l}<br>y_{1}=C_{1} x_{1}+D_{1} u_{1} \<br>y_{2}=C_{2} x_{2}+D_{2} u_{2}<br>\end{array}\right. \Rightarrow y=\left[\begin{array}{ll}<br>C_{1} &amp; C_{2}<br>\end{array}\right] \cdot\left[\begin{array}{l}<br>x_{1} \<br>x_{2}<br>\end{array}\right]+(D_{1} D_{2}) u<br>$$<br>整理并联系统的传递函数矩阵为：<br>$$<br>\begin{aligned}<br>W(s) &amp;=C(sI-A)^{-1}B+D\<br>&amp; =\left[\begin{array}{ll}<br>C_{1} &amp; C_{2}<br>\end{array}\right] \cdot\left[\begin{array}{cc}<br>(s I-A_{1})^{-1} &amp; 0 \<br>0 &amp; (s I-A_{2})^{-1}<br>\end{array}\right] \cdot\left[\begin{array}{l}<br>B_{1} \<br>B_{2}<br>\end{array}\right]+(D_{1}  D_{2}) \<br>&amp; =\left[C_{1}(s I-A_{1})^{-1} B_{1}+D_{1}\right] \pm\left[C_{2}(s I-A_{2})^{-1} B_{2}+D_{2}\right]\<br>&amp;=W_1(s)+W_2(s)<br>\end{aligned}<br>$$<br>各子系统存在相同输入$u_1=u_2=u$，组合后的系统的输出是各子系统输出的代数和$y=y_1+y_2$。</p><h4 id="2-3-2-串联">2.3.2 串联</h4><p>系统结构如<a href="#fig5">图5</a>所示<br><span id="fig5"></span></p><center><img src="./ModernControlTheory/串联.png">图5</center><p>传递函数阵为$W(s)=W_2(s)W_1(s)$，系统传递函数等于子系统传递函数阵之积。</p><h4 id="2-3-3-反馈">2.3.3 反馈</h4><p>系统状态表达式如下：<br>$$<br>\left{<br>s\begin{array}{l}<br>\dot{x}<em>{1}=A</em>{1} x_{1}+B_{1}\left(u-C_{2} x_{2}\right) \<br>\dot{x}<em>{2}=A</em>{2} x_{2}+B_{2} C_{1} x_{1} \<br>y=y_{1}=C_{1} x_{1}<br>\end{array}<br>\right.<br>$$<br>矩阵形式为：<br>$$<br>\begin{bmatrix}<br>\dot x_1\\dot x_2<br>\end{bmatrix}=\begin{bmatrix}<br>A_1&amp;-B_1C_2\<br>B_2C_1&amp;A_2<br>\end{bmatrix}\cdot \begin{bmatrix}<br>x_1\x_2<br>\end{bmatrix}+\begin{bmatrix}<br>B_1\0<br>\end{bmatrix}u\<br>y=\begin{bmatrix}<br>C_1&amp;0<br>\end{bmatrix}\cdot\begin{bmatrix}<br>x_1\x_2<br>\end{bmatrix}<br>$$</p><p>系统结构如<a href="#fig6">图6</a>所示<br><span id="fig6"></span></p><center><img src="./ModernControlTheory/反馈.png">图6</center><p>系统的传递函数阵为；<br>$$<br>W(s)=\begin{bmatrix}<br>C_1&amp;0<br>\end{bmatrix}\cdot\left[\begin{array}{cc}<br>s I-A_{1} &amp; B_{1} C_{2} \<br>-B_{2} C_{1} &amp; s I-A_{2}<br>\end{array}\right]^{-1} \cdot\left[\begin{array}{c}<br>B_{1} \<br>0<br>\end{array}\right]<br>$$</p><h3 id="2-4-传递函数的实现">2.4 传递函数的实现</h3><p>实现问题指的是由输入到输出的动态关系的运动方程式或传递函数直接建立状态空间表达式的过程。从传递函数求得的状态空间表达式并不是唯一的，其中$A,b,c,d$可以取无穷多种形式，这是实现的非唯一性。尽管实现非唯一，但只要原系统中不出现零极对消现象，则n阶系统中必有n个独立状态的变量，必有n个一阶微分方程与之等效，即使系统A的元素取值不同，但由于是同一个系统的实现，因此特征根必然相同。通常我们将没有零极对消的传递函数的实现称为最小实现。</p><h4 id="2-4-1-零极对消">2.4.1 零极对消</h4><p>传递函数的零极点模型为<br>$$G(s)=\frac{K(s-z_1)(s-z_2)…(s-z_m)}{(s-p_1)(s-p_2)…(s-p_n)}$$<br>当分子为零的解即零点$z_1,z_2…,z_m$各不相等，且分母为零的解即极点$p_1,p_2…p_n$各不相同，就不会出现零极点对消。</p><h4 id="2-4-2-传递函数中没有零点的实现">2.4.2 传递函数中没有零点的实现</h4><p>这种情况下，系统的微分方程为：$$y^{(n)}+a_{n-1}y^{(n-1)}+…+a_1\dot y+a_0y=b_0u(t)$$</p><p>对系统微分方程进行处理，将每个积分器的输出取作状态变量，或称为相变量，它是输出$y$或$y/b_0$的各阶导数。</p><p>$$<br>\begin{aligned}<br>&amp;y^{(n)}+a_{n-1} y^{(n-1)}+\cdots+a_{1} \dot{y}+a_{0} y=b_{0} u(t) \<br>&amp;\frac{y^{(n)}}{b_{0}}+\frac{a_{n-1} y^{(n-1)}}{b_{0}}+\cdots+\frac{a_{1} \dot{y}}{b_{0}}+\frac{a_{0} y}{b_{0}}=u(t) \<br>&amp;\frac{y^{(n)}}{b_{0}}=-a_{n-1} \underbrace{\frac{y^{(n-1)}}{b_{0}}}<em>{x_n}-\cdots-a</em>{1} \underbrace{\frac{\dot{y}}{b_{0}}}<em>{x_2}-a</em>{0}\underbrace{\frac{y}{b_{0}}}<em>{x_1}+u(t)<br>\end{aligned}<br>$$<br>此时系统的状态方程为：<br>$$<br>\left{\begin{array}{l}<br>\dot{x}</em>{1}=x_{2} \<br>\dot{x}<em>{2}=x</em>{3} \<br>\vdots \<br>\dot{x}<em>{n-1}=x</em>{n} \<br>\dot{x}<em>{n}=-a</em>{0} x_{1}-a_{1} x_{2}-\cdots-a_{n-2} x_{n-1}-a_{n-1} x_{n}+u<br>\end{array}\right.<br>$$<br>输出方程为：$y=b_0x_1$。<br>表示成矩阵形式如下：<br>$$<br>\begin{array}{l}<br>{\left[\begin{array}{c}<br>\dot{x}<em>{1} \<br>\dot{x}</em>{2} \<br>\vdots \<br>\dot{x}<em>{n- 1} \<br>\dot{x}</em>{n}<br>\end{array}\right]=\left[\begin{array}{ccccc}<br>0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \<br>\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \<br>-a_{0} &amp; -a_{1} &amp; -a_{2} &amp; \cdots &amp; -a_{n-1}<br>\end{array}\right] \cdot\left[\begin{array}{c}<br>x_{1} \<br>x_{2} \<br>\vdots \<br>x_{n- 1} \<br>x_{n}<br>\end{array}\right]+\left[\begin{array}{c}<br>0 \<br>0 \<br>\vdots \<br>0\<br>1<br>\end{array}\right] u}<br>\end{array}<br>$$<br>$y=\begin{bmatrix}<br>b_0&amp;0&amp;0&amp;\cdots&amp;0<br>\end{bmatrix}x$</p><p>系统结构模拟图如<a href="#fig7">图7</a>所示。<br><span id="fig7"></span></p><center><img src="./ModernControlTheory/传递函数无零点实现系统结构模拟图.png">图7</center><p>其传递函数为<br>$$W(s)=\frac{Y(s)}{U(s)}=\frac{b_0}{s^{(n)}+a_{n-1}s^{(n-1)}+…+a_1s+a_0},\quad m&lt;n$$</p><h4 id="2-4-3-传递函数中有零点的实现">2.4.3 传递函数中有零点的实现</h4><p>这种情况下，系统的微分方程为：<br>$$<br>y^{(n)}+a_{n-1} y^{(n-1)}+\cdots+a_{1} \dot{y}+a_{0} y=b_{m} u^{(m)}+b_{m-1} u^{(m-1)}+\cdots+b_{1} \dot{u}+b_{0} u<br>$$</p><p>传递函数为：<br>$$<br>W(s)=\frac{b_{m} s^{(m)}+b_{m-1} s^{(m-1)}+\cdots+b_{1} s+b_{0}}{s^{(n)}+a_{n-1} s^{(n-1)}+\cdots+a_{1} s+a_{0}}, \quad m \leq n<br>$$</p><p>对于传递函数中有零点的实现这种包含有输入函数导数的情况，关键在于需要选取合适的结构是的状态方程中不包含输入函数的导数项，否则将给求解和物理实现带来麻烦。</p><h2 id="3-约旦标准型矩阵">3 约旦标准型矩阵</h2><p>将系统矩阵变换为约旦标准型矩阵，即为将$\left{\begin{array}{l}<br>\dot x=Ax+Bu\<br>y=Cx<br>\end{array}\right.$变换为$\left{<br>\begin{array}{l}<br>\dot z=Jz+T^{-1}Bu\<br>y=cTz<br>\end{array}<br>\right.$<br>其中$J=T^{-1}AT$。</p><h3 id="3-1-A-为任意型">3.1 $A$为任意型</h3><p>当$A$的特征值无重根时，可直接构造变换矩阵$T=\begin{bmatrix}<br>p_1&amp;p_2&amp;\cdots&amp;p_n<br>\end{bmatrix}$，则$T^{-1}AT=\Lambda$。此时$J=\Lambda$。<br>当$A$的特征值有$q$个值为$\lambda_1$的重根，其余$(n-q)$个根互异时，令变换矩阵$T=\begin{bmatrix}<br>p1&amp;p2&amp;\cdots&amp;p_q&amp;p_{q+1}&amp;\cdots&amp;p_n<br>\end{bmatrix}$，其中$p_{q+1},…p_n$同无重根情况，对于$q$个$\lambda_1$重根的各特征矢量$p_1,…p_q$,$p_1$为$\lambda_1$对应的特征矢量，其余所有特征矢量称为其广义特征矢量。此时<br>$$<br>J=\left[\begin{array}{cccccccc}<br>\lambda_{1} &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; &amp; &amp; \<br>0 &amp; \lambda_{1} &amp; \ddots &amp; \ddots &amp; 0 &amp; &amp; &amp; \<br>\vdots &amp; \ddots &amp; \ddots &amp; \ddots &amp; 0 &amp; &amp; 0 &amp; \<br>0 &amp; \ddots &amp; \ddots &amp; \lambda_{1} &amp; 1 &amp; &amp; &amp; \<br>0 &amp; 0 &amp; \cdots &amp; 0 &amp; \lambda_{1} &amp; &amp; &amp; \<br>&amp; &amp; &amp; &amp; &amp; \lambda_{q+1} &amp; 0 &amp; 0 \<br>&amp; &amp; 0 &amp; &amp; &amp; 0 &amp; \ddots &amp; 0 \<br>&amp; &amp; &amp; &amp; &amp; 0 &amp; 0 &amp; \lambda_{n}<br>\end{array}\right]$$</p><h3 id="3-2-A-为标准型">3.2 $A$为标准型</h3><p>此时$<br>A=\left[\begin{array}{ccccc}<br>0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \<br>\vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots \<br>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \<br>-a_{0} &amp; -a_{1} &amp; -a_{2} &amp; \cdots &amp; -a_{n-1}<br>\end{array}\right]<br>$<br>特征值无重根时，其变换是一个范德蒙德矩阵<br>$${T}=\left[\begin{array}{cccc}<br>1 &amp; 1 &amp; \cdots &amp; 1 \<br>\lambda_{1} &amp; \lambda_{2} &amp; \cdots &amp; \lambda_{n} \<br>\lambda_{1}^{2} &amp; \lambda_{2}^{2} &amp; \cdots &amp; \lambda_{n}^{2} \<br>\vdots &amp; \vdots &amp; \vdots &amp; \vdots \<br>\lambda_{1}^{n-1} &amp; \lambda_{2}^{n-1} &amp; \cdots &amp; \lambda_{n}^{n-1}<br>\end{array}\right]$$<br>特征值有重根时<br>$$<br>\begin{aligned}<br>T &amp;=\left[p_1\quad p_2\quad p_3\quad p_q\quad \cdots\quad p_n\right]\<br>&amp;=\left[{p}<em>{1}\quad\frac{d {p}</em>{1}}{d \lambda_{1}} \quad \frac{1}{2 !} \frac{d^{2} {p}<em>{1}}{d \lambda</em>{1}^{2}} \quad \cdots \quad \frac{1}{(q-1) !} \frac{d^{q-1} {p}<em>{1}}{d \lambda</em>{1}^{q-1}} \quad {p}<em>{q+1} \quad \cdots \quad {p}</em>{n}\right]<br>\end{aligned}<br>$$</p><h2 id="4-线性定常状态方程">4 线性定常状态方程</h2><p>一般的问题中我们讨论的都是线性定常状态方程，其中包括线性定常齐次状态方程和线性定常其次状态方程。本节中我们分别讨论这两种状态方程的求法。</p><h3 id="4-1-线性定常齐次状态方程">4.1 线性定常齐次状态方程</h3><h4 id="4-1-1-线性定常其次状态方程的自由解">4.1.1 线性定常其次状态方程的自由解</h4><p>自由解：即线性定常齐次状态方程的解，指的是当控制系统的输入为零时，由初始状态引起的震动。</p><p>系统的状态空间表达式一般为线性定常齐次状态方程$\dot x=Ax+Bu$，令输入为零即$u=0$，则有$\dot x=Ax$。若系统初始即$t=t_0$时，$x(t_0)=x_0$，则方程有唯一的确定解$x(t)=e^{A(t-t_0)}x_0,\quad t\geq t_0$。</p><p>令$\dot x=Ax$的解为$t$的矢量幂级形式，即：<br>$$x(t)=b_0+b_1t+b_2t^2+\cdots+b_kt_k+\cdots$$<br>则有$\dot x(t)=b_1+2b_2t+3b_3t^2+\cdots+kb_kt^{k-1}+\cdots$<br>代入原方程，有<br>$$<br>\begin{aligned}<br>&amp;b_1+2b_2t+3b_3t^2+\cdots+kb_kt^{k-1}+\cdots\<br>=&amp;A(b_0+b_1t+b_2t^2+\cdots+b_kt_k+\cdots)<br>\end{aligned}<br>$$<br>等式两边指数项相同则系数项相同，因此有<br>$$<br>\begin{aligned}<br>b_1&amp;=Ab_0\<br>b_2&amp;=\frac12Ab_1=\frac{1}{2!}A^2b_0\<br>b_3&amp;=\frac13Ab_2=\frac{1}{3!}A^3b_0\<br>&amp;\cdots\cdots\<br>b_k&amp;=\frac1kAb_{k-1}=\frac{1}{k!}A^kb_0\<br>&amp;\cdots\cdots\<br>\end{aligned}<br>$$</p><p>令$t=0$，则有$x(t)=b_0+b_1t+b_2t^2+\cdots+b_kt_k+\cdots=b_0$。代入$x(t)$的矢量幂级解，得<br>$$x(t)=(1+At+\frac1{2!}A^2t^2+\cdots+\frac{1}{k!}A^kt_k+\cdots)x_0$$<br>刚好为$e^{At}$的泰勒展开，$e^{At}$也称状态转移矩阵。</p><h4 id="4-1-2-状态转移矩阵">4.1.2 状态转移矩阵</h4><p>可将齐次矩阵微分方程表示为$x(t)=\Phi(t)x(0)$。</p><p>性质1：组合型。<br>$$<br>\left{<br>\begin{aligned}<br>\Phi(t)\Phi(\tau)=\Phi(t+\tau)\<br>e^{At}e^{A\tau}=e{A(t+\tau)}<br>\end{aligned}<br>\right.<br>$$<br>有$x(t_2)=\Phi(t_2-t_1)x(t_1)=\Phi(t_2-t_1)\Phi(t_1)x(0)$，矩阵表示为$e^{At_2}=e^{A(t_2-t_1)}e^{At_1}$</p><p>性质2:状态矢量由$t$时刻转移到$t$时刻时状态矢量不变。<br>$$<br>\left{<br>\begin{aligned}<br>\Phi(t-t)=I\<br>e^{A(t-t)}=I<br>\end{aligned}<br>\right.<br>$$</p><p>性质3：状态转移矩阵的逆意味着时间的逆转，可以用于求出小于时刻$t$的状态矢量。<br>$$<br>\left{<br>\begin{aligned}<br>[\Phi(t)]^{-1}=\Phi(-t)\<br>[e^{A(t-t)}]^{-1}=e^{-At}<br>\end{aligned}<br>\right.<br>$$</p><p>性质4：矩阵与$A$矩阵是可交换的。<br>$$<br>\left{<br>\begin{aligned}<br>\dot\Phi(t)=A\Phi(t)=\Phi(t)A\<br>\frac{d}{dt}e^{At}=Ae^{At}=e^{At}A<br>\end{aligned}<br>\right.<br>$$</p><p>性质5：<br>$A,B$为$n$阶方阵，当且仅当$AB=BA$时，$e^{At}e^{Bt}=e^{(A+B)t}$<br>当$AB\ngeq BA$时，$e^{At}e^{Bt}\ngeq e^{(A+B)t}$</p><h3 id="4-2-线性定常非齐次状态方程">4.2 线性定常非齐次状态方程</h3><p>在线性定常非齐次状态方程中讨论的是线性定常系统在控制$u(t)$作用下的强制运动，状态方程为$\dot x=Ax+Bu$。<br>令初始状态为$t_0$，其解为：<br>$$<br>x(t)=\underbrace{\Phi(t-t_0)x(t_0)}<em>{初始状态引起的振动}+\underbrace{\int</em>{t_0}^{t}\Phi(t-\tau)Bu(\tau)d\tau}<em>{控制激励引起的强制振动}<br>$$<br>以下为求解过程：<br>$$<br>\begin{aligned}<br>\dot x-Ax&amp;=Bu\<br>e^{-At}(\dot x-Ax)&amp;=e^{At}Bu\<br>\frac{d}{dt}[e^{-At}x(t)]&amp;=e^{-At}Bu\<br>[e^{-Atx(t)}]|</em>{t_0}^{t}&amp;=\int_{t_0}^{t}e^{-A\tau}Bud\tau\<br>e^{-At}x(t)-e^{-At_0}x(t_0)&amp;=\int_{t_0}^te^{-A\tau}Bud\tau\<br>e^{-At}x(t)=e^{-At_0}x(t_0)&amp;+\int_{t_0}^te^{-A\tau}Bud\tau\<br>x(t)=e^{A(t-t_0)x(t_0)}&amp;+\int_{t_0}^t e^{A(t-\tau)}Bud\tau<br>\end{aligned}<br>$$</p><h2 id="5-系统的能控性和能观性">5 系统的能控性和能观性</h2><p>1960年，卡尔曼提出了能控型和能观性的概念。能控性和能观性是最优控制和最优估计的基础，现代控制理论建立的基础是用状态空间描述，其中状态方程用来描述输入$u(t)$引起状态$x(t)$的变化过程，输出方程用来描述由状态变化引起的输出$y(t)$的变化。因此能控型表示了分析输入$u(t)$对状态$x(t)$的控制能力，能观性表示了输出$y(t)$对状态$x(t)$的反映能力。</p><h3 id="5-1-系统能控型">5.1 系统能控型</h3><p>能控型研究的是系统在$u(t)$的控制下，$x(t)$的转移情况，与输出无关，因此只研究状态方程。</p><blockquote><p>定义：在有限时间区间$[t_0,t_f]$内，如果存在一个分段连续的输入$u(t)$，使得系统由某一初始状态$x(t_0)$转移到指定的任意终端状态$x(t_f)$，则称此状态是能控的。若所有状态都能控，则系统是状态完全能控的，或简称系统是能控的。</p></blockquote><p>点系统能控的条件：$n$维空间中的点系统中的点$P \in \mathbb{R}^n$能在输入的作用下驱动到任意指定的状态$P_1,P_2,P_3…P_n$。<br>状态完全能控的条件：能控状态充满整个状态空间,即对于任意初始状态都能找到相应的控制输入$u(T)$，使得在有限的时间区间$[t_0,t_f]$内,将状态转移到状态空间内的任意指定状态，则称系统完全能控，即系统任意点都能控。从结构图看，对于状态的能控型，只需要观察是否有与$u(t)$无关的鼓励方块。以下是几种判别能观性的方法。</p><h4 id="5-1-1-约旦标准型判别能控性">5.1.1 约旦标准型判别能控性</h4><p>对于一个单输入系统，若特征值互异，即没有重根，此时的特征矩阵为对角阵，状态方程为$\dot x=\Lambda x+bu$，若有重根，则特征矩阵为约旦块，状态方程为$\dot x =Jx+bu$。<br>系统的能控型取决于$A$和$b$，$A$由系统的结构和内部参数决定，$b$与控制作用的施加点相关。对于约旦标准型矩阵：</p><ol><li>若$A$为对角矩阵时，如果$b$中有元素为0，则对应状态不能控，此时状态是不完全能控的。</li><li>若$A$为约旦标准矩阵，前一个状态总是受下一状态的控制，如果$b$中相应于约旦块中的最后一个元素为0，则状态不完全能控。</li></ol><p>对于具有一般系统矩阵的多输入系统，即$\dot x=Ax+Bu$，令变换矩阵$T$使得$x=Tz$，可变为约旦标准型$\dot z=\Lambda z+T^{-1}Bu$或$\dot z=Jz+T^{-1}Bu$。系统的线性变换不会改变系统的能控性条件，证明如下：<br>$$<br>\left{<br>\begin{array}{l}<br>某第i个状态x_i不能控\<br>x^i(0)e^{\lambda_i t}的自由分量不能控\<br>相应特征值的自然模式e^{\lambda_i t}不能控\<br>线性变换不改变系统的特征值<br>\end{array}\right.\Rightarrow线性变换不改变能控性<br>$$</p><p>线性变换后，判别依据如下：</p><ol><li>若$A$的特征值互异，则$\dot z=\Lambda z+T^{-1}Bu$，系统能控的充要条件为$T^{-1}B$中的各行元素都不为全零。</li><li>若$A$的特征值有相同，则$\dot z=Jz+T^{-1}Bu$，系统能控的充要条件为<ul><li>对于$T^{-1}B$中特征值互异的部分，它的各行元素没有全零的。</li><li>对于$T^{-1}B$中特征值相同的部分，每个约旦块最后一行相对应的元素没有全为0的。</li></ul></li></ol><h4 id="5-1-2-通过-A-、-B-矩阵判别系统的能控性">5.1.2 通过$A$、$B$矩阵判别系统的能控性</h4><p>状态完全能控的充要条件是矩阵$M=\begin{bmatrix}<br>B&amp;AB&amp;A^2B\cdots&amp;A^{n-1}B<br>\end{bmatrix}$的秩为$n$。</p><h4 id="5-1-3-通过传递方程判别系统的能控型">5.1.3 通过传递方程判别系统的能控型</h4><p>状态完全能控的充要条件是$W_{ux}(s)=(sI-A)^{-1}b$没有零点和极点重合现象，否则被相消的极点就是不能控的模式，系统为不能控系统。</p><h3 id="5-2-系统能观性">5.2 系统能观性</h3><p>控制系统大多采用反馈控制的形式，在现代控制理论中，反馈信息是由系统的状态变量组合而成的。然而并非所有的系统的状态变量在物理上都能测取到。因此我们考虑能否通过对输出的测量获得全部状态变量的信息，这就是系统的能观性。能观性所表示的是输出$y(t)$反映状态矢量$x(t)$的能力，与控制作用没有直接关系，所以分析能观性问题，只需要从齐次状态方程和输出方程出发。</p><h4 id="5-2-1-约旦标准型判别能观性">5.2.1 约旦标准型判别能观性</h4><p>系统有如下形式：<br>$$<br>\dot{x}=Ax\<br>y=Cx<br>$$<br>其中$x(t_0)=x_0$。</p><ol><li>$A$为对角矩阵<br>$$<br>A=\Lambda=\left[\begin{array}{llll}<br>\lambda_{1} &amp; &amp; &amp; 0 \<br>&amp; \lambda_{2} &amp; &amp; \<br>&amp; &amp; \ddots &amp; \<br>0 &amp; &amp; &amp; \lambda_{n}<br>\end{array}\right],<br>C=\begin{bmatrix}<br>c_{11}&amp;c_{12}&amp;\cdots&amp;c_{1n}\<br>c_{21}&amp;c_{22}&amp;\cdots&amp;c_{2n}\<br>\vdots&amp;\vdots&amp;\ddots&amp;\vdots\<br>c_{m1}&amp;c_{m2}&amp;\cdots&amp;c_{mn}<br>\end{bmatrix}<br>$$<br>在系统矩阵$A$为对角型的情况下，系统能观的充要条件是输出矩阵$C$中没有全为零的列。如果第$i$列全为零，则与之相对应的$x_i$是不能观的。</li><li>$A$为约旦标准型矩阵，以三阶为例<br>$$<br>A=J=\begin{bmatrix}<br>\lambda_1&amp;1&amp;0\<br>0&amp;\lambda_1&amp;1\<br>0&amp;0&amp;\lambda_1<br>\end{bmatrix},<br>C=\begin{bmatrix}<br>c_{11}&amp;c_{12}&amp;c_{13}\<br>c_{21}&amp;c_{22}&amp;c_{23}\<br>c_{31}&amp;c_{32}&amp;c_{33}<br>\end{bmatrix}<br>$$<br>充要条件是矩阵$C$中，对应每个约旦块开头的一列的元素不全为零。</li><li>$A$为任意矩阵<br>将矩阵$A$经过$T^{-1}AT$变换得到对角型或约旦型，再判别对应$CT$的列是否为零来确定系统的能观性。</li></ol><h4 id="5-2-2-通过-A、C-判断系统的能观性">5.2.2 通过$A、C$判断系统的能观性</h4><p>完全能观的充要条件是矩阵$N=\begin{bmatrix}<br>C\CA\\vdots\CA^{n-1}<br>\end{bmatrix}$的秩为$n$。</p><h3 id="5-3-能控与能观的对偶关系">5.3 能控与能观的对偶关系</h3><p>通过卡尔曼对偶原理，可以确定系统能控性和能观性之间存在内在的对偶关系。利用这种对偶关系可以将系统能控性分析转化为能观性分析，从而可以将最优控制问题与最优估计问题联系起来。</p><h4 id="5-3-1-从矩阵结构了解对偶系统">5.3.1 从矩阵结构了解对偶系统</h4><p>现存在两个系统$\Sigma_1=\left{\begin{aligned}<br>\dot x_1&amp;=A_1B_1u_1\<br>y_1&amp;=C_1x_1<br>\end{aligned}\right.$和$\Sigma_2=\left{\begin{aligned}<br>\dot x_2&amp;=A_2x_2+B_2u_2\<br>y_2&amp;=C_2x_2<br>\end{aligned}\right.$<br>其中：<br>$x_1,x_2$为$n$维状态矢量<br>$u_1,u_2$分别为$r$维与$m$维控制矢量<br>$y_1,y_2$分别为$m$维与$r$维输出矢量<br>$A_1,A_2$为$n\times n$系统矩阵<br>$B_1,B_2$分别为$n\times r$与$n\times m$控制矩阵<br>$C_1,C_2$分别为$m\times n$与$r\times n$输出矩阵<br>若满足$A_2=A_1^T,B_2=C_1^T,C_2=B_1^T$，则$\Sigma_1$与$\Sigma_2$互为对偶。显然，$\Sigma_1$是一个$r$维输入$m$维输出的$n$阶系统，其对偶系统$\Sigma_2$是一个$m$维输入$r$维输出的$n$阶系统。</p><h4 id="5-3-2-从结构图了解对偶系统">5.3.2 从结构图了解对偶系统</h4><p>$\Sigma_1$和$\Sigma_2$的方块结构图如<a href="#fig8">图8</a>所示<br><span id="fig8"></span></p><center><img src="./ModernControlTheory/对偶系统.png">图8</center>我们可以看到，互为对偶的两系统有如下性质：  <ol><li>输入端与输出端互换</li><li>信号传递方向相反</li><li>信号引出点和综合点互换</li><li>对应矩阵转置</li></ol><h4 id="5-3-3-从传递函数矩阵来看对偶系统的关系">5.3.3 从传递函数矩阵来看对偶系统的关系</h4><p>传递函数矩阵$W_1(s)$为$m\times r$矩阵<br>$$<br>W_1(s)=C_1(sI-A_1)^{-1}B_1<br>$$<br>传递函数矩阵$W_2(s)$为$r\times m$矩阵<br>$$<br>\begin{aligned}<br>&amp;W_2(s)\<br>=&amp;C_2(sI-A_2)^{-1}B_2\<br>=&amp;B_1^T(sI-A_1^T)^{-1}C_1^T\<br>=&amp;B_1^T[(sI-A_1)^{-1}]^TC_1^T<br>\end{aligned}<br>$$<br>我们可以得到$W_2^T(s)=W_1(s)$，对偶系统的传递函数矩阵互为转置。</p><h4 id="5-3-4-对偶系统的性质">5.3.4 对偶系统的性质</h4><p>若系统$\Sigma_1$与系统$\Sigma_2$互为对偶，则有$\left{<br>\begin{aligned}<br>\Sigma_1 完全能控\Leftrightarrow \Sigma_2 完全能观\<br>\Sigma_1 完全能观\Leftrightarrow \Sigma_2 完全能控<br>\end{aligned}\right.<br>$<br>证明：<br>对于系统$\Sigma_2$而言，能控性判别矩阵$(n\times m)$<br>$$<br>M_2=\begin{bmatrix}<br>B_2&amp;A_2B_2\cdots A_2^{n-1}B_2<br>\end{bmatrix}<br>$$<br>$M_2$的秩为$n$，此时系统状态为完全能控的。<br>又$A_2=A_1^T,B_2=C_1^T,C_2=B_1^T$，则有<br>$$M_2=\begin{bmatrix}<br>C_1^T&amp;A_1^TC_1^T&amp;\cdots&amp;(A_1^T)^{n-1}C_1^T<br>\end{bmatrix}=N_1^T$$<br>说明当$\Sigma_2$能控时，系统$\Sigma_1$是能观的。同理，有<br>$$<br>\begin{aligned}<br>&amp;N_2^T\<br>=&amp;\begin{bmatrix}<br>C_2^T&amp;A_2^TC_2^T&amp;\cdots&amp;(A_2^T)^{n-1}C_2^T<br>\end{bmatrix}\<br>=&amp;\begin{bmatrix}<br>B_1&amp;A_1B_1&amp;\cdots&amp;A_1^{n-1}B_1<br>\end{bmatrix}\<br>=&amp;M_1<br>\end{aligned}<br>$$<br>因此，$\Sigma_2$能观时$\Sigma_1$是能控的</p><h3 id="5-4-标准型">5.4 标准型</h3><p>由于状态变量的选择非唯一，因此状态空间的表达式也不唯一，根据研究需要，我们可以将状态空间表达式化成三种标准形式：</p><ol><li>约旦标准型。约旦标准型主要用于求状态转移矩阵$\Phi(t)=e^{At}$，还可以用来分析可控性和可观性。</li><li>能控标准型。能控标准型主要用于研究系统的状态反馈。</li><li>能观标准型。能观标准型主要用于状态观测器的设计和系统辨识。</li></ol><p>状态空间表达式能够转换成能控标准型和能观标准型的理论依据是非奇异变换不改变系统的能控型和能观性。但是转换成能控标准型需要系统本身状态完全能控，同理转换成能观标准型需要系统本身状态完全能观。</p><h4 id="5-4-1-单输入系统的能控标准型">5.4.1 单输入系统的能控标准型</h4><p>转换成能控标准型，首先需要判断系统的系统能控性。对于一般$n$维定常系统$\left{\begin{aligned}<br>\dot x&amp;=Ax+Bu\<br>y&amp;=Cx<br>\end{aligned}\right.$,系统能控的要求为rank$\begin{bmatrix}<br>B&amp;AB&amp;A^{n-1}B<br>\end{bmatrix}=n$<br>对于多输入多输出系统，我们可以在$M$中$nr$个列矢量中选取一组$n$个线性无关的列矢量，此时能控标准型不唯一。<br>对于单输入单输出系统，判别矩阵$M$只有$n$个列矢量，因此当系统能控时线性无关矢量的组合是唯一的，此时能控标准型的形式是唯一的。</p><p>我们主要研究单输入单输出系统的两种能控标准型。</p><h5 id="能控标准-mathrm-I-型">能控标准$\mathrm{I}$型</h5><p>若线性定常系统$\left{\begin{aligned}<br>\dot x&amp;=Ax+bu\<br>y&amp;=cx<br>\end{aligned}\right.$是能控的，则存在线性奇异变换$x=T_{c1}\bar x$，其中<br>$$<br>T_{c1}<br>=\begin{bmatrix}<br>A^{n-1} b &amp; A^{n-2} b &amp; \cdots &amp; b<br>\end{bmatrix} \cdot\left[\begin{array}{ccccc}<br>1 &amp; &amp; &amp; &amp; 0 \<br>\alpha_{n-1} &amp; 1 &amp; &amp; &amp; \<br>\vdots &amp; &amp; \ddots &amp; &amp; \<br>\alpha_2 &amp; \alpha_3 &amp; &amp; \ddots &amp; \<br>\alpha_1 &amp; \alpha_2 &amp; \cdots &amp; \alpha_{n-1} &amp; 1<br>\end{array}\right]<br>$$<br>其中$\alpha_i$为特征多项式的各项系数，特征多项式为<br>$$|\lambda I-A|=\lambda^n+\alpha_{n-1}\lambda^{n-1}+\cdots+\alpha_1\lambda+\alpha_0$$<br>此时，状态空间表达式化成$\left{\begin{aligned}<br>\dot{\bar{x}}&amp;=\bar A\bar x+b\bar u\<br>y&amp;=\bar{cx}<br>\end{aligned}\right.$其中：<br>线性系统中的状态矩阵<br>$$<br>\bar{A}=T_{c 1}^{-1} A T_{c 1}=<br>\left[\begin{array}{ccccc}<br>0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \<br>0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \<br>\vdots&amp;\vdots &amp; \cdots &amp; \ddots &amp; \vdots\<br>0 &amp; 0 &amp; \cdots &amp; 0 &amp; 1 \<br>-\alpha_0 &amp; -\alpha_1 &amp; \cdots &amp; -\alpha_{n-2} &amp; -\alpha_{n-1}<br>\end{array}\right],$$<br>控制矩阵$<br>\bar{b}=T_{c 1}^{-1} b=\left[\begin{array}{c}<br>0 \<br>0 \<br>\vdots \<br>0 \<br>1<br>\end{array}\right],<br>$<br>输出矩阵$\bar{c}=c T_{c 1}=\left[\begin{array}{llll}<br>\beta_0 &amp; \beta_1 &amp; \cdots &amp; \beta_{n-1}<br>\end{array}\right]<br>$</p><p>此时系统的传递函数为:<br>$$<br>W(s)=\bar c(sI-A)^{-1}\bar b=\frac{\beta_{n-1}s^{n-1}+\beta_{n-2}s^{n-2}+\cdots +\beta_1 s+\beta_0}{s^nn+\alpha_{n-1}s^{n-1}+\cdots+\alpha_1s+\alpha_0}<br>$$</p><p>其中分子的系数为$\bar c中的元素$，分母中的元素为$\bar A$中最后一行的元素的负值。</p><h5 id="能控标准-mathrm-II-型">能控标准$\mathrm{II}$型</h5><p>若线性定常系统$\left{\begin{aligned}<br>\dot x&amp;=Ax+bu\<br>y&amp;=cx<br>\end{aligned}\right.$是能控的，则存在线性奇异变换<br>$$<br>x=T_{c2}\bar x<br>=\begin{bmatrix}<br>b &amp; Ab &amp; \cdots  &amp; A^{n-1} b<br>\end{bmatrix} \cdot \bar{x}<br>$$<br>相应的状态表达式为$\left{\begin{aligned}<br>\bar{\dot x}&amp;=\bar A\bar x+\bar bu\<br>y&amp;=\bar{cx}<br>\end{aligned}\right.$</p><p>其中：<br>线性变换后的状态矩阵<br>$$<br>\bar{A}=T_{c 2}^{-1} A T_{c 2}=\left[\begin{array}{ccccc}<br>0 &amp; 0 &amp; \cdots &amp; 0 &amp; -\alpha_0 \<br>1 &amp; 0 &amp; \cdots &amp; 0 &amp; -\alpha_1 \<br>0 &amp; 1 &amp; \cdots &amp; 0 &amp; -\alpha_2 \<br>\vdots &amp; \vdots &amp; \cdots &amp; \vdots &amp; \vdots \<br>0 &amp; 0 &amp; \cdots &amp; 1 &amp; -\alpha_{n-1}<br>\end{array}\right],\<br>$$<br>控制矩阵$<br>\bar{b}=T_{c 2}^{-1} b=\left[\begin{array}{c}<br>1 \<br>0 \<br>\vdots \<br>0 \<br>0<br>\end{array}\right],$<br>输出矩阵$\bar{c}=c T_{c 2}=\left[\begin{array}{llll}<br>\beta_0 &amp; \beta_1 &amp; \cdots &amp; \beta_{n-1}<br>\end{array}\right]<br>$</p><h4 id="5-4-2-单输出系统的能观标准型">5.4.2 单输出系统的能观标准型</h4><h5 id="能观标准-mathrm-I-型">能观标准$\mathrm{I}$型</h5><p>若线性定常系统$\left{\begin{aligned}<br>\dot x&amp;=Ax+bu\<br>y&amp;=cx<br>\end{aligned}\right.$是能观的，则存在非奇异变换$x=T_{01}\tilde{x}$使得状态空间表达式转换为$\left{\begin{aligned}<br>\dot{\tilde x}&amp;=A\tilde x+\tilde bu\<br>y&amp;=\tilde c\tilde x<br>\end{aligned}\right.$，其中：<br>线性系统中的状态矩阵<br>$$<br>\tilde{A}=T_{01}^{-1} A T_{01}=\left[\begin{array}{ccccc}<br>0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \<br>0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \<br>0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \<br>\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \<br>-\alpha_0 &amp; -\alpha_1 &amp; -\alpha_2 &amp; \cdots &amp; -\alpha_{n-1}<br>\end{array}\right],\$$<br>控制矩阵$\tilde { b } = T <em>{ 01 } ^ { - 1 } b = \left[ \begin{array} { c } { \beta _ { 0 } } \ { \beta _ { 1 } } \ { \vdots } \ { \beta _ { n \cdot 1 } } \end{array} \right],$<br>输出矩阵$\tilde { c } = c T</em> { 01 } = \left[ \begin{array} { l l l l l } { 1 } &amp; { 0 } &amp; { 0 } &amp; { \cdots } &amp; { 0 } \end{array} \right]<br>$</p><h5 id="能观标准-mathrm-II-型">能观标准$\mathrm{II}$型</h5><p>若线性定常系统$\left{\begin{aligned}<br>\dot x&amp;=Ax+bu\<br>y&amp;=cx<br>\end{aligned}\right.$是能观的，则存在非奇异变换$x=T_{02}\tilde{x}$使得状态空间表达式转换为$\left{\begin{aligned}<br>\dot{\tilde x}&amp;=A\tilde x+\tilde bu\<br>y&amp;=\tilde c\tilde x<br>\end{aligned}\right.$，其中：<br>线性变换后的状态矩阵<br>$$<br>A= \left[ \begin{array} { c c c c c } { 0 } &amp; { 0 } &amp; { \cdots } &amp; { 0 } &amp; { - \alpha _ { 0 } } \ { 1 } &amp; { 0 } &amp; { \cdots } &amp; { 0 } &amp; { - \alpha _ { 1 } } \ { 0 } &amp; { 1 } &amp; { \cdots } &amp; { 0 } &amp; { - \alpha _ { 2 } } \ { \vdots } &amp; { \vdots } &amp; { \cdots } &amp; { \vdots } &amp; { \vdots } \ { 0 } &amp; { 0 } &amp; { \cdots } &amp; { 1 } &amp; { - \alpha _ { n - 1 } } \end{array} \right],\<br>$$<br>变换矩阵<br>$$<br>T_ { 02 } ^ { - 1 } = \left[ \begin{array} { c c c c c } { 1 } &amp; { \alpha _ { n - 1 } } &amp; { \cdots } &amp; { \alpha _ { 2 } } &amp; { \alpha _ { 1 } } \ { 0 } &amp; { 1 } &amp; { \cdots } &amp; { \alpha _ { 3 } } &amp; { \alpha _ { 2 } } \ { \vdots } &amp; { \vdots } &amp; { \cdots } &amp; { \vdots } &amp; { \vdots } \ { 0 } &amp; { 0 } &amp; { \cdots } &amp; { 1 } &amp; { \alpha _ { n - 1 } } \ { 0 } &amp; { 0 } &amp; { \cdots } &amp; { 0 } &amp; { 1 } \end{array} \right] \cdot \left[ \begin{array} { c } { c A ^ { n - 1 } } \ { c A ^ { n - 2 } } \ { \vdots } \ { c A } \ { c } \end{array} \right],\<br>$$<br>控制矩阵$<br>\tilde { b } = T <em>{ 02 } ^ { - 1 } b = \left[ \begin{array} { c } { \beta _ { 0 } } \ { \beta _ { 1 } } \ { \vdots } \ { \beta _ { n - 1 } } \end{array} \right],$<br>输出矩阵$\tilde c=cT</em>{02}=\begin{bmatrix}<br>0&amp;0&amp;0&amp;\cdots&amp;1<br>\end{bmatrix}$</p><h2 id="6-李雅普诺夫稳定">6 李雅普诺夫稳定</h2><p>设所研究系统的非齐次方程为$\dot x=f(x,t)$一般为时变非线性函数，当方程中不显含$x$时，则为定常非线性系统。<br>设该方程在给定初始条件$(x_0,t_0)$下有唯一解：$x=\Phi(t;x_0,t_0)$。其中$x_0=\Phi(t_0;x_0,t_0)$为$x_0$在初始时刻$t_0$时的状态，而$t$是从$t_0$开始观察的时间变量，由此我们可以描述从初始条件$(t_0,x_0)$出发的一条状态运动的轨迹。</p><h3 id="6-1-平衡状态">6.1 平衡状态</h3><p>若存在矢量$x_e$，使得$\forall t,f(x_e,t)=0$，则称$x_e$为平衡状态。而由于平衡状态的各分量不随时间变化，若已知状态方程，则令$\dot x=0$,求得解$x$即为平衡状态。<br>值得注意的是，对于任意一个系统而言，不一定存在平衡点，即使存在，也不一定是唯一的。并且由于任意一个已知的平衡状态，都可以通过坐标变换将其移动到坐标原点，因此我们在讨论平衡点或稳定性时，可以仅讨论系统在坐标原点的平衡点。</p><h3 id="6-2-稳定性">6.2 稳定性</h3><h4 id="6-2-1-相关的基本概念">6.2.1 相关的基本概念</h4><p>欧几里得范数：在$n$维状态空间中，欧几里得范数可表示为$||x-x_e||=[(x_1-x_{e1})^2+(x_2-x_{e2})^2+\cdots+(x_n+x){en}^2]^{\frac12}$</p><p>距离：若用$||x-x_e||$表示状态矢量$x$与平衡状态$x_e$的距离，用点集$s(\varepsilon)$表示以$x_e$为中心$\varepsilon$为半径的超球体，那么$x\in \varepsilon$可以表示为$||x-x_e||&lt;&lt;\varepsilon$。</p><p>领域：当$\varepsilon$足够小，即可称$s(\varepsilon)$为$x_e$的领域。因此若有$x_0\in s(\delta)$，则意味着初始偏差$||x_0-x_e||&lt;&lt;\delta$。同理，若状态空间方程的解$\Phi(t;x_0,t_0)$位于球形领域$s(\varepsilon)$内，便有平衡状态偏差$||\Phi(t;x_0,t_0)-x_e||&lt;&lt;\varepsilon,t\geq t_0$。因此在系统中初态$x_0$或短暂扰动引起的自由响应应是有界的。</p><h4 id="6-2-2-稳定性的定义">6.2.2 稳定性的定义</h4><p>系统稳定性是指系统的输出是否在输入变化时保持相对稳定，通常是指系统的本振响应（输入和输出之间的时间关系）是否稳定，如果系统的本振响应稳定，则系统可以通过控制参数来抑制输入变化对输出的影响，从而保持系统的稳定性。<br><a href="#fig9">图9</a>直观地展示了系统的稳定性与范围定义。<br><span id="fig9"></span></p><center><img src="./ModernControlTheory/稳定.png"/>图9</center><p>系统的稳定性可以分为以下几类：李雅普诺夫意义下的稳定、渐近稳定、大范围渐近稳定以及不稳定。</p><h4 id="6-2-3-李雅普诺夫稳定">6.2.3 李雅普诺夫稳定</h4><p>李亚普诺夫稳定如<a href="#fig10">图10</a>所示<br><span id="fig10"></span></p><center><img src="./ModernControlTheory/李雅普诺夫稳定.png">图10</center><p>如果系统对任意选定的实数$\varepsilon&gt;0$都对应存在另一个实数$\delta(\varepsilon,t_0)&gt;0$时，从任意初始状态$x_0$出发的解都满足$||x(t)-x_e||\leq \varepsilon,t_0&lt;t&lt;\infin$，则称平衡状态$x_e$为李雅普诺夫意义下的稳定。<br>实数$\delta$与$\varepsilon$有关，一般情况下也与$t_0$有关。当实数$\delta$与$\varepsilon$有关而与$t_0$无关时，我们称平衡状态$x_e$为一致稳定。</p><h4 id="6-2-4-渐近稳定">6.2.4 渐近稳定</h4><p>渐近稳定如<a href="#fig11">图11</a>所示。<br><span id="fig11"></span></p><center><img src="./ModernControlTheory/渐近稳定.png">图11</center><p>当平衡状态$x_e$稳定，且当$t$无限增长时，轨线不仅不超出$s(\varepsilon)$，最终还收敛于$x_e$，则称平衡状态$x_e$是渐近稳定的。</p><h4 id="6-2-5-大范围渐近稳定">6.2.5 大范围渐近稳定</h4><p>大范围渐近稳定相比于渐近稳定的区别如<a href="#fig12">图12</a>所示。<br><span id="fig12"></span></p><center><img src="./ModernControlTheory/大范围渐近稳定.png">图12</center><p>当平衡状态$x_e$是稳定的,且状态空间中所有初始状态出发的轨线都具有渐近稳定性,就可以称平衡状态$x_e$是大范围渐近稳定的</p><h4 id="6-2-6-不稳定">6.2.6 不稳定</h4><p>不稳定状态如<a href="#fig13">图13</a>所示<br><span id="fig13"></span></p><center><img src="./ModernControlTheory/不稳定.png">图13</center><p>若$\varepsilon&gt;0,\delta&gt;0$,无论$\delta$多么小,只要$s(\delta)$内出发的状态轨线至少有一个轨线越过$s(\varepsilon)$，则称平衡状态$x_e$不稳定。</p><h3 id="6-3-线性系统的稳定判据">6.3 线性系统的稳定判据</h3><p>有线性定常系统$\Sigma:(A,b,c){\begin{aligned}<br>\dot x&amp;=Ax+bu\<br>y&amp;=cx<br>\end{aligned}$，对于平衡状态$x_e=0$渐近稳定的充要条件为矩阵$A$的所有特征根均具有负实部，此时系统为状态稳定。</p><p>当系统对于有界输入$u$所引起的输出$y$有界时，系统为输出稳定，线性定常系统输出稳定的充要条件是传递函数的极点全部位于$s$的左半平面。</p><p>状态不稳定时，输出不一定不稳定，但是当系统的传递函数不出现零极对消的现象，且$A$的特征值和传递函数的极点相同时，状态稳定与输出稳定是一致的。</p><h3 id="6-4-非线性系统的稳定性判据">6.4 非线性系统的稳定性判据</h3><h4 id="6-4-1-间接法（间接法）判断">6.4.1 间接法（间接法）判断</h4><p>令系统的状态方程为：$\dot x=f(x,t)$，满足：</p><ol><li>$f(x,t)$对$x$具有连续的偏导数</li><li>$f(x,t)$为与$x$同维的矢量函数（函数矩阵）</li><li>$x_e$为平衡状态</li></ol><p>此时我们讨论系统在$x_e$处的稳定性。我们先将线性矢量函数$f(x,t)$在$x_e$领域内进行泰勒展开。<br>$$<br>\dot x=\dot x_e+\frac{\partial f(x,t)}{\partial x}(x-x_e)+\underbrace{\mathbf{R}(x)}<em>{高阶导数项}<br>$$<br>令$\Delta x=x-x_e$，则$\dot x-\dot x_e=\frac{\partial f(x,t)}{\partial x}(x-x_e)+\mathbf{R}$，可得$\Delta \dot x=A\Delta x$，其中雅各比矩阵<br>$$<br>A=\frac{\partial f}{\partial x}|</em>{x=x_e}<br>=\left[\begin{array}{cccc}<br>\frac{\partial f_{1}}{\partial x_{e1}} &amp; \frac{\partial f_{1}}{\partial x_{e2}} &amp; \cdots &amp; \frac{\partial f_{1}}{\partial x_{en}} \<br>\frac{\partial f_{2}}{\partial x_{e1}} &amp; \frac{\partial f_{2}}{\partial x_{e2}} &amp; \cdots &amp; \frac{\partial f_{2}}{\partial x_{en}} \<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \<br>\frac{\partial f_{n}}{\partial x_{e1}} &amp; \frac{\partial f_{n}}{\partial x_{e2}} &amp; \cdots &amp; \frac{\partial f_{n}}{\partial x_{en}}<br>\end{array}\right]<br>$$</p><p>非线性系统的稳定性根据$A$的特征值进行判断：</p><ol><li>$A$的特征值都具有负实部时，系统在平衡状态$x_e$处渐近稳定，其稳定性与$\mathbf{R}(x)$无关。</li><li>$A$的特征值至少有一个具有正实部时，在$x_e$处不稳定。</li><li>$A$的特征值至少有一个的实部为零时，此时处于临界状态，稳定性与$\mathbf{R}(x)$有关。</li></ol><h4 id="6-4-2-第二法（直接法）判断">6.4.2 第二法（直接法）判断</h4><p>从系统的能量角度出发，借助李雅普诺夫函数直接判断系统的平衡状态稳定性。当一个系统被激励后，其存储的能量随着时间的推移逐渐衰减，达到平衡状态时，能量达到最小值，这个平衡状态是渐近稳定的。反之，不断从外界吸收能量，储能越来越大，那么这个平衡状态是不稳定的。</p><p>李雅普诺夫函数是一个正定的标量函数，也是一个虚构的广义能量函数，它通过能量函数对时间的导数的符号来判断稳定性。<br>令系统的状态方程为$\dot x=f(x)$，平衡状态为$x_e=0$，满足$f(x_e)=0$，如果存在一个标量函数$V(x)$满足：</p><ol><li>$V(x)$对所有$x$都具有连续的一阶偏导数。</li><li>$V(x)$是正定的，即$\left{\begin{aligned}<br>x=0,V(x)=0\<br>x\neq 0,V(x)&gt;0<br>\end{aligned}\right.$</li></ol><p>就可以根据$\dot V(x)$判断系统的稳定性</p><ol><li><p>$\dot V(x)$为半负定，则$x_e$在李雅普诺夫意义下稳定。（稳定判据）</p></li><li><p>$\dot V(x)$为负定，则$x_e$为渐近稳定<br>$<br>\left.<br>\begin{aligned}<br>\left.<br>\begin{array}{l}<br>\dot{V}(x) \text { 为半负定 } \<br>x\left(t_{0}\right) \neq 0 \<br>x \neq 0 \Rightarrow \dot{V}(x) \not \equiv 0<br>\end{array}\right}\Rightarrow x_e渐近稳定\<br>||x||\rightarrow\infty\Rightarrow V(x)\rightarrow\infty<br>\end{aligned}<br>\right}\Rightarrow大范围渐近稳定<br>$</p></li><li><p>$\dot V(x)为正定$，系统不稳定</p></li></ol><p>李雅普诺夫步骤如下：</p><ol><li>构造$V(x)$二次型。</li><li>求$\dot V(x)$，并代入状态方程。</li><li>判断$V(x)$定号性。</li><li>判断非零情况下，$\dot V[x(t;t_0,x_0)]$是否恒为零。</li></ol><p>令$\dot V(x,t)\equiv 0$</p><ol><li>若$x\neq 0,\dot V(x,t)$成立，则在李雅普诺夫意义下稳定。</li><li>若仅$x=0,\dot V(x,t)\equiv0$成立，则为渐近稳定。</li></ol><p>李雅普诺夫函数第二法判断的依据是充分条件，如果能找到李雅普诺夫函数就能对系统做出肯定的判断，但是找不到李雅普诺夫函数无法做出否定的结论。除此之外，在李雅普诺夫函数的细节上我们有如下讨论：</p><ol><li>$V(x)$是正定的标量函数，且对$x$应具有连续的一阶偏导数。</li><li>对于一个给定的系统，$V(x)$是可以找到的，通常非唯一，但不影响结论的一致性。</li><li>$V(x)$的最简单形式是二次型函数，但不一定都是简单的二次型。</li><li>如果$V(x)$的最简单形式可以表示成标准二次型，$V(x)$就表示从远点到$x$的距离。其中$V(x)$表征了系统相对原点的速度。</li><li>$V(x)$函数只是表示系统在平衡点附近某领域内局部运动的稳定情况，丝毫不提供域外运动的任何信息。</li><li>由于构造$V(x)$函数的过程较为复杂，因此一般在用其他方法无效或难以判别的问题时才会考虑李雅普诺夫第二法。</li></ol><h2 id="7-线性反馈控制系统">7 线性反馈控制系统</h2><p>线性反馈控制系统的基本结构是由受控对象和反馈控制器两部分构成的闭环系统，与经典控制理论的区别是在经典控制理论中的反馈是输出反馈，而现代控制理论中的反馈是状态反馈，相对而言能提供更丰富的状态信息和可供选择的自由度，使得系统拥有更优异的性能。</p><h3 id="7-1-状态反馈">7.1 状态反馈</h3><p>状态反馈的框图如<a href="#fig14">图14</a>所示。<br><span id="fig14"></span></p><center><img src="./ModernControlTheory/状态反馈.png"/>图14</center><p>反馈系统是将系统的每一个状态变量乘以相应的反馈系数并且反馈到输入端，与参考输入相加形成控制律作为受控系统的控制输入。图中受控系统的状态空间表达式为：<br>$$<br>\left{<br>\begin{aligned}<br>\dot x=Ax+Bu\quad x\in \mathbf{R}^n;u\in \mathbf{R}^r;y\in \mathbf{R}^m\<br>y=Cx+Du\quad A_{n\times n},B_{n\times r},C_{m\times n},D_{m\times r}<br>\end{aligned}<br>\right.<br>$$<br>若$D=0$，则受控系统$<br>\left{<br>\begin{aligned}<br>\dot x&amp;=Ax+Bu\y&amp;=Cx<br>\end{aligned}<br>\right.<br>$简记为$\Sigma_0=(A,B,C)$。<br>状态控制律为$u=Kx+v$。其中$r\times n$维矩阵$K$为状态反馈矩阵，$r\times 1$维矩阵$v$为参考输入。<br>将$U=Kx+v$代入系统方程，整理可得<br>$$<br>\left{<br>\begin{aligned}<br>\dot x&amp;=(A+BK)x+Bv\y&amp;=(C+DK)x+Dv<br>\end{aligned}<br>\right.<br>$$<br>若$D=0$，则上式为$<br>\left{<br>\begin{aligned}<br>\dot x&amp;=(A+BK)x+Bv\y&amp;=Cx<br>\end{aligned}<br>\right.<br>$<br>简记作<br>$$<br>\Sigma_K=((A+BK),B,C)<br>$$<br>则闭环系统的传递函数矩阵为<br>$$<br>W_k(s)=C\left[sI-(A+BK)\right]^{-1}B<br>$$</p><p>相比于开环系统，$K$的引入并不增加系统的维数，并且可以通过$K$的选择自由改变闭环系统的特征值，使得系统获得所要求的性能。</p><h3 id="7-2-输出反馈">7.2 输出反馈</h3><p>输出反馈的系统框图如<a href="#fig15">图15</a>所示。<br><span id="fig15"></span></p><center><img src="./ModernControlTheory/输出反馈.png"/>图15</center><p>采用输出矢量$y$构成线性反馈律，将输出信号$y$乘上相应的反馈系数$H$反馈到输入$u$。<br>受控系统$\Sigma_0=(A,B,C,D)\left{\begin{aligned}<br>\dot x=Ax+Bu\y=Cx+Du<br>\end{aligned}<br>\right.$<br>则输入<br>$$<br>\begin{aligned}<br>u&amp;=Hy+v\&amp;=H(Cx+Du)+v\&amp;=HCx+HDu+v\&amp;=(I-HD)^{-1}(HCx+v)<br>\end{aligned}<br>$$<br>则得到新的状态空间表达式<br>$$<br>\left{<br>\begin{aligned}<br>\dot x&amp;=[A+B(I-HD)^{-1}HC]x+B(I-HD)^{-1}v\<br>y&amp;=[C+D(1-HD)^{-1}HC]x+D(1-HD)^{-1}v<br>\end{aligned}<br>\right.<br>$$<br>当$D=0$时，上式简化为<br>$$<br>\left{<br>\begin{aligned}<br>\dot x&amp;=[A+BHC]x+Bv\<br>y&amp;=Cx<br>\end{aligned}<br>\right.<br>$$<br>简记为$\Sigma_H=[(A+BHC),B,C]$。<br>输出反馈系统的传递函数阵为<br>$$<br>W_H(s)=C\left[sI-(A+BHC)\right]^{-1}B<br>$$<br>与开环系统相比，我们引入了反馈增益矩阵$H$，改变了闭环系统的特征值，系统的控制特性也由此发生改变。</p><h3 id="7-3-两种反馈的对比">7.3 两种反馈的对比</h3><ol><li>输出反馈中的$HC$与状态反馈中的$K$相当。</li><li>输出反馈只能相当于一部分状态反馈。$\Leftarrow m&lt;n，H$可供选择的自由度远比$K$小。</li><li>只有当$C=I$时，$HC=K$，才能等同于全状态反馈。</li><li>在不添加补偿器的条件下，输出反馈的效果显然不如状态反馈的系统性能好。</li><li>输出反馈在技术实现上的方便性是其突出优点。</li></ol><h4 id="7-3-1-从输出到状态矢量导数-dot-x-反馈">7.3.1 从输出到状态矢量导数$\dot x$反馈</h4><p>设受控系统$\Sigma_0=(A,B,C,D)\left{\begin{aligned}<br>\dot x=Ax+Bu\y=Cx+Du<br>\end{aligned}\right.$加入从输出$y$到状态矢量导数$\dot x$的反馈增益矩阵$G$得到<br>$$<br>\left{<br>\begin{aligned}<br>\dot x&amp;=Ax+Bu+Gy\<br>y&amp;=Cx+Du<br>\end{aligned}<br>\right.\Rightarrow<br>\left{<br>\begin{aligned}<br>\dot x&amp;=[A+GC]x+[B+GD]y\<br>y&amp;=Cx+Du<br>\end{aligned}<br>\right.<br>$$<br>若$D=0$，则可以简化为<br>$$<br>\left{<br>\begin{aligned}<br>\dot x&amp;=[A+GC]x+Bu\<br>y&amp;=Cx<br>\end{aligned}<br>\right.<br>$$<br>记作$\Sigma_G=[(A+GC),B,C]$。闭环系统的传递函数阵为<br>$$<br>W_G(s)=C\left[sI-(A+GC)\right]^{-1}B<br>$$<br>我们可以通过选择矩阵$G$来改变闭环系统的特征值从而影响系统的特性。</p><h4 id="7-3-2-闭环系统的能控性和能观性">7.3.2 闭环系统的能控性和能观性</h4><p>有如下定理：</p><blockquote><p>对于状态反馈，系统的能控型不变，但是能观性会受到影响。<br>对于输出反馈，受控系统的能控型和能观性都不变。</p></blockquote><p>由于对单输入单输出进行状态反馈会改变系统的极点，但是不影响系统的零点，因此可能会导致传递函数出现零极点对消现象，从而破坏系统能观性。</p><p>对比开环系统和闭环系统的传递函数，开环系统的传递函数为<br>$$<br>W_0(s)=C\left[sI-A\right]^{-1}B=\frac{b_{n-1}s^{n-1}+b_{n-2}s^{n-2}+\cdots+b_0}{s^n+a_{n-1}s^{n-1}+\cdots+a_1s+a_0}<br>$$<br>闭环系统的传递函数为：<br>$$<br>\begin{aligned}<br>{W}<em>{k}(s)&amp;={c}[s {I}-({A}+{b K})]^{-1} {b}\&amp;=\frac{b</em>{n-1} s^{n-1}+\cdots+b_{1} s+b_{0}}{s^{n}+\left(a_{n-1}-k_{n-1}\right) s^{n-1}+\cdots+\left(a_{1}-k_{1}\right) s+\left(a_{0}-k_{0}\right)}<br>\end{aligned}<br>$$<br>分子多项式不变，分母每一项系数可通过选择$K$而改变，因此可能导致极点相消破坏其能观性。</p><h3 id="7-4-极点配置问题">7.4 极点配置问题</h3><p>对于控制系统而言，极点配置问题是非常重要的。极点在根平面上的分布对控制系统的性能有很大影响。因此，往往需要给定一组期望的极点，或根据时域指标转化成一组等价的期望极点。通过选择反馈控制矩阵，将闭环系统的极点配置在所期望的位置，可以获得所期望的动态性能。本文仅讨论单输入单输出系统中，在指定极点分布的情况下，如何选择反馈增益阵的问题。</p><p>我们可以采用状态反馈和输出反馈来配置极点。对于状态反馈，采用状态反馈对系统$\Sigma_0(A,b,c)$任意配置极点的充要条件是$\Sigma_0$完全能控。对于输出反馈，对于完全能控的单输入单输出系统$\Sigma_0(A,b,c)$不能采用输出线性反馈实现闭环系统极点的任意配置，而对系统$\Sigma_0(A,b,c)$采用从输出到$\dot x$的线性反馈实现闭环极点任意配置的充要条件是$\Sigma_0$完全能观。</p><h3 id="7-5-状态观测器">7.5 状态观测器</h3><p>当确定受控对象是可控的，利用状态反馈配置极点时，需用传感器测量出状态变量以形成反馈，但传感器通常用来测量输出，许多中间状态变量不易测得，于是我们可以构造一个状态观测器（状态估计器、重构器），利用输出量和输入量来重构不可测量的状态。</p><p><span id="fig16"></span></p><center><img src="./ModernControlTheory/状态观测器1.png"/>图16</center><p><span id="fig17"></span></p><center><img src="./ModernControlTheory/状态观测器2.png"/>图17</center><p><a href="#fig16">图16</a>$\Sigma_0$为一个可控系统，需要将状态$x$反馈给输入$u$，但是我们无法直接测得$x$，只能测得$y$，因此如<a href="#fig17">图17</a>所示，我们采用状态观测器$\hat\Sigma$，通过输入量$u$和输出量$y$作为其输入量，重构出状态变量$\hat x$，使得$\lim |x-\hat x|=0$，然后将$\hat x$代替$x$反馈给$u$。</p><p>构造观测器应遵循以下原则：</p><ol><li>观测器$\hat \Sigma$的输入量是$\Sigma_0$的输入$u$和输出$y$。</li><li>必须保证$\Sigma_0$广义上能观，即系统整体完全能观，或其子系统中不能观的部分渐近稳定，才能满足$\lim|x-\hat x|=0$。</li><li>$\hat \Sigma$有足够宽的频带才能使得$\hat \Sigma$的输出$\hat x$能够以足够快的速度渐近$x$，但频带过宽也会导致抗干扰性变弱，因此要具体问题具体对待，根据实际情况进行取舍。</li><li>$\hat\Sigma$在结构上的设计以简单为目标，以便于工程实现或物理实现，例如其维数可以尽可能低。</li></ol><p>我们从简单到复杂，从工程上容易实现到不容易实现介绍三种观测器。</p><h4 id="7-5-1-基本观测器">7.5.1 基本观测器</h4><p>若线性定常系统$\Sigma_0=(A,B,C)$完全能观，则其状态矢量$x$可由输入$u$和输出$y$进行重构。证明过程如下：</p><ol><li>将输出方程对$t$逐次求导得到<br>$$<br>\left{<br>\begin{array}{l}<br>y=C x \<br>\dot{y}-C B u=C A x \<br>\ddot{y}-C B \dot{u}-C A B u=C A^{2} x \<br>\vdots \<br>y^{(n-1)}-C B u^{(n-2)}-C A B u^{(n-3)}-\cdots-C A^{(n-2)} B u=C A^{(n-1)} x \<br>\end{array}<br>\right.<br>$$</li><li>将各式左边用矢量表示，得到：<br>$$<br>z=\begin{bmatrix}<br>z_1 \z_2\ \vdots \z_{n}<br>\end{bmatrix}=\begin{bmatrix}<br>y\\dot{y}-C B u\\vdots\y^{(n-1)}-C B u^{(n-2)}-\cdots-C A^{(n-2)} B u<br>\end{bmatrix}=\begin{bmatrix}<br>C\CA\\vdots\CA^{(n-1)}<br>\end{bmatrix}\cdot x<br>$$<br>若系统完全能观，则rank $N=n$，则$x=N^{-1}z$</li><li>由此可以构造新的系统$z$，其输入量为$u$和$y$，输出量为$z$，再根据$z$经过$N^{-1}$变换，求得$x=N^{-1}z$。系统框图如<a href="#fig18">图18</a>所示。<br><span id="fig18"></span></li></ol><center><img src="./ModernControlTheory/状态观测器系统框图.png"/><p>图18</p></center><h4 id="7-5-2-开环观测器">7.5.2 开环观测器</h4><p>这种观测器是有缺点的，系统$z$中包含$0$阶到$n-1$阶的微分器会大大加剧测量噪声对于状态估值的影响，因此这种观测器并没有工程价值。针对以上缺点我们提出如下对策：仿照系统$\Sigma_0=(A,B,C)$的结构设计一个相同的系统来观测状态$x$。</p><p><span id="fig19"></span></p><center><img src="./ModernControlTheory/开环观测器.png"/><p>图19</p></center><p>如<a href="#fig19">图19</a>所示，即为开环观测器，上半部分是原系统$\Sigma_0$，下半部分是一个相同的系统来观测状态$x$。但是这种观测器也存在不足：显然，如果需要使得观测器的输出$\hat x$严格等于系统开环观测器的实际状态$x$，则观测器的初始状态$\hat x_0$必须与系统的初始状态$x_0$完全相同，否则二者从初态就已经不相同，伴随着干扰和系统参数变化的不一致性，差别将会越来越大，预测结果也没有意义，然而想要保证系统初态与观测器初态完全一致是不可能的，因此开环观测器也同样没有工程价值。</p><h4 id="7-5-3-渐近观测器">7.5.3 渐近观测器</h4><p>在开环观测器的基础上，我们利用输出信息对状态误差$|x-\hat x|$进行校正，增加反馈校正通道，从而构成渐近状态观测器。渐近观测器的系统结构图如<a href="#fig20">图20</a>所示，其中蓝色部分为反馈校正通道。<br><span id="fig20"></span></p><center><img src="./ModernControlTheory/渐近观测器.png"/>图20</center><p>校正过程如下：</p><ol><li>在某一状态中$\hat x\neq x$，存在状态误差</li><li>由于状态误差，观测器的输出$\hat y$与实际状态$y$不同</li><li>由$\hat y=Cx$，得到输出误差$y-\hat y=y-Cx$</li><li>经反馈矩阵$G$反馈送到观测器中的每个积分器的输入端，参与调整观测器的状态$\hat x$，使$x$以一定的精度和速度趋近于系统的真实状态x。</li></ol><p>我们可以根据状态结构图写出状态方程<br>$$<br>\hat x=A\hat x+Bu+Gy-GC\hat x<br>$$<br>其中$\hat x$为状态矩阵的观测矢量，使状态$x$的估计值，$\hat y$是状态观测器的输出矢量，$G$是状态观测器的输出误差反馈矩阵。将上式化简可得$\hat x=(A-GC)\hat x+Gy+Bu$，根据化简后的状态空间表达式我们可以得到新的状体空间结构图，如<a href="#fig21">图21</a>所示。<br><span id="fig21"></span></p><center><img src="./ModernControlTheory/渐近观测器简化.png"/>图21</center><p>新的结构图中有两个输入，即待观测系统的控制作用$u$和待观测系统中的输出$y$，有一个输出即为状态估计值$\hat x$。</p><h3 id="7-6-结合状态观测器的状态反馈">7.6 结合状态观测器的状态反馈</h3><p><span id="fig22"></span></p><center><img src="./ModernControlTheory/状态观测器1.png"/>图22 直接反馈系统</center><p><span id="fig23"></span></p><center><img src="./ModernControlTheory/状态观测器2.png"/>图23 带观测器状态反馈系统</center><p>我们先讨论利用真实状态$x$直接反馈和利用观测器估计状态$\hat x$进行反馈的异同。利用真实状态$x$直接反馈的系统结构图如<a href="#fig22">图22</a>所示，利用观测器估计状态$\hat x$进行反馈的系统结构图如<a href="#fig23">图23</a>所示。</p><p>带状态观测器的状态反馈系统如<a href="#fig24">图24</a>所示。<br><span id="fig24"></span></p><center><img src="./ModernControlTheory/带观测器的状态反馈结构图.png"/><p>图24</p></center><p>从系统的结构和状态空间表达式来看，设能控能观的系统$\Sigma_0=\left{\begin{aligned}<br>\dot x&amp;=Ax+Bu\y&amp;=Cx<br>\end{aligned}\right.$，状态观测器$\Sigma_G=\left{\begin{aligned}<br>\dot{\hat x}&amp;=(A-GC)\hat x+Gy+Bu\\hat y&amp;=C\hat x<br>\end{aligned}\right.$,反馈控制律为$u=v+K\hat x$。整理可得整个闭环系统的状态空间表达式为<br>$$<br>\left{\begin{array}{l}<br>\dot{x}=A x+B K \hat{x}+B v \<br>\dot{\hat{x}}=G C x+(A-G C+B K) \hat{x}+B v \<br>y=C x<br>\end{array}\right.<br>$$<br>写作矩阵形式为<br>$$<br>\left[\begin{array}{c}<br>\dot{x} \<br>\dot{\hat{x}}<br>\end{array}\right]=\left[\begin{array}{cc}<br>A &amp; B K \<br>G C &amp; A-G C+B K<br>\end{array}\right]\left[\begin{array}{l}<br>x \<br>\hat{x}<br>\end{array}\right]+\left[\begin{array}{l}<br>B \<br>B<br>\end{array}\right] v\<br>y=\begin{bmatrix}<br>C&amp;0<br>\end{bmatrix}\cdot \begin{bmatrix}<br>x \\hat x<br>\end{bmatrix}<br>$$</p><p>闭环系统有如下几个特性：</p><ol><li>闭环系统的分离性<br>对有观测的系统$\Sigma_K=(A+BK,B,C)$，$\Sigma_0$直接状态反馈系统的极点和观测器$\Sigma_G$的极点相互独立分离，可以进行单独选择。</li><li>传递函数矩阵不变性<br>用观测器构成的状态反馈系统和状态直接反馈系统的传递函数相同</li><li>观测器反馈与直接状态反馈的等效性<br>用观测器构成的状态方程$\dot {\tilde x}=(A-GC)x$可以通过选择$G$使得$A-GC$的特征根均具有负实部，因此可以得到$\lim \tilde x=0$，因此当$t\rightarrow\infty$时，观测器进入稳态，观测器反馈等效于直接状态反馈，并且可以选择$G$加速$\tilde x\rightarrow 0$，即$\hat x$渐近于真实状态$x$的速度。</li></ol><h2 id="总结">总结</h2><p>本文介绍了现代控制理论中的知识点，包括系统的状态表达方程求解、系统的状态空间表达、系统的传递函数表达、系统的能控型和能观性、系统的状态反馈、系统的状态观测器、系统的状态反馈与观测器的结合等。本文的内容主要来自[1]，并且在此基础上进行了一些补充。</p><h2 id="引用">引用</h2><p>[1] <a href="https://detail.tmall.com/item.htm?ali_refid=a3_430673_1006:1125575197:N:FfkiwNyLQhEG06QYe83G8g4uJYoGG5IP:ae733d8f84ee5bbdea7de0d5002730fe&amp;ali_trackid=1_ae733d8f84ee5bbdea7de0d5002730fe&amp;id=577893358117&amp;spm=a2e0b.20350158.31919782.1">现代控制理论 第三版；刘豹，唐万生</a></p><h2 id="查重结果">查重结果</h2><p><img src="./ModernControlTheory/%E6%9F%A5%E9%87%8D%E7%BB%93%E6%9E%9C.png" alt="查重结果"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>高斯过程回归</title>
      <link href="/2022/11/22/Gaussian-Process-Regression/"/>
      <url>/2022/11/22/Gaussian-Process-Regression/</url>
      
        <content type="html"><![CDATA[<h2 id="线性回归">线性回归</h2><p>对于有$n$个点的数据集$D={(x_1,y_1),(x_2,y_2),…(x_n,y_n)}$,其中$x_i\in\mathbb{R}^n,y_i\in\mathbb{R}$，$x$与$y$存在相关关系。</p><h3 id="矩阵表示">矩阵表示</h3><p>对于$x\in\mathbb{R}^p$，有$n$个样本的数据集<br>$$<br>X=\begin{pmatrix}<br>x_1&amp;x_2&amp;\cdots&amp;x_n<br>\end{pmatrix}^T=<br>\begin{pmatrix}<br>x_1\<br>x_2\<br>\vdots\<br>x_n<br>\end{pmatrix}=<br>\begin{pmatrix}<br>x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1p}\<br>x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2p}\<br>\vdots&amp;\vdots&amp;\ddots&amp;\vdots\<br>x_{n1}&amp;x{n2}&amp;\cdots&amp;x_{np}<br>\end{pmatrix}<em>{n\times p}\<br>Y=\begin{pmatrix}<br>y_1\y_2\\vdots\y_n<br>\end{pmatrix}</em>{n\times1}<br>$$<br>对于$x\in\mathbb{R}^1$和其对应输出$y$,我们有以下样本<br><img src="img/Gaussian-Process-Regression/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A0%B7%E6%9C%AC.png" alt="线性回归样本"></p><h3 id="矩阵视角-LSE">矩阵视角(LSE)</h3><p>最小二乘法：通过最小化误差的平方和寻找数据的最佳函数匹配。<br>令误差函数<br>$$L(w)=\sum_{i=1}^n||w^Tx_i-y_i||^2$$<br>为了能在计算机上实现并行运算，我们需要将它解释为矩阵意义。<br>$$<br>\begin{align}<br>&amp;\sum_{i=1}^n||w^Tx_i-y_i||^2\nonumber\<br>=&amp;\sum_{i=1}^{n}(w^Tx_i-y_i)^2\nonumber\<br>=&amp;\begin{pmatrix}<br>w^Tx_1-y_1&amp;w_Tx_2-y_2&amp;\cdots&amp;w_Tx_n-y_n<br>\end{pmatrix}<br>\begin{pmatrix}<br>w^Tx_1-y_1\w_Tx_2-y_2\\vdots\w_Tx_n-y_n<br>\end{pmatrix}\nonumber\<br>=&amp;(W^TX^T-Y^T)(XW-Y)\nonumber\<br>=&amp;W^TX^TXW-2W^TX^TY+Y^TY\nonumber<br>\end{align}<br>$$<br>我们需要得到$\hat{W}=\arg\min L(W)$,在$\frac{\partial L(W)}{\partial W}=2X^TXW-2X^TY=0$时取得最优解，解的$W=(X^TX)^{-1}X^TY$</p><h3 id="概率视角-MLE">概率视角(MLE)</h3><p>现实中的数据会受到噪声干扰，假设其服从正态分布$\varepsilon\sim N(0,\sigma^2)$，则$y=f(w)+\varepsilon$。因此有$y|x_iw\sim N(w^Tx,\sigma ^2)$,得$P(y|x,w)=\frac1{\sqrt{2\pi}\sigma}\exp(-\frac{(y-w^Tx)^2}2\sigma^2)$</p><p>我们能定义这样一个极大似然估计<br>$$<br>\begin{align}<br>&amp;\mathcal{L}(w)\nonumber\<br>=&amp;\log P(Y|X,w)=log\prod_{i=1}^{n}P(y_i|x_i,w)\nonumber\<br>=&amp;\sum_{i=1}^{N}\log P(y_i|x_i,w)\nonumber\<br>=&amp;\sum_{i=1}^{n}\log\frac1{\sqrt{2\pi}\sigma}+\log\exp(-\frac{(y-w^Tx)^2}2\sigma^2)\nonumber\<br>=&amp;\sum_{i=1}^{n}(\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{(y-w^Tx)^2}2\sigma^2)\nonumber\<br>\end{align}<br>$$<br>由此我们得到最终的优化目标:<br>$$<br>\begin{align}<br>\hat W\nonumber=&amp;\arg\max\mathcal{L}(W)\nonumber\<br>=&amp;\arg\max (-\frac{(y-w^Tx)^2}2\sigma^2)\nonumber\<br>=&amp;\arg\min(y_i-w_Tx_i)^2\nonumber<br>\end{align}<br>$$</p><h3 id="岭回归">岭回归</h3><p>梯度下降</p><h2 id="贝叶斯线性回归">贝叶斯线性回归</h2><p>由上节已知如下系统：<br>$$<br>\begin{equation}<br>\left{<br>\begin{array}{lr}<br>f(x)=w^Tx=x^Tw\<br>y=f(x)+\varepsilon<br>\end{array}\nonumber<br>\right.<br>\end{equation}<br>$$<br>其中$x\in\mathbb{R}^n,y\in\mathbb{R},\varepsilon\sim N(0,\sigma^2)$,$x,y,\varepsilon$为随机变量，$w$为参数。</p><p>贝叶斯方法认为参数$w$不是未知的参数，而是一个概率分布。贝叶斯方法包含两个部分：Infrence和Pridiction.</p><h3 id="Inference">Inference</h3><p>$$<br>\begin{align}<br>&amp;P(W|Data)\nonumber\<br>=&amp;P(W|X,Y)\nonumber\<br>=&amp;\frac{P(W,Y|X)}{P(Y|X)}\nonumber\<br>=&amp;{\frac{P(Y|W,X)\cdot P(W|X)}{\underbrace{\int P(Y|W,X)\cdot P(W)\text{dw}}_{const}}}\nonumber<br>\end{align}<br>$$<br>其中$P(Y|W,X)$是似然，$P(W|X)$是先验。</p><p>$$P(Y|W,X)=\prod_{i=1}^{n}P(y_i|w,x_i)=\prod_{i=1}^{n}N(y_i|w^Tx_i,\sigma^2)$$</p><p>由于$X$和$W$是独立的，因此<br>$$<br>P(W|X)=P(W)=N(0,\Sigma_p)<br>$$<br>其中$p$为$W$的维数。<br>且由于高斯分布是自共轭的，因此两个高斯分布乘积也为高斯分布。则可以得到<br>$$<br>\begin{align}<br>\underbrace{P(W|Data)}<em>{Gaussian}&amp;\propto \underbrace{P(Y|W,X)}</em>{Gaussian}\cdot \underbrace{P(W)}<em>{Gaussian}\<br>&amp;\propto\prod</em>{i=1}^{n}N(y_i|w^Tx_i,\sigma^2)\cdot N(0,\Sigma_p)<br>\end{align}<br>$$<br>令$P(W|Data)=N(\mu_w,\Sigma_w)$,我们需要求出$\mu_w和\Sigma_w$。</p><p>推导过程：<br>对似然部分取高斯分布:<br>$$<br>\begin{aligned}<br>&amp; P(Y \mid X, w)=\prod_{i=1}^{N} \frac{1}{(2 \pi)^{\frac{1}{2}} \sigma^{\prime}} \exp \left{-\frac{1}{2 \sigma^{2}}\left(y_{i}-w^{\top} x_{i}\right)^{2}\right} \<br>=&amp; \frac{1}{(2 \pi)^{\frac{N}{2}} \cdot \sigma^{N}} \exp \left{-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(y_{i}-w^{\top} x_{i}\right)^{2}\right} \<br>=&amp; \frac{1}{(2 \pi)^{\frac{N}{2}} \sigma^{N}} \exp \left{-\frac{1}{2}(Y-X w)^{\top} \sigma^{-2} \cdot I(Y-X w)\right} \<br>=&amp; N\left(X w, \sigma^{-2} I\right)<br>\end{aligned}<br>$$<br>接着我们继续求$P(W|Data)$<br>$$<br>\begin{align}<br>&amp;P(W|Data)\nonumber\<br>\propto&amp; N(Xw,\sigma^{-2}I)\times N(0,\Sigma_p)\nonumber\<br>\propto&amp;\exp(-\frac12(Y-Xw)^T\sigma^{-2}I(Y-Xw))\times \exp(-\frac12w^T \Sigma_p^{-1}w)\nonumber\<br>=&amp;\exp (-\frac{1}{2\sigma^2}(Y^T-w^TX^T)(Y-Xw)-\frac12w^T\Sigma_p^{-1}w)\nonumber\<br>=&amp;\exp(-\frac{1}{2\sigma^2}(Y^TY-2Y^TXw+w^TX^TXw)-\frac12w^T\Sigma_p^{-1}w)\nonumber\<br>\end{align}<br>$$</p><blockquote><p>$$<br>\begin{align}<br>&amp;\text{若}P(x)=N(\mu, \Sigma) \nonumber \<br>&amp;\text{指数部分有：}\nonumber \<br>&amp;\exp(-\frac12(x-\mu)^T\Sigma^{-1}(x-\mu))\nonumber \<br>=&amp;\exp(-\frac12(\underbrace{x^T\Sigma^{-1}x}<em>{二次项}-\underbrace{2\mu^T\Sigma^{-1}x}</em>{一次项} +\underbrace{\mu^T\Sigma\mu}_{常数项}))\nonumber \<br>\end{align}<br>$$</p></blockquote><p>对于$P(W|Data)$：<br>二次项：$=-\frac{1}{2}w^T\underbrace{(\sigma^{-2}X^TX+\Sigma^{-1}<em>p)}</em>{对应\Sigma_w^{-1}}w$<br>一次项：$\underbrace{\sigma^{-2}Y^TX}_{对应\mu_w^T\Sigma_w^{-1}}w$</p><p>由上我们得知$\mu_w^T\Sigma_w^{-1}=\sigma^{-2}Y^TX$,求得$\mu_w=\sigma^{-2}\Sigma X^TY$，其中$\Sigma^{-1}=\sigma^{-2}x^Tx+\Sigma_p^{-1}$</p><p>最后得出$P(W|Data)=N(\mu_w,\Sigma_W)$,其中$\mu_w$和$\Sigma_w$已求出</p><h3 id="Prediction">Prediction</h3><blockquote><p>贝叶斯方法的一个重要意义是能给出预测的不确定性量化</p></blockquote><p>在预测问题中，给定$x^<em>,求y^</em>$<br>模型：<br>$$<br>\left{<br>\begin{align}<br>&amp;f(x)=w^Tx=x^Tw\nonumber\<br>&amp;y=f(x)+\varepsilon\nonumber<br>\end{align}<br>\right.<br>$$<br>求$f(x^<em>)$:<br>$$<br>\left{<br>\begin{align}<br>&amp;f(x)=x^Tw\nonumber\<br>&amp;w\sim N(\mu_w,\Sigma_w)\nonumber<br>\end{align}<br>\right.\Rightarrow f(x^</em>)=x^{<em>T}w\sim N(x^{<em>T}\mu_w,x^{<em>T}\Sigma_wx^</em>)<br>$$<br>求$y^</em>$：$y^</em>=f(x^<em>)+\varepsilon$<br>最后得出$P(y^</em>|Data,x^*)=N(x^{*T}\mu_w,x^{<em>T}\Sigma_wx^</em>+\sigma^2)$</p><h2 id="高斯过程回归">高斯过程回归</h2><h3 id="高斯分布">高斯分布</h3><p>一维高斯分布:<br>$$<br>X\sim N(\mu,\sigma^2)\Rightarrow f(x)=\frac1{\sqrt{2\pi}\sigma}\exp(-\frac{(x-\mu)^2}{2\sigma^2})<br>$$</p><p>多元高斯分布：<br>$$<br>p(x_1,x_2,…x_n)=\prod p(x_i)=\frac{1}{(2\pi)^\frac{n}{2}\sigma_1\sigma_2…\sigma_n}\exp(-\frac12[\frac{(x_1-\mu_1)^2}{\sigma_1}+\frac{(x_2-\mu_2)^2}{\sigma_2}+…+\frac{(x_n-\mu_n)^2}{\sigma_n}])<br>$$</p><h3 id="协方差">协方差</h3><p>在多维的高斯分布当中，两两分布之间存在某种联系，$n$维的高斯分布需要$n$个均值和$n$个方差来确定，通常情况下我们将均值定为一个均值向量，方差组用一个矩阵来表示即协方差矩阵，当两两分布之间相互独立，则这个协方差矩阵为对角矩阵，反之为非对角矩阵。</p><h3 id="高斯过程">高斯过程</h3><p>一个均值和一个方差能确定一个一维高斯分布，那么一组均即一个均值向量和一个协方差矩阵就能确定一个多维高斯分布了。</p><p>将多元高斯分布的维数放大到无限大，其中均值向量里的每个元素都遵循均值函数，即若在这个无限维的高斯分布中给出某一维的高斯分布，就可以根据均值函数确定出这一维高斯分布的均值，同理协方差函数也可以得出对应的协方差矩阵。</p><p>在高斯过程中，可以用核函数作为协方差函数。协方差矩阵如下生成：<br>$$<br>k(x,x’)=\sigma_f^2\exp(\frac{(x-x’)^2}{2l^2})\Rightarrow K=\begin{bmatrix}<br>k(x_1,x_1)&amp;k(x_1,x_2)&amp;\cdots&amp;k(x_1,x_n)\<br>k(x_2,x_1)&amp;k(x_2.x_2)&amp;\cdots&amp;k(x_2,x_n)\<br>\vdots&amp;\vdots&amp;\ddots&amp;\vdots\<br>k(x_n,x_1)&amp;k(x_n,x_2)&amp;\cdots&amp;k(x_n,x_n)<br>\end{bmatrix}<br>$$</p><p>将无限维高斯过程中的每一维展开就能得到一个高斯分布曲线，其中每一个点都对应了一个一维的高斯分布，而每个点都有一个均值和方差，因此高斯过程中的每个点都对应一个均值和方差。</p><h3 id="样例">样例</h3><p>已知<br>x=[-1.5, -1, -0.75, -0.4, -0.25, 0];<br>y=[-1.65, -1.08, -0.27, 0.27, 0.59, 0.82];<br>求解：$x^*=0.2时,y=?$</p><h4 id="第一步">第一步</h4><p>确定一个数据点为高斯分布的采样点，即$y(x)\sim N(\mu(x),k(x,x’))$</p><h4 id="第二步">第二步</h4><p>确定均值函数，不妨设为0。</p><h4 id="第三步">第三步</h4><p>确定协方差函数为高斯核函数，并进行修正。</p><p>$$<br>k(x,x’)=\sigma^2\exp[-\frac{-(x-x’)^2}{2l^2}]+\sigma_n^2\delta(x,x’)<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SVD奇异值分解</title>
      <link href="/2022/11/22/SVD/"/>
      <url>/2022/11/22/SVD/</url>
      
        <content type="html"><![CDATA[<h2 id="共轭">共轭</h2><p>复数取共轭，实部不变，虚部取反即可。</p><h3 id="复数共轭">复数共轭</h3><p>取共轭是对复数而言，$z=a+bj$为复数，其中$j=\sqrt{-1}$，为虚数单位。<br>那么复数$z$的共轭为：$z^<em>=a-bj$。<br>举例：$z=2+3j$,则$z^</em>=2-3j$</p><h3 id="复值函数共轭">复值函数共轭</h3><p>令复值函数$$z(x)=a(x)+jb(x)$$其中$a(x)$和$b(x)$都为实值函数，x为实数，那么$z(x)$的共轭函数$$z^*(x)=a(x)-jb(x)$$</p><h2 id="共轭转置">共轭转置</h2><p>一般指$m*n$型矩阵$A$做的一种数学变换，其中矩阵$A$中元素为复数。<br>具体来说，将$A$中的每个元素$a_{ij}$取共轭得到$b_{ij}$,得到新的矩阵记为$B$，对矩阵$B$取转置得到$B^T$，此时矩阵$A$的共轭转置$A^H=B^T$</p><h2 id="酉矩阵">酉矩阵</h2><h3 id="定义">定义</h3><p>若$n$阶复矩阵$A$满足$A^HA=AA^H=E$，则$A$为酉矩阵。</p><h3 id="性质">性质</h3><p>对于酉矩阵$A$:<br>$A^{-1}=A^H$<br>$A^{-1}$也是酉矩阵<br>$\det(A)=1$<br>充分条件是它的$n$个列向量为两两正交的单位向量</p><h2 id="奇异值分解SVD">奇异值分解SVD</h2><h3 id="定义-2">定义</h3><p>对非方阵的$m\times n$的矩阵分解，定义矩阵$A$的SVD为:<br>$$A=U\Sigma V^T$$其中$U$为$m\times m$的矩阵，$\Sigma$为$m\times n$的矩阵，主对角线外的元素全零，主对角线上的元素称为奇异值。$V$是一个$n\times n$的矩阵。$U$和$V$都是酉矩阵，即$U^HU$=$E$,$V^HV=E$。</p><h3 id="求解">求解</h3><p>$AA^T$的特征向量组成的矩阵为SVD中的$U$矩阵,$A^TA$的特征向量组成的矩阵为SVD中的$V$矩阵。证明如下，以$V$矩阵为例：<br>$$<br>A^TA=V\Sigma U^TU\Sigma V^T=V\Sigma^2V^T<br>$$从中也可以看到特征值矩阵$\Lambda=\Sigma^2$</p><p>同时我们也可以看出特征值矩阵等于奇异值$\Sigma$的平方。</p><p>对方阵$AA^T$做特征分解，满足$(AA^T)v_i=\lambda u_i$，得到n个特征值和n个特征向量${u_i|i=1,2,…,n}$,这n个特征向量组合张成的矩阵就是$U$。</p><p>对方阵$A^TA$做特征分解，满足$(A^TA)v_i=\lambda v_i$，得到n个特征值和n个特征向量${v_i|i=1,2,…,n}$,这n个特征向量组合张成的矩阵就是$V$。</p><p>接下来求奇异矩阵$\Sigma$。<br>$$<br>\begin{align}<br>&amp;A=U\Sigma V^T\nonumber\<br>\Rightarrow&amp;AV=U\Sigma V^TV\nonumber\<br>\Rightarrow&amp;AV=U\Sigma\nonumber\<br>\Rightarrow&amp;Av_i=\sigma_iu_i\nonumber\<br>\Rightarrow&amp;\sigma_i=Av_i/u_i\nonumber<br>\end{align}<br>$$<br>由于特征值矩阵为$\Sigma^2$,奇异值也可以直接计算$\sigma_i=\sqrt{\lambda_i}$。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ADL综述</title>
      <link href="/2022/11/19/ADL-Survey/"/>
      <url>/2022/11/19/ADL-Survey/</url>
      
        <content type="html"><![CDATA[<h2 id="介绍">介绍</h2><p>背景：深度学习的蓬勃发展<br>问题：数据集太大，label样本工作量繁重<br>目的：标记高质量样本<br>实质：与用户交互等方式，为新的数据点标记上期望输出，在统计学中叫做优化实验设计(ODE)，同时也是处理样本不足问题的一种实践方式</p><h2 id="概述">概述</h2><h3 id="符号定义">符号定义</h3><p>$$<br>\begin{array}{l}<br>\begin{array}{ll}<br>\hline \text {Variables} &amp; \text { Definition }\<br>\hline D^{tr} &amp; \text { training data set } \<br>\hline D^{ts} &amp; \text { testing data set }\<br>\hline D^{\text {cdd }} &amp; \text { candidate data set }  \<br>\hline B_{n} &amp; \text { selected data set in the } n \text {-the iteration }\<br>\hline F_{\theta}(\cdot) &amp; \text { predictor with parameter } \theta   \<br>\hline S_{\psi}(\cdot) &amp; \text { selector with parameter } \psi\<br>\hline \left(\mathbf{x}<em>{i}, y</em>{i}\right) &amp; \text { the } i \text {-th sample } \mathbf{x}<em>{i} \text { with label } y</em>{i} \<br>\hline<br>\end{array}<br>\end{array}<br>$$<br><img src="./ADL-Survey/fig1~AL%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B.png" alt="AL训练过程"><br>在任意第n次迭代中，AL使用其选择器从候选数据集$D^{cdd}$中来选择一些未标记样例作为集合$B_n$对其进行标记。然后通过新的训练集$D^{tr}<em>n$重新训练$F</em>\theta(\cdot)$，其中$D_n^{tr}=D^{tr}<em>{n-1}+B_n$。<br>如果没有选择器$S</em>{\psi}({\cdot})$,预测器$F_{\theta}(\cdot)$只是个普通的分类器。当$S_{\psi}({\cdot})$与$F_{\theta}(\cdot)$无关时，AL工作将会十分简单。然而普遍情况下它们相互影响。当新增样例只有一个的时候，可以如下表示：<br>$$\hat{\mathbf{x}}=\arg \max_{\mathbf{x}<em>{k} \in D^{c d d}} S</em>{\psi}\left(\mathbf{x}_{k}, D^{c d d}, D^{t r}, \theta\right)$$</p><h3 id="核心问题">核心问题</h3><p><img src="./ADL-Survey/fig2~AL%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98.png" alt="AL三个核心问题"></p><p>至少有三个方面是决定选择器设计的基础和关键因素：模型因素、数据因素、度量因素。<br>模型因素：主要指预测器的结构、形式和特点等方面。对于不同结构的模型如逻辑回归和SVM所设计的选择器会有很多不同。选择器到底是什么样子，跟选择器服务的对象是有关的。主动学习的选择器要适应具有深度结构的预测器，这也在主动学习的选择器上提出了更高的要求。因此预测器的模型特征其实对选择器的构建产生了非常深刻的影响。<br>数据因素：主要指有标签数据或者无标签数据内在的统计特性或者结构特征。如果把预测问题看作求解回归的问题，那么数据的潜在分布就是其先验知识。数据与样本之间的相互关系会导致训练样本具有不同的价值，因此很多选择器的策略如代表性和差异性都是典型的基于数据分布构建的。<br>度量因素：主要指的是对预测结果的损失或状态的度量，比如预测误差的分布和状态一般能为选择器的设计提供良好的参考信息。对无标签数据预测后的状态能度量预测器对于这些样本预测的结果的信心。典型方法有基于不确定性采样、委员会查询、误差变化等。并且度量因素只看结果，数量不大时非常方便，有时候可以做到与预测器关系不大，因此可以很容易地迁移到深度网络的预测器。</p><h3 id="优化问题">优化问题</h3><p>对于数据驱动的主动深度学习来说，选择器的优化计算过程通常更加重要，不管是数据驱动还是模型驱动，它们的选择器都需要根据自己的特点进行优化计算,它们的选择器没有显性的定义和形式而建立选择器的过程需要从大量数据中获得信息和支持。</p><h3 id="初始训练">初始训练</h3><p>初始训练样本在大多数情况下会对主动深度学习后期的表现有很深刻的影响。初始训练可以有两种方式：第一完全随机初始训练，第二有选择的初始训练。初始训练样本决定了预测器的初始参数，对于深层预测器，不同的初始训练样本会导致不同的参数状态，以至于初始预测的表现经常会有不稳定的情况。初始样本经过策略选择称为热启动，其初始样本集合称为核心集合，否则就叫其冷启动。核心集合的研究跟主动学习有着密切的联系。</p><h3 id="查询方式">查询方式</h3><p>常见的分三种：流式查询，池式查询，合成式成员查询。<br>流式查询：利用选择器$S\psi(\cdot)$确定每个数据点是否需要标注，一般计算量大，因为每次判断一个数据都需要考虑整体。一般适合于只用少量数据就能进行训练的预测器。<br>池式查询：从没有标签的数据中抽取一部分形成样本池，然后批量选择标注。一般会从选择器对样本里所有的样本进行度量，取前$N$个最有价值的样本进行标注。<br>合成式成员查询：标注过程中能生成一个或多个查询点，一般都是具有较大的信息量，从而对当前预测器具有较大的价值。生成的过程大多基于无标签数据和预测器当前的状态，对于真实世界来说生成的样本时常不存在于自然界导致难以解释。近期生成对抗网络能够隐性的拟合复杂分布的数据并能够批量生成高质量的样本，在合成式成员查询方面显示了很大的潜力。</p><h3 id="分类">分类</h3><p><img src="./ADL-Survey/fig3~AL%E5%88%86%E7%B1%BB.png" alt="AL分类"><br>无论主动学习AL的选择器是否为深度网络，只要其预测器是深度模型，那就认为主动学习是主动深度学习ADL。主动深度学习分为两大类：模型驱动以及数据驱动。</p><p>对于模型驱动的主动深度学习，它的选择器类似于传统主动学习的选择器，一般来说最常见的有三种模型驱动的主动深度学习模型：<br>第一类基于无标签样本的不确定性，例如不确定性采样，委员会查询等。<br>第二类基于预测器对模型的影响，例如基于梯度长度、基于Fisher信息等。<br>第三类基于无标签样本内在分布和结构，例如流形学习、聚类等。<br>基于贝叶斯理论的方法可以单独列为一类，因为其可以从不同角度解释主动学习，但是实际上它比较接近不确定采样这类。<br>也有一些混合的主动深度学习方法，它们同时联合两种准则构建自己的选择器，在选择样本的时候也有一定的优势。<br>基于模型驱动的ADL的共同特点就是其选择器是模型驱动，模型驱动一般意味着需要手工设计，而且是浅层模型，因此许多主动深度学习的选择器只是从传统主动学习的浅层模型中借鉴过来，并没有经过根本性的改进和提升。</p><p>对于数据驱动的主动深度学习，它们的选择器一般采用深度结构，其特征不再是手工特征，而是数据训练形成的。基于深层结构和特征学习的选择器正在渐渐占据主流。基于数据驱动的主动深度学习可以分为：<br>基于元学习的主动深度学习；<br>基于强化学习的主动深度学习；<br>基于不确定量的主动深度学习；<br>基于数据增强的主动深度学习。</p><p>在数据驱动的主动学习方面几乎看不到手工特征或手工度量标准如信息熵，KL散度，高斯相似性等，取而代之，最重要的问题在于它的数据驱动机制或结构的设计，进而实现选择器的功能。许多新颖的神经网络理念如强化深度学习、元学习、生成对抗网络等都可以构建选择器。大体上ADL的研究成果也是基于当前机器学习的最新进展而形成的。</p><h2 id="模型驱动的主动学习">模型驱动的主动学习</h2><p>模型驱动的主动学习主要指学习器$S_\psi(\cdot)$由手工特征或手动设计来完成，早期的主动学习几乎都是模型驱动的，数据驱动在主动学习中并没有像在深度学习中那样快速发展。一些研究直接将模型驱动的选择器移植到ADL中，它们的选择器是传统的显示模型如熵或fisher信息等，模型驱动的ADL几乎都是无监督模型。</p><p>优点：<br>具有明确的统计和物理意义<br>很多传统方法可以直接迁移到一个深度学习预测器中形成新的ADL<br>选择器的构建不需要太多样本</p><p>缺点：<br>严重依赖人的先验和假设<br>模型驱动是浅层模型，可能失效，对于深度预测器可能不稳定<br>效率与预测器类型有关，也不容易利用深度模型预测器的参数状态</p><h3 id="基于不确定量的主动学习">基于不确定量的主动学习</h3><h2 id="数据驱动的主动学习">数据驱动的主动学习</h2><p>数据驱动的ADL主要指ADL中的选择器是数据驱动的，往往是深度架构。在ADL中关于如何设计数据驱动的具有深层网络的选择器的研究还处在早期阶段。<br>我们的主要关注点在于数据驱动的选择器，我们通常可以把ADL中的选择器看作是一个构造特征或度量的过程，数据驱动意味着进行的是自动特征学习或自动度量学习。模型驱动中ADL的选择器是无监督的手动特征或指标，而数据驱动中ADL的选择器可以是有监督或无监督的自动特征学习。但是有一个难题在于，ADL面对的是未标记的样本，因此设计一种监督机制实现端到端的样本获取并不容易。<br>选择器可以通过有监督或无监督的方式，或两者结合。对于无监督数据驱动的类型，通常基于可以提供样本代表性的自动编码器（如VAE）或可以增强样本多样性的GAN。 对于有监督学习数据驱动的类型，一方面需要通过建立新的数据结构新的任务模式为训练选择器提供监督信息，另一方面可以通过探索一些新的机器学习架构如元学习等模拟选择器和预测器之间的连接。<br>如果我们把数据驱动的选择器看成是小样本学习的问题，那么它就引出了元学习选择器。如果我们将数据驱动的选择器作为对环境（预测器）起作用的代理，则它会倾向于强化学习选择器。</p><h3 id="Meta-Learning-元学习">Meta-Learning 元学习</h3><p>ADL的目标于小样本学习密切相关，因此元学习是在ADL中实现数据驱动的好方法。元学习也可以称为“学会学习”，旨在设计出通过少量训练示例快速学习新技能或适应新环境的模型。元学习的学习过程并非学习样本是什么，而是识别样本之间的关系，例如样本是否为同一类。举个例子，对于一个参观动物园的小朋友，他不认识眼前的动物，但是给他一组卡片，他能将眼前的动物与卡片上的动物对比进行匹配从而得知眼前是什么动物，这项能力就是“学会学习”的目标。<br>元学习的三个特征与主动学习一致：</p><ol><li>新任务：主动学习对未标记样本做出决定时，就类似于面临一个新任务</li><li>学习新技能：为预测器服务的选择器需要获得新的学习技能，因为预测器在训练中是不断增强的。</li><li>学习样本少：以更少的训练样本获得更高的训练性能。</li></ol><p>元学习的任务结构：<br>训练集和测试集中的元素是一个由Support集和Query集组成的任务。其中训练集就是元集，支持集用于训练，查询集用于测试。</p><p>K-way N-shot Support Set：<br><img src="./ADL-Survey/k_way_n_shot.png" alt="kn"><br>训练集定义如下：<br>$$<br>D^{tr}={&lt;D_k^{tr-spt},D_k^{tr-qry}&gt;}<em>{k=1}^K\<br>D_k^{tr-spt}={(x_k^1,y_k^1),…,(x_k^p,y_k^p)}\<br>D_k^{tr-qry}={(x_k^1,y_k^1),…,(x_k^q,y_k^q)}<br>$$<br>测试集定义如下：<br>$$<br>D^{ts}={&lt;D_n^{ts-spt},D_k^{ts-qry}&gt;}</em>{n=1}^N\<br>D_k^{ts-spt}={(x_k^1,y_k^1),…,(x_k^p,y_k^p)}\<br>D_k^{ts-qry}={(x_k^1,y_k^1),…,(x_k^q,y_k^q)}<br>$$<br>常见的监督学习的训练样本是一对标记样本$(x_i,y_i)$，元学习的训练集和测试集的基本元素是任务。对于每个任务通常只有很少的样本。<br>元参数$\psi$应由集合$D^{tr}$中的不同任务预先训练<br>$$<br>\hat{\psi}=\arg\underbrace{\max}<em>{\psi}\log p(\psi|D^{tr})=\arg\min\sum</em>{i=1}^P{N}(D_i^{tr-qry}|D_i^{tr-spt})<br>$$<br>然后基于$D_{ts}$中的新任务和元参数$\psi$搜索参数$\phi$<br>$$<br>\phi=\arg\max\log p(\phi|D^{ts},\hat{\psi})<br>$$</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>傅立叶变换</title>
      <link href="/2022/11/15/FourierTransform/"/>
      <url>/2022/11/15/FourierTransform/</url>
      
        <content type="html"><![CDATA[<p><img src="https://s1.ax1x.com/2022/11/19/zucXb8.png" alt="zucXb8.png"></p><h2 id="变换">变换</h2><p>举个栗子，在直角坐标系上定义单位向量$\vec{e}_x,\vec{e}_y$内积<br>$$<br>\vec{e}_x\times\vec{e}_x=1\<br>\vec{e}_y\times\vec{e}_y=1\<br>\vec{e}_x\times\vec{e}_Y=0<br>$$<br>称$\vec{e}_x$和$\vec{e}_y$为一对标准正交基<br>我们在坐标系上有向量$\vec{A}$,$\vec{B}$,$\vec{C}$。我们可以将$\vec{A}\vec{B}\vec{C}$变换为数字形式，如$\vec{X}=(m,n)$,含义为$\vec{X}=m\vec{e}_x+n\vec{e}_y$,则有<br>$$<br>\vec{A}=(2,1)\<br>\vec{B}=(1,2)\<br>\vec{C}=(3,3)<br>$$<br>同时也可以定义一系列运算如<br>$$\vec{A}+\vec{B}=(2,1)+(1,2)=(1+2,2+1)=(3,3)=\vec{C}$$<br>同样从图像变为数字后也可以从数字变换回图像</p><h2 id="傅立叶级数">傅立叶级数</h2><p>首先上结论，任何周期性函数都可以用正弦函数余弦函数表示，即$f(t)=\sum (\sin+\cos)$<br>有一个周期函数可以分解为以下正余弦函数之和。<br><img src="./img/FourierTransform/%E5%8F%98%E6%8D%A2%E8%BF%87%E7%A8%8B.png" alt="变换过程"><br>将其展开，如下图所示。<br>从正面看自变量为时间$t$,函数值随时间变化，从侧面看自变量为频率$\omega$,函数值随频率变化。从正面分析就是从时域分析，从侧面分析就是从频域分析。频域中正余弦波形受相位影响，时域频域相互转换。<br>将其表示为<br>$$<br>\begin{align}<br>f(t)&amp;=\frac{a_0}{2}+\sum a_n\sin(n\omega t+\varphi_n)\nonumber\<br>&amp;=\frac{a_0}{2}+\sum a_n\sin(n\omega t)+\sum b_n\cos(n\omega t)\nonumber<br>\end{align}<br>$$<br>其中，$1,\sin(n\omega t),\cos(n\omega t)$为标准正交基,这里的正交基计算为积分形式，后面会用到。</p><h2 id="傅立叶变换">傅立叶变换</h2><h3 id="欧拉公式">欧拉公式</h3><p>坐标系如下：<br>横坐标为1，纵坐标为$i$，则$A$的坐标有<br>$$\left{<br>\begin{align}<br>\cos\theta+i\sin\theta&amp;=e^{i\theta}\nonumber\<br>\theta&amp;=\omega t\nonumber<br>\end{align}<br>\right.\Rightarrow e^{iwt}<br>$$</p><h3 id="连续非周期傅立叶变换">连续非周期傅立叶变换</h3><p>对于非周期函数$f(x)$，计算<br>$$<br>F_T=\int_{-\infty}^{\infty}f(t)e^{-jwt}dt=<br>\left{<br>\begin{align}<br>&amp;=0,不含\omega\nonumber\<br>&amp;\neq0,含\omega\nonumber<br>\end{align}<br>\right.<br>$$<br>与$e^{jwt}$相乘,将含有$\omega$的成分摘出来,相当于将正交基$\sin n\omega t和\cos n\omega t$摘出来。<br>$F_T$为复数，实部为频率$\omega$，虚部为相位$\varphi$，通过图像可知某频率的幅度大小以及相位。<br>将傅里叶变换逆变换回去，称为逆傅立叶变换。<br>$$<br>f(t)=\int_{-\infty}^{\infty}F_T(\omega)e^{j\omega t}d\omega<br>$$</p><h3 id="连续周期傅立叶变换">连续周期傅立叶变换</h3><p>周期为$T$<br>$$<br>f(t)=\sum_{n=-\infty}^{\infty}c_ne^{jn\omega t},\omega=\frac{2\pi}{T}\<br>c_n=\frac1T\int_{0}^{T}f(t)e^{-jnwt}dt<br>$$</p><h3 id="离散非周期信号">离散非周期信号</h3><p>$t\rightarrow n\Delta t$,为采样间隔$\Delta t=\frac1{f_s}$。<br>$$<br>F(jw)=\sum_{n=-\infty}^{\infty}f(n\Delta t)e^{-j\omega n\Delta t}\<br>\Delta t=1s时，F(j\omega)=\sum_{n=-\infty}^{\infty}f(n)e^{-j\omega n}<br>$$</p><p>特点:以$2\pi$为周期<br>$$<br>F(j\omega+2\pi)=\sum_{n=-\infty}^{\infty}f(n)e^{-(j\omega+2\pi) n}=\sum_{n=-\infty}^{\infty}f(n)e^{-j\omega n}e^{-2\pi n}<br>$$</p><h3 id="离散周期信号">离散周期信号</h3><p>$t\rightarrow k\Delta t$。$T=N\Delta t$，令$\Delta t=1s$,则$T=N$<br>$$<br>F_n=\frac{1}{N}\sum_{k=0}^{n-1}f(k\Delta t)e^{-jn\omega k}<br>$$<br>性质：<br>$$<br>F(k)=\frac{1}{N}f(n)e^{-jn\omega k}=\frac{1}{N}f(n)e^{-jn\frac{2\pi}{N} k}<br>F(k+N)=</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>俞博论文命名</title>
      <link href="/2022/11/08/Nomenclature/"/>
      <url>/2022/11/08/Nomenclature/</url>
      
        <content type="html"><![CDATA[<h2 id="Nomenclature">Nomenclature</h2><h3 id="Roman-Symbols">Roman Symbols</h3><p>$$<br>\begin{array}{l}<br>\bar{s}<em>{i, j}&amp;\text{scaled sensitivity of the i-th state variable over the j-th parameter}\<br>D&amp;\text{divergence of model predictions between different models}\<br>\mathbf{F}&amp;\text{parametric Jacobian matrix}\<br>\mathbf{f}&amp;\text{a set of state transition functions}\<br>\mathbf{G}&amp;\text{derivatives of model outputs w.r.t. parameters}\<br>g</em>{i}&amp;\text{derivative of the model equation w.r.t. the i-th input variable}\<br>\mathbf{H}&amp;\text{Hessian matrix}\<br>\mathbf{h}&amp;\text{a set of measurement functions}\<br>\mathbf{J}&amp;\text{Jacobian matrix}\<br>k_{i}&amp;\text{the i-th kinetic parameter}\<br>m&amp;\text{number of outputs that can be measured}\<br>n&amp;\text{number of state variables}\<br>N&amp;\text{number of sampling time points}\<br>p&amp;\text{number of parameters}\<br>Q&amp;\text{measurement error covariance matrix}\<br>R&amp;\text{residual of parameter estimation}\<br>\mathbb{R}^{i}&amp;\text{the i-dimensional real number set}\<br>\mathbf{S}&amp;\text{parameter sensitivity matrix}\<br>\mathbf{S}<em>{0}&amp;\text{initial condition fo local parameter sensitivity matrix}\<br>s</em>{i, j}&amp;\text{absolute sensitivity of the i-th state variable over the j-th parameter}\<br>t&amp;\text{time in a continuous model}\<br>\mathbf{T}<em>{s p}&amp;\text{sampling design factors}\<br>V&amp;\text{parameter estimation error covariance matrix}\<br>W&amp;\text{weighting matrix}\<br>\mathbf{X}&amp;\text{vector of state variables}\<br>\mathbf{X}</em>{0}&amp;\text{vector of state variables}\<br>\mathcal{X}<em>{i}&amp;\text{ initial condition of state variables}\<br>Y&amp;\text{ vector of measured model outputs}\<br>\hat{\mathbf{Y}}&amp;\text{vector of model outputs predictions}\<br>y</em>{i}&amp;\text{the i-th model output}\<br>\hat{y}_{i}&amp;\text{the i-th model output prediction}\<br>\mathbf{z}&amp;\text{measurement set design factors}\<br>\end{array}<br>$$</p><h3 id="Greek-Symbols">Greek Symbols</h3><p>$$<br>\begin{array}{l}<br>\chi^{2}&amp;\text{chi-square distribution}\<br>\delta_{i}&amp;\text{CIs of the i-th model parameter}\<br>\delta_{j}^{\operatorname{msqq}}&amp;\text{mean squared root of absolute local sensitivites of j-th parameter to model outputs}\<br>\gamma&amp;\text{collinearity index}\<br>\lambda_{\min }&amp;\text{minimal eigenvalue of a matrix}\<br>\lambda_{\max }&amp;\text{maximal eigenvalue of a matrix}\<br>\phi&amp;\text{vector of experimental design factors}\<br>\sigma_{i}^{2}&amp;\text{measurement error variance of yi}\<br>\boldsymbol{\theta}&amp;\text{vector of model parameters}\<br>\Theta&amp;\text{admissible range of parameters}\<br>\hat{\boldsymbol{\theta}}&amp;\text{estimated model parameters}\<br>\theta_{i}&amp;\text{the i-th model parameter}\<br>v&amp;\text{objective function of OED}\<br>\varsigma&amp;\text{input design factors}\<br>\xi&amp;\text{vector of measurement errors}\<br>\end{array}<br>$$</p><h3 id="Superscripts">Superscripts</h3><p>$$<br>\begin{array}{l}<br>T&amp;\text{transpose of a vector}<br>\end{array}<br>$$</p><h3 id="Other-Symbols">Other Symbols</h3><p>$$<br>\arg \min f(x) \text { value of argument } x \text { that minimise } f(x)<br>$$</p><h3 id="Acronyms-Abbreviations">Acronyms / Abbreviations</h3><p>$$<br>\begin{array}{l}<br>AIM &amp;\text{analytically integrated Magnus method}\<br>CI &amp;\text{confidence intervals}\<br>CRLB &amp;\text{Cramer-Rao lower bound}\<br>CV &amp;\text{P control vector parametrisation}\<br>DDM &amp;\text{direct differential method}\<br>FDM &amp;\text{finite difference method}\<br>FIM &amp;\text{Fisher information matrix}\<br>GAs &amp;\text{genetic algorithm}\<br>GFM &amp;\text{green function method}\<br>GSA &amp;\text{Global sensitivity analysis}\<br>i.i.d.&amp;\text{i.d. identically and independently distributed}\<br>LSA &amp;\text{local sensitivity analysis}\<br>LSE &amp;\text{least square estimation}\<br>MLE &amp;\text{maximum likelihood estimation}\<br>NLP &amp;\text{non-linear programming}\<br>NP−hard &amp;\text{non-deterministic polynomial-time hard}\<br>ODEs &amp;\text{ordinary differential equations}\<br>OED &amp;\text{Optimal experimental design}\<br>OFAT &amp;\text{one-factor-at-a-time}\<br>PDEs &amp;\text{partial differential equations}\<br>PSO &amp;\text{Particle swarm optimisation}\<br>RED &amp;\text{Robust experimental design}\<br>\end{array}<br>$$</p><h2 id="ODE">ODE</h2><h3 id="Biodiesel-Production">Biodiesel Production</h3><p>$$<br>\left{<br>\begin{array}{l}<br>\frac{d(T\cdot V)}{d t}=-V \cdot r_{2} \<br>\frac{d(D\cdot V)}{d t}=V \cdot\left(r_{3}-r_{4}\right) \<br>\frac{d(M\cdot V)}{d t}=V \cdot\left(r_{5}-r_{6}\right) \<br>\frac{d(BD \cdot V)}{d t}=V \cdot r_{9} \<br>\frac{d(FFA \cdot V)}{d t}=V \cdot r_{8} \<br>\frac{d(G\cdot V)}{d t}=V \cdot r_{7} \<br>\frac{d(W\cdot V)}{d t}=-V \cdot r_{8} \<br>\frac{d(CH \cdot V)}{d t}=-V \cdot\left(r_{9}+r_{10}\right) \<br>\frac{d(E \cdot V)}{d t}=V \cdot\left(r_{1}-r_{2}-r_{4}-r_{6}+r_{8}+r_{9}-r_{10}\right) \<br>\frac{d(E X \cdot V)}{d t}=V \cdot\left(r_{3}+r_{5}+r_{7}-r_{8}-r_{9}\right) \<br>\frac{d(E T \cdot V)}{d t}=V \cdot\left(r_{2}-r_{3}\right) \<br>\frac{d(E D \cdot V)}{d t}=V \cdot\left(r_{4}-r_{5}\right) \<br>\frac{d(E M \cdot V)}{d t}=V \cdot\left(r_{6}-r_{7}\right) \<br>\frac{d(E C H \cdot V)}{d t}=V \cdot r_{10} \<br>\frac{d\left(E_{b u l k} \cdot V\right)}{d t}=-V \cdot r_{1} \<br>\frac{d V_p}{d t}=R_{G}+R_{W}, \<br>\frac{d V}{d t}=F_{a}<br>\end{array}<br>\right.<br>\Rightarrow<br>\left{<br>\begin{array}{l}<br>\frac{dT}{dt}=-\frac{T}{V}\cdot Fa - (r_{2})\<br>\frac{dD}{dt}=-\frac{D}{V}\cdot Fa + (r_{3} - r_{4})\<br>\frac{dM}{dt}=-\frac{M}{V}\cdot Fa + (r_{5} - r_{6})\<br>\frac{dB}{dt}=-\frac{B}{V}\cdot Fa + (r_{9})\<br>\frac{dFFA}{dt}=-\frac{FFA}{V}\cdot Fa + (r_{8})\<br>\frac{dG}{dt}=-\frac{G}{V}\cdot Fa + (r_{7})\<br>\frac{dW}{dt}=-\frac{W}{V}\cdot Fa - (r_{8})\<br>\frac{dC}{dt}=-\frac{C}{V}\cdot Fa - (r_{9} + r_{10})\<br>\frac{dE}{dt}=-\frac{E}{V}\cdot Fa + (r_{1} - r_{2} - r_{4} - r_{6} + r_{8} + r_{9} - r_{10})\<br>\frac{dEX}{dt}=-\frac{EX}{V}\cdot Fa + (r_{3} + r_{5} + r_{7} - r_{8} - r_{9})\<br>\frac{dET}{dt}=-\frac{ET}{V}\cdot Fa + (r_{2} - r_{3})\<br>\frac{dED}{dt}=-\frac{ED}{V}\cdot Fa + (r_{4} - r_{5})\<br>\frac{dEM}{dt}=-\frac{EM}{V}\cdot Fa + (r_{6} - r_{7})\<br>\frac{dEC}{dt}=-\frac{EC}{V}\cdot Fa + (r_{10})\<br>\frac{dEb}{dt}=-\frac{Eb}{V}\cdot Fa - (r_{1})\<br>\frac{dVp}{dt}=RG + RW\<br>\frac{dV}{dt}=Fa<br>\end{array}<br>\right.<br>$$</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>只有我不存在的街道</title>
      <link href="/2022/10/23/dream1023/"/>
      <url>/2022/10/23/dream1023/</url>
      
        <content type="html"><![CDATA[<center>## 只有我不存在的街道</center><center><p>带着记忆回到了小时候<br>但那并不是我的过去<br>我是需要完成某件事的</p><p><a href="./dream1023/1.jpg"><img src="./dream1023/1.jpg" alt="1"></a><br><a href="./dream1023/2.png"><img src="./dream1023/2.png" alt="2"></a></p><p>同桌是一个凉薄的小女孩<br>仿佛被笼罩在旁人无法触及的阴影下<br>周围的人有着一般孩子的童真<br>我和他们搞好了关系<br>也渐渐打开了她的心门</p><p><a href="./dream1023/3.png"><img src="./dream1023/3.png" alt="3"></a><br><a href="./dream1023/4.png"><img src="./dream1023/4.png" alt="4"></a></p><p>每天回家的时候她会等我<br>我们便一起走<br>我是需要完成某件事的<br>故事里的电动车也许十分重要<br>有一天它被挪了位置<br>我没在意<br>没过多久看见有人骑走了它<br>我没在意<br>于是车就丢了<br>我不确定那是不是我的车<br>我并没有去争取<br>这稍纵即逝的机会</p><p><a href="./dream1023/5.png"><img src="./dream1023/5.png" alt="5"></a></p><p>第二天的课程需要烤陶器<br>有人陶土不够，拿走了我的<br>我没在意<br>他把做好的陶器送给了她<br>我看着燃尽的灰烬<br>心里不是很舒服<br>但是我没在意<br>直到晚上回家的时候<br>她没等我</p><p><a href="./dream1023/6.png"><img src="./dream1023/6.png" alt="6"></a></p><p>我拿出手机想联系她<br>发现我忘记了她的名字<br>翻遍列表也没有想起她的名字<br>我并没有趁还记得其他人的名字的时候联系他们<br>只是戏谑地觉得丢人<br>迟疑间却连其他人的名字也忘记了<br>我是要完成某件事的<br>但是我好像搞砸了<br>懦弱，怠惰，迟疑<br>我也许错过了无数的机会<br>一切就都搞砸了<br>我并不是不在意<br>我只是什么都没做<br>接着便醒了<br>我不能保证这只是一场梦<br>也许是命运中失败的一条时间线<br>以一场梦的名义遭到了废弃<br>他们的名字也许本就是故人的名字<br>只是我早就不记得了<br>故事的结束始于遗忘</p></center>]]></content>
      
      
      <categories>
          
          <category> 日记 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>写好一篇综述</title>
      <link href="/2022/10/23/WriteReview/"/>
      <url>/2022/10/23/WriteReview/</url>
      
        <content type="html"><![CDATA[<h2 id="规则">规则</h2><h3 id="确定一个主题和受众">确定一个主题和受众</h3><p>如何选择要审查的主题？当代科学有很多问题，你可以用一生的时间来参加会议和阅读文献，只是思考要回顾什么。一方面，如果你花几年时间来选择，其他几个人可能在此期间也有同样的想法。另一方面，只有一个经过深思熟虑的主题才有可能导致一篇出色的文献综述。这个主题至少必须是：</p><ol><li>你感兴趣（理想的情况是，你应该遇到一系列与你的工作领域相关的、需要进行批判性总结的最新论文）。</li><li>该领域的一个重要方面（这样，许多读者会对综述感兴趣，并有足够的材料来写），和</li><li>一个定义明确的问题（否则你有可能包括成千上万的出版物，这将使综述没有帮助）。</li></ol><p>潜在的综述想法可能来自于提供有待回答的关键研究问题清单的论文 [9]，也可能来自于偶然的阅读和讨论中的偶发事件。除了选择主题外，您还应该选择目标受众。在很多情况下，主题(比如计算生物学中的网络服务)会自动定义一个受众(比如计算生物学家)，但是同一主题也可能是邻近领域(比如计算机科学、生物学等)感兴趣的。</p><h3 id="文献的搜索和再搜索">文献的搜索和再搜索</h3><p>在选择了你的主题和听众之后，开始检查文献并下载相关论文。这里有五条建议。</p><ol><li>记录你使用的搜索项目（以便你的搜索可以被复制）。</li><li>保留一份你不能立即访问其pdf文件的论文清单（以便以后用其他策略检索）。</li><li>使用一个论文管理系统（如Mendeley, Papers, Qiqqa, Sente）。</li><li>在审查过程中尽早确定一些排除不相关论文的标准（这些标准可以在审查中描述，以帮助确定其范围），以及</li><li>不要只找你想审查的领域的研究论文，也要找以前的审查。</li></ol><p>很有可能有人已经发表过文献综述（图1），如果不是完全针对你打算解决的问题，至少也是针对一个相关的主题。如果已经有几篇或数篇关于你的问题的文献综述，我的建议是不要放弃，而是继续进行你自己的文献综述。</p><p>根据发表的研究论文和文献综述的数量，需要不同类型的文献综述的概念图。</p><p>右下角的情况（文献综述多，研究论文少）不仅仅是一种理论上的情况；例如，它适用于气候变化对植物疾病影响的研究，在这种情况下，文献综述似乎比研究报告多</p><ol><li>在你的综述中讨论过去综述的方法、限制和结论。</li><li>试图找到一个在以前的综述中没有充分涉及的新角度，以及</li><li>纳入自它们出现以来不可避免地积累的新材料。</li></ol><p>在搜索文献中的相关论文和综述时，通常的规则适用。</p><ol><li>要彻底。</li><li>使用不同的关键词和数据库来源（例如，DBLP、Google iii: Scholar、ISI Proceedings、JSTOR Search、Medline、Scopus、Web of Science），并且</li><li>看看谁引用了过去的相关论文和书籍章节。</li></ol><h3 id="阅读时勤做笔记">阅读时勤做笔记</h3><p>如果你先读论文，然后才开始写综述，你将需要一个非常好的记忆力来记住谁写了什么，以及你在读每篇论文时的印象和联想。我的建议是，在阅读时，开始写下有趣的信息，关于如何组织综述的见解，以及关于写什么的想法。这样，当你读完你选择的文献时，你就已经有了一份综述的粗略草稿。</p><p>当然，这个草稿仍然需要大量的改写、重组和重新思考，以获得一个具有连贯论点的文本，但你将避免盯着一个空白文件所带来的危险。如果你临时从文献中逐字复制，在做笔记时要注意使用引号。然后，最好在最后的草稿中用你自己的话重新表述这些引文。重要的是，在这个阶段就应该小心地记下参考文献，以避免错误的归属。从你的工作一开始就使用参考文献软件将节省你的时间。</p><h3 id="选择你想写的综述类型">选择你想写的综述类型</h3><p>在阅读文献时做了笔记后，你将对可用于综述的材料数量有一个大致的概念。这可能是决定是写一篇小型综述还是全面综述的好时机。一些期刊现在倾向于发表专注于过去几年的短篇综述，对字数和引文有限制。小综述不一定是小综述：它很可能吸引忙碌的读者的更多关注，尽管由于篇幅限制，它不可避免地会简化一些问题，并遗漏一些相关材料。全文综述的好处是有更大的自由度，可以详细报道某一特定科学发展的复杂性，但可能会被那些没有时间阅读主要专著的读者留在非常重要的论文堆里 “待读”。</p><p>在小型综述和全面综述之间可能存在着一个连续体。同样的观点也适用于描述性综述和综合性综述的二分法。描述性综述侧重于每个综述研究的方法、结果和解释，而综合性综述则试图从综述材料中找到共同的观点和概念。叙述性综述和系统性综述之间也存在类似的区别：叙述性综述是定性的，而系统性综述则试图根据已发表的证据来检验一种假设，这些证据是通过预定的协议来收集的，以减少偏见。当系统性回顾以定量的方式分析定量结果时，它们就成了元分析。对不同综述类型的选择必须根据具体情况而定，这不仅取决于所发现材料的性质和目标期刊的偏好，而且还取决于撰写综述的时间和共同作者的数量</p><h3 id="保持评论的重点，但又不失广泛的兴趣">保持评论的重点，但又不失广泛的兴趣</h3><p>无论你的计划是写一篇小型或完整的综述，保持重点是个好建议16, 17。如果只是为了写材料而写材料，很容易导致综述试图同时做太多的事情。对于跨学科综述来说，保持综述的重点是有问题的，因为跨学科综述的目的是在各领域之间架起桥梁。例如，如果你要写一篇关于流行病学方法如何用于模拟思想传播的综述，你可能会倾向于包括流行病学和文化传播研究这两个母体领域的材料。这在某种程度上可能是必要的，但在这种情况下，有重点的综述将只详细处理那些在流行病学和思想传播之间的研究。</p><p>虽然重点是成功综述的一个重要特征，但这一要求必须与使综述与广大读者相关的需要相平衡。可以通过讨论所综述的主题对其他学科更广泛的影响来圈定这个方格。</p><h3 id="要有批判性和连贯性">要有批判性和连贯性</h3><p>审查文献不是集邮。好的综述不只是对文献进行总结，而是对其进行批判性的讨论，找出方法论问题，并指出研究差距。阅读完文献综述后，读者应该对以下内容有一个大致的了解。</p><ol><li>该领域的主要成就。</li><li>争论的主要领域，以及</li><li>尚未解决的研究问题。</li></ol><p>在所有这些方面实现成功的回顾是具有挑战性的。一个解决办法是让一组互补的共同作者参与进来：有些人擅长描绘已经取得的成就，有些人非常擅长识别地平线上的乌云，有些人则擅长预测解决方案的来源。如果你的期刊社正好有这样的团队，那么你肯定应该写一篇文献综述 除了批判性思维，文献综述还需要一致性，例如在选择被动语态与主动语态、现在时与过去时方面。</p><h3 id="找到一个逻辑结构">找到一个逻辑结构</h3><p>就像一个烤得很好的蛋糕一样，一篇好的综述有一些明显的特征：它值得读者花时间，及时，系统，写得好，有重点，有批判性。它还需要一个好的结构。对于综述来说，研究论文通常分为引言、方法、结果和讨论的做法是行不通的，或者很少使用。然而，对背景的一般性介绍，以及在结尾处对所涉及的主要内容的复述和带回家的信息，在综述中也是有意义的。对于系统性综述，现在的趋势是包括如何搜索文献的信息（数据库、关键词、时间限制）。</p><p>如何组织综述主体的流程，使读者被吸引并引导其阅读？一般来说，画一个综述的概念图是很有帮助的，例如，用思维导图技术。这种图可以帮助认识到一种逻辑性的方式来排列和连接综述的各个部分。这不仅在写作阶段是这样，如果图被作为图表包含在综述中，对读者来说也是如此。仔细选择与综述主题相关的图表和数字，对文章的结构也有很大帮助。</p><h3 id="利用反馈">利用反馈</h3><p>文献综述通常与研究论文一样接受同行评议，这是正确的[。作为一项规则，采纳审稿人的反馈意见，大大有助于改进综述草案。审稿人在以新的思路阅读综述后，可能会发现一些不准确、不一致和模棱两可的地方，而这些地方是作者由于重读了太多次排版稿而没有注意到的。然而，建议在提交前再重读一次草稿，因为在最后一刻纠正错别字、跳跃和混乱的句子可能会使审稿人专注于对内容而不是形式提供建议。</p><p>反馈对写好综述至关重要，应向不同的同事征求意见，以获得对草案的不同看法。在某些情况下，这可能会导致对论文的优点和如何改进论文的意见冲突，但这种情况总比没有反馈要好。对文献综述的反馈观点的多样性有助于确定共识观点在当前对某一问题的科学理解中的位置。</p><h3 id="包括你自己的相关研究，但要客观">包括你自己的相关研究，但要客观</h3><p>在许多情况下，文献综述员会发表与他们所写综述相关的研究。这可能会造成利益冲突：审稿人怎么能客观地报告自己的工作？一些科学家可能对自己发表的东西过于热衷，因此有可能在综述中对自己的研究结果给予过多的重视。然而，偏见也可能发生在另一个方向：一些科学家可能对自己的成就不屑一顾，因此他们在审查某一领域时倾向于淡化自己的贡献（如果有的话）。</p><p>总的来说，对文献的审查既不应该是一本公共关系手册，也不应该是一种竞争性的自我否定。如果一个综述员能够胜任工作，写出一篇条理清晰、有条不紊的综述，语句通顺，为读者提供服务，那么在综述自己的相关发现时，就应该能够做到客观。在多作者撰写的综述中，可以通过将对共同作者的结果的审查分配给不同的共同作者来实现。</p><h3 id="要与时俱进，且温故知新">要与时俱进，且温故知新</h3><p>鉴于科学论文发表的速度逐渐加快，今天的文献综述不仅需要了解一个调查领域的总体方向和成就，还需要了解最新的研究，以免在这些研究发表之前就已经过时了。理想情况下，文献综述不应该把一个刚刚在一系列出版的论文中得到解决的问题确定为主要的研究空白（当然，这也适用于较早的、被忽视的研究（“沉睡的美人”））。这意味着文献综述者最好密切关注电子版的论文清单，因为这些论文可能需要几个月才能出现在科学数据库中。有些文献综述宣称他们已经扫描了某个时间点的文献，但是考虑到同行评审可能是一个相当漫长的过程，在修订阶段对新出现的文献进行全面搜索可能是值得的。评估刚刚出现的论文的贡献尤其具有挑战性，因为没有什么角度可以衡量它们对进一步研究和社会的意义和影响。</p><p>不可避免的是，在综述发表后，关于综述主题的新论文（包括独立撰写的文献综述）会从各方面出现，因此，可能很快就需要更新综述。但这是科学的本质。我祝愿大家在撰写文献综述时好运。</p><h2 id="框架设计">框架设计</h2><p>逻辑上review = recall + viewpoint。一篇review的内容在于对别人工作的回顾以及自己的观点。</p><p>物理上review = introduction + maintext +conclusion。<br>introduction包含了recall，包括研究背景，重要性，难点。mainText部分就是recall + viewpoint，除了recall还要加入自己的观点，总结关键的指标以及数据。conclusion部分就是结合自己的观点以及对未来发展的建议。</p><p><a href="https://imgse.com/i/xgo4UA"><img src="https://s1.ax1x.com/2022/10/23/xgo4UA.jpg" alt="xgo4UA.jpg"></a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>AutoODE-COVID</title>
      <link href="/2022/10/19/AutoODE-COVID/"/>
      <url>/2022/10/19/AutoODE-COVID/</url>
      
        <content type="html"><![CDATA[<h2 id="1-动力模型">1. 动力模型</h2><p>令$y\in \mathbb{R}^d$表示为观察变量，$u\in \mathbb{R}^p$表示为为观察变量, 本文的目标是学习一个动力系统如下表示:<br>$$<br>\begin{cases}<br>\frac{dy}{dt}=f_\theta(t, y, u)\<br>\frac{du}{dt}=g_\theta(t, y, u)\<br>y(t_0)=y_0\<br>u(t_0)=u_0<br>\end{cases}\tag{1.1}<br>$$<br>在这个动力系统中，本文的观测值$(y_0, y_1, …, y_{k-1})$作为输入，其中$k$为样本范围，任务是学习$f_\theta和g_\theta$，并且产生精准的$(y_k, …, y_{k+q-1})$，其中$q$为预测范围。</p><h2 id="2-数据驱动模型">2. 数据驱动模型</h2><p>在动力系统中，$f$和$g$是未知的，训练集和测试集可以来源于一个长序列中的切片子序列（参数相同，初始条件不同），也可以来源于系统中的独立样本（参数不同，初始条件相同），假设$p_S$为训练数据分布，$p_T$为特使数据分布。本文使用深度学习寻求一个假设$h \in \mathcal{H}: \mathbb{R}^{d \times k} \mapsto \mathbb{R}^{d \times q}$<br>$$<br>h(y_0^{(i)}, …, y_{k-1}^{(i)}=y_k^(i), …, y_{k+q-1}^{(i)})<br>$$<br>其中$i$表示单个样本，$k$表示输入长度，$q$表示输出长度。<br>标准的学习模型定义最小化的训练损失如下：<br>$$\hat{L}<em>1(h)=\frac 1n\sum</em>{i=1}^{n}l(y^{(i)}, h)$$<br>其中$y^{(i)}=(y_0^{(i)}, …, y_{k+q-1}^{(i)})$~$p_s$为第$i$个样本, $l$为其损失函数，以均方误差作为损失函数，本文得到<br>$$l(y^{(i)}, h)=||h(y_0^{(i)}, …, y_{k-1}^{(i)})-(y_k^{(i)}, …, y_{k+q-1}^{(i)})||<em>2^2$$<br>本文继续定义测试误差如下：<br>$$L_1(h)=\Epsilon</em>{y-p_T[l(y, h)]}$$<br>本文的训练目标是使得测试误差$L_1(h)$最小，并且当$|\hat{L_1}(h)-L_1(h)|$很小时说明模型的泛化性很好。</p><h2 id="3-物理驱动模型">3. 物理驱动模型</h2><p>物理建模中假设本文已知模型大致可以描述为一个ODE系统，本文知道有函数$f$和$g$，但是不知道具体的参数，本文使用自动微分法来估计未知参数$\theta$和初始值$u_0$, 这个过程就是AutoODE。$\theta$和$u_0$是未知的参数。其中$u$和$y$需要有足够的相关性，这样AutoODE才能根据$y$正确学习到所有的参数。</p><blockquote><p>AutoODE算法:<br>0: 随机初始化公式(1.1)中的未知参数$\theta, u_0$<br>1: 对公式(1.1)进行微调，并应用四阶Runge Kutta (RK4)方法<br>2: 对$(\hat{y_0}, …, \hat{y_k})$进行估计<br>3: 使用Adam算法最小化损失函数$L_2(\theta, u_0)$<br>$$<br>L_2(\theta, u_0)=\frac1k\sum_{i=0}^{k-1}||\hat{y}<em>i(\theta, u, t)-y_i(\theta, u, t)||^2<br>$$<br>4: 收敛后，使用估计出的参数$\hat{\theta}, \hat{u_0}$产生最后的预测$(y_k, …y</em>{k+q-1})$</p></blockquote><p>对于ODE的还有一种解法为NeuralODE，使用邻接(adjoint)法来通过数值求解器进行微分，在需要复杂数值积分的高维神经网络模型中比较有效，本案例处理的是低维ODE，RK4足够产生精准的预测。</p><h2 id="4-COVID-19预测">4. COVID-19预测</h2><p>本文对预测累积确诊、移除和死亡病例中 COVID-19 动态的不同方法进行了基准测试。本文提出了 SuEIR 隔室模型的扩展，并使用 AutoODE 估计模型参数。<br>$$\begin{cases}<br>\frac{dS_i}{dt}=-\frac{\sum_j\beta_i{(t)}A_{ij}(I_j+E_j)S_i}{N_i}\<br>\frac{dE_i}{dt}=\frac{\sum_j\beta_i(t)A_{ij}(I_j+E_j)S_i}{N_i}-\sigma_iE_i\<br>\frac{dU_i}{dt}=(1-\mu_i)\sigma_iE_i\<br>\frac{dI_i}{dt}=\mu_i\sigma_iE_i-\gamma_iI_i\<br>\frac{dR_i}{dt}=\gamma_iI_i\<br>\frac{dD_i}{dt}=r_i(t)\frac{dR_i}{dt}\<br>N_i=S_i+E_i+U_i+I_i+R_i<br>\end{cases}\tag{4.1}$$</p><h3 id="4-1-时空SuEIR模型">4.1 时空SuEIR模型</h3><p>本文提出了时空SuEIR模型(Spatiotemporal-SuEIR)，详细内容如下:<br>未知参数$\beta_i, \sigma_i, \mu_i和\gamma_i$分别为传播、潜伏、发现和恢复率。<br>未观察变量$u={S, E, U}$即易感者(susceptible)、暴露(exposed)和未报告病例(unreported)的累积数量<br>预测观察到的变量$y={I, R, D}$, 对应感染(infected)，移除(removed)和死亡(death)的累计数量。其中我们假定$N_i=S_i+E_i+U_i+I_i+R_i$是恒定的。</p><h4 id="4-1-1-稀疏传输矩阵-A-ij-的低秩近似">4.1.1 稀疏传输矩阵$A_{ij}$的低秩近似</h4><p>墨艐艎墨艐艎莫膭墨艐艎膷麓蘑墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎$A$脛艝墨艐艎墨艐艎50墨艐艎墨艐艎墨艐艎墨艐艎脰沤墨艐艎墨艐艎脛麓墨艐艎墨艐艎墨艐艎墨艐艎臉膭墨艐艎$A$墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎浓末墨艐艎墨艐艎脷艎亩啪墨艐艎墨艐艎墨艐艎$M$墨艐艎脥麓墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎$C$墨艐艎墨艐艎脭膾墨艐艎墨艐艎墨艐艎墨艐艎脣膭墨艐艎墨艐艎墨艐艎$A=M\odot C\in\mathbb{R}^{50\times50}$, $C$墨艐艎漠麓墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎姆艝纽麓墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脣艎芒末艎50墨艐艎墨艐艎墨艐艎墨艐艎脰沤墨艐艎墨艐艎脛麓墨艐艎墨艐艎墨艐艎墨艐艎臉艝墨艐艎$M$脦膾墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎浓末墨艐艎墨艐艎脷艎亩啪墨艐艎墨艐艎墨艐艎臉膭墨艐艎墨艐艎墨艐艎脣藝墨艐艎墨艐艎墨艐艎墨艐艎脷藳墨艐艎墨艐艎墨艐艎脰沤墨艐艎墨艐艎脛麓墨艐艎墨艐艎墨艐艎, 墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎莫膭墨艐艎脦膾墨艐艎脣艎墨艐艎艑模墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎挪藳墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎膼搂墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎$O(kn)$, 墨艐艎墨艐艎墨艐艎脙末脥脓墨艐艎墨艐艎墨艐艎墨艐艎脝藝墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脴啪墨艐艎墨艐艎墨艐艎$C=B^TD$, 墨艐艎墨艐艎墨艐艎墨艐艎$B, D\in\mathbb{R}^{k\times n}$, 墨艐艎墨艐艎墨艐艎墨艐艎$k&lt;&lt;n$</p><h4 id="4-1-2-分段性传输率-beta-i-t">4.1.2 分段性传输率$\beta_i(t)$</h4><p>墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脛艝墨艐艎脥艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎$\beta$墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脥纽墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎漠末墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脭颅墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎卯募炉墨艐艎脛膮盲模炉墨艐艎墨艐艎墨艐艎墨艐艎臉拧墨艐艎墨艐艎7墨艐艎墨艐艎脛募墨艐艎墨艐艎墨艐艎脭陇墨艐艎墨艐艎墨艐艎脷麓墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎艑藳墨艐艎墨艐艎墨艐艎脺藝墨艐艎墨艐艎墨艐艎墨艐艎盲模炉墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎臉拧墨艐艎墨艐艎墨艐艎墨艐艎臉膮墨艐艎墨艐艎盲模炉墨艐艎脛藝脰募墨艐艎墨艐艎墨艐艎墨艐艎脭膿墨艐艎墨艐艎墨艐艎$\beta_i(t)$, 墨艐艎墨艐艎墨艐艎艑艎墨艐艎墨艐艎莫末茫膭母膼膮墨艐艎臉膭墨艐艎脝蘑墨艐艎墨艐艎墨艐艎墨艐艎脦膾墨艐艎墨艐艎艆末墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎</p><h4 id="4-1-3-墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎臉艎墨艐艎脛艝">4.1.3 墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎臉艎墨艐艎脛艝</h4><p>墨艐艎脹脓墨艐艎膶纽墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脰沤墨艐艎墨艐艎脛拧墨艐艎莫末墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脭膭墨艐艎脰赂墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脦艝墨艐艎墨艐艎墨艐艎墨艐艎漠脓墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎$r_i(t)$脦膾$a_ix+b$墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脭膿墨艐艎墨艐艎漠掳墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脥拧墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎a, b脦膾墨艐艎墨艐艎艆搂莫掳墨艐艎墨艐艎墨艐艎墨艐艎</p><h4 id="4-1-4-墨艐艎墨艐艎膶篓墨艐艎墨艐艎臉搂墨艐艎墨艐艎墨艐艎墨艐艎">4.1.4 墨艐艎墨艐艎膶篓墨艐艎墨艐艎臉搂墨艐艎墨艐艎墨艐艎墨艐艎</h4><p>臉拧墨艐艎墨艐艎AutoODE墨艐艎墨艐艎膼膭墨艐艎墨艐艎墨艐艎墨艐艎膶篓墨艐艎墨艐艎臉搂墨艐艎墨艐艎墨艐艎墨艐艎<br>$$<br>L(A, \beta, \sigma, \gamma, r)=\frac1k\sum_{t=0}^{k-1}w(t) [l(\hat{I_t}, I_t)+\alpha_1l(\hat{R_t}, R_t)+\alpha_2l(\hat{D_t}, D_t)]<br>$$<br>$\alpha1, \alpha2$墨艐艎墨艐艎脝艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脳麓臇纽墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎脡末墨艐艎墨艐艎墨艐艎臉搂墨艐艎墨艐艎脥纽臉膮脦膾墨艐艎脣赂墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎浓末墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎膶篓脰末墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎墨艐艎$w(t)=\sqrt{t}$墨艐艎墨艐艎$\alpha1, \alpha2, k$墨艐艎墨艐艎墨艐艎墨艐艎脰陇墨艐艎墨艐艎墨艐艎莫末墨艐艎墨艐艎墨艐艎</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>贝叶斯定理</title>
      <link href="/2022/10/18/Bayesian/"/>
      <url>/2022/10/18/Bayesian/</url>
      
        <content type="html"><![CDATA[<h2 id="是什么？">是什么？</h2><p>在信息和条件有限的情况下基于过去的数据，通过动态调整的方法预测出事件发生的接近真实的概率。<br>根本思想是</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Learning representations by back-propagating errors</title>
      <link href="/2022/09/21/PR-BackPropagating/"/>
      <url>/2022/09/21/PR-BackPropagating/</url>
      
        <content type="html"><![CDATA[<h2 id="引言">引言</h2><p>在神经网络中描述一个算法，通过反复调整权重，最小化网络输出向量与目标输出向量之间的误差。非输入输出的隐变量&quot;hidden units&quot;通过测试集的学习结果来表示features, 捕获节点间的相互作用以寻找其中的规律。相比感知机，back-propagation算法能创造出更多有用的features。</p><p>设计出自组织(self-organizing)的神经网络的目的是找到一种强力的规则允许任意连接的神经网络开发内部结构(internal structure，使其能够通过给定的输入单元的每个状态向量得到输出单元的期望状态向量。输入单元直接连接输出单元时可以直接迭代出最小误差的结果，    。（在感知机里也有特征分析(feature analysers), 但是它们的输入连接是手动固定的，因此它们的状态完全由输入决定，所以并不能称之为隐藏单元）。learning proicedure需要决定在什么样的情况下激活隐藏单元，目标是取得input与output之间的behavior，从而解释了神经元能代表什么，本文提出了具有泛化性的简单又强力的方法。</p><h2 id="网络构建">网络构建</h2><p>学习目标是找到权重weights来确保输入向量经过网络得到的输出向量能与其目标输出向量尽可能接近，这一步我们称之为拟合。我们首先构建一个最基本的网络，它由最底层的输入层，若干中间层和最高层的输出层所组成。如下图所示。</p><p><a href="https://imgse.com/i/xizGwj"><img src="https://s1.ax1x.com/2022/09/21/xizGwj.png" alt="xizGwj.png"></a></p><p>其中，层间禁止连接，并且禁止从上层向下层连接，但是连接可以跳过中间层。网络从最底层的输入层开始，通过设置最底层的输入单元的状态来将输入向量表示到网络中，层和层之间的关系由公式1和公式2组成。同层神经元中的所有单元并行计算，而不同层中的单元存在递进关系，因此需要一层一层地按照时序计算。<br>$$<br>\begin{equation}<br>x_j=\sum y_iw_{ji}<br>\end{equation}<br>$$<br>$$\begin{equation}<br>y_j=\frac{1}{1+e^{-x_j}}<br>\end{equation}$$</p><p>在层间网络的构建中，并非只能选择这两个函数作为层间推进的方法，任何有界导数都可以胜任这项任务，只是说在实践中先进行线性计算再进行非线性计算可以更大程度地简化学习过程。</p><h2 id="权重公式">权重公式</h2><p>公式1是一个加权求和函数，我们的加权求和还需要额外引入一个偏置量$b$，也就是bias，相当于一个threshold的作用，我们在计算中可以将它视作一个普通的权重，而与其对应的输入则恒为常量1。</p><h2 id="激活函数">激活函数</h2><p>公式2是非线性激活函数的，它的作用是将输入从无限的空间缩放到(0, 1)之间。首先它是非线性的。因为如果连续的层间的关系函数都是线性，那不管有多少层，根据四则运算，最后的结果就等同于是一层线性函数。非线性函数解决了这个问题，同时其将数据缩放到（0，1）后也起到了激活的作用，函数值更接近0可以视作0，更接近1可以视作为1. 同时作为可导函数也可以参与到梯度下降中去。这里的公式2是激活函数的一种，叫sigmoid函数。</p><h2 id="损失函数">损失函数</h2><p>如果要实现学习目标，即使得网络产生的输出向量与目标输出向量尽可能接近，我们可以定义一个代价函数cost function，和层间推进公式一样，代价函数也不是唯一的。这里我们假设的cost function如公式3所示，是一个均方误差函数。<br>$$\begin{equation}\begin{aligned}<br>E(w)&amp;=\frac12\sum_c\sum_j(y_{j, c}-d_{j, c})^2\<br>&amp;=\frac12\sum_c\sum_j(x_j\times w_{j, c}-d_{j, c})^2<br>\end{aligned}\end{equation}$$<br>这是一个关于$w$的公式。其中$c$是输入输出对数，也就是层数-1，$j$是当前层的输出节点的数量，这个函数的意思是将各层中网络输出和目标输出的差值的平方进行求和，网络输出与目标输出相差越大损失函数的结果越大。所以我们只需要将损失函数降低到最小也就能最大程度保证网络输出与目标输出的差值越小，从而使得网络输出更接近目标输出。在这里我们最小化E的方式就是以偏导数的方式通过梯度下降法求出损失函数的最小值，此时损失函数对应的权重矩阵$w$即是最优解，这样我们就能把一个机器学习问题转化为一个最优化问题。</p><h2 id="链式求导">链式求导</h2><p>对于给定的case，偏导数的求法分为两步，第一步是前向传播，由公式1和公式2得出，第二步是反向传播。<br>反向传播从输出单元开始向前计算。从计算对$y_j$的偏导开始<br>$$\begin{equation}<br>\frac{\partial E}{\partial y_j}=y_j-d_j<br>\end{equation}$$<br>通过链式求导计算得到对$x$的偏导为<br>$$<br>\frac{\partial E}{\partial x_j}=\frac{\partial E}{\partial y_j}\times \frac{y_j}{x_j}<br>$$<br>对公式2求导可以得到$\frac{dy_j}{dx_j}$, 以此得到<br>$$\begin{equation}<br>\frac{\partial E}{\partial x_j}=\frac{\partial E}{\partial y_j}\times y_j(1-y_j)<br>\end{equation}$$<br>这样我们总体上得到了改变输入$x$对误差的影响, 而总输入又是其下层关于权重$w$的一个线性函数，由此也可以计算出改变权重$w$将如何对权重$w$造成影响。对于具体的权重$w_{j_i}$，可以得到其偏微分为<br>$$\begin{equation}\begin{aligned}<br>\frac{\partial E}{\partial w_{ij}}&amp;=\frac{\partial E}{\partial x_j}\times \frac{x_j}{w_{ji}}\<br>&amp;=\frac{\partial E}{\partial x_j}\times y_i<br>\end{aligned}\end{equation}$$<br>其中对于第$i$层的单元求对$y$的偏导为<br>$$\begin{equation}\begin{aligned}<br>\frac{\partial E}{\partial y_i}&amp;=\frac{\partial E}{\partial x_j}\times \frac{\partial x_j}{\partial y_j}\<br>&amp;=\frac{\partial E}{\partial x_j}\times w_{ji}<br>\end{aligned}\end{equation}$$<br>公式看似复杂，实质上是将每一层到前一层的函数关系用链式求导逐层分解，得到每一个变量对E的偏导数。而我们所需要的是公式6，也就是对参数的偏导数，通过对参数的偏导数去更新参数。</p><p>利用公式7能将参数从最后一层传递到倒数第二层，反复重复以上步骤就可以得到所有层的偏导，这其中就包括我们所需要的每一层对参数也就是权重的偏导，我们称之为错误变化率。通过变化率作为梯度进行梯度下降就可以得到最优化问题的最优解。</p><h2 id="梯度下降">梯度下降</h2><p>参数当前处于$\mathbb{R}^n$中的某一点，以cost function对参数的偏导数作为梯度，朝着相反的方向迭代就可以使得函数值减小。<br>$$\begin{equation}<br>\Delta w=-\varepsilon\times \frac{\partial E}{\partial w}<br>\end{equation}$$<br>其中$\varepsilon$的作用是放缩一次迭代的长度，称作步长。在这里我们将步长设定为一个常量，称为固定步长的梯度下降法。固定步长的优点是执行起来很简单，同时可以进行并行计算保证效率。<br>我们对固定步长的梯度下降算法进行优化如公式9所示。<br>$$\begin{equation}<br>\Delta w=-\varepsilon \frac{\partial E}{\partial w(t)}+\alpha \Delta w(t-1)<br>\end{equation}$$<br>其中t为迭代的次数，$\alpha$是一个指数衰减的调整因子，范围在(0, 1)之间，公式是将本次迭代的变化加上前一时刻步长变化与$\alpha$的乘积。<br>随机梯度下降法：对于每一个样本都直接对其求偏导更新一次参数<br>批量梯度下降法：求出所有样本的总梯度再更新参数。本文用的是批量梯度下降算法。</p><h2 id="参数初始化">参数初始化</h2><p>如果参数直接初始化为全0或全1使得参数具有对称性，那么在训练过程中会导致所有参数的对应梯度都是一样的，这样就导致了参数在进行梯度下降时失去了参数间的独立性，使得一整层的网络只有一个有效的features。因此在参数初始化时我们需要注意让参数初始不相同从而保证将参数间的特征区分开来。</p><h2 id="具体例子">具体例子</h2><p><a href="https://imgse.com/i/xFD5H1"><img src="https://s1.ax1x.com/2022/09/22/xFD5H1.md.png" alt="xFD5H1.md.png"></a><br>中间的是输入层，侧边两个是隐藏层，输出单元在顶层，层间连线上的数字是权重，隐藏单元和输出单元中的值为bias值。$\varepsilon=0.1$, $\alpha=0.9$。权重weight在(-0.3, 0.3)之间随机初始化。<br>这个网络的功能是检测输入的对称性，训练后的结果如下。<br>在这个网络中使得权重中心对称，这样就能保证当输入单元产生对称时，可以使得输入的加权和为0，或者只剩下一个很小的值，起作用的只剩下bias值，bias经过sigmoid函数后的结果为0，加权和为0，再到输出层，同理起作用的只剩下bias值，经过sigmoid函数后结果为1。而当输入不对称时，其中一个隐藏单元不为0，由于两个隐藏单元的权重是左右对称的，因此另一个隐藏单元与其互为相反数，则这两个隐藏单元经过sigmoid变换后的结果为1和0，经过加权使得输出单元的值必定小于0，经过sigmoid变换后结果为0。</p><h2 id="循环网络">循环网络</h2><p>到目前为止，处理的都是分层的前馈网络，事实上处理的还有循环神经网络，可以如图将循环网络展开为一个层级网络。<br><a href="https://imgse.com/i/xFyWjg"><img src="https://s1.ax1x.com/2022/09/22/xFyWjg.md.png" alt="xFyWjg.md.png"></a><br>在展开过程中，有两个复杂的难题需要解决。首先就是在循环神经网络中，每一层都会依赖以前的输入，而不仅仅是其前一层，因此我们需要存储所有隐藏层的输出数据。<br>其次，循环神经网络中只有一个权重$w$，因此当它展平为层级网络后，所有的层间权重都应该相等。对于这样的问题，我们可以对其偏导数求平均，更新权重参数的时候也更新平均梯度，保证层间权重都共享唯一一个平均权重。</p><h2 id="局部最优">局部最优</h2><p>反向前馈网络和许多网络一样都有一个缺点就是容易陷入局部最小值，因此梯度下降不一定能保证找到全局最优，不过通过实践可以得到，相比全局最小值，即使会陷入局部最小值，也不一定会陷入特别差的局部最小值，对此我们可以稍微加入少许连接帮助网络模型跳出局部最优。<br>补充一点，如果采用小批量随机梯度下降法也能够帮助模型跳出局部最优。</p><h2 id="后记">后记</h2><p>BP算法并不能真正模拟大脑学习，不过在权重空间内应用梯度下降算法后能发现很多有意思的特征，因此在认知科学和神经网络中也许存在一些必然的联系，因此可以在生物学中尝试观察是否能够一些方法和思想值得借鉴。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>基础拓扑笔记</title>
      <link href="/2022/09/20/BaseTopology/"/>
      <url>/2022/09/20/BaseTopology/</url>
      
        <content type="html"><![CDATA[<h2 id="预备知识">预备知识</h2><p>交、并、补、差、乘积、；单射双射满射；原像，像<br>连续函数<br>一点点抽象代数（拓扑群往后）：群的定义，同态，商群</p><h2 id="介绍">介绍</h2><h3 id="拓扑">拓扑</h3><p>忘掉具体的形状信息，研究其更本质的空间结构</p><h3 id="多面体">多面体</h3><p>由若干个多边形沿着边粘起来的一个封闭曲面所包围的立体</p><h3 id="欧拉定理（弱版本）">欧拉定理（弱版本）</h3><p>对于凸多面体$X$，记<br>$$\begin{aligned}<br>v&amp;=X的点数(vertex)\<br>e&amp;=X的棱数(edge)\<br>f&amp;=X的面数(face)<br>\end{aligned}$$<br>则有$v-e+f=2$</p><h3 id="亏格">亏格</h3><p>设多面体$X$的表面可连续的变化为<br>则该多面体称为$g$亏格曲面</p><h3 id="单连通">单连通</h3><p>设$X$是拓扑空间，如果$X$中任何一个点的回路都可以连续地收缩成这个点，那么就称$X$为单连通的。一个连通表面的亏格为0时，它才是单连通的</p><h3 id="欧拉定理">欧拉定理</h3><p>对于$g$亏格曲面$X$，有$v-e+f=2-2g$</p><h3 id="同胚">同胚</h3><p>存在连续双射$\varphi$使得$S\rightarrow E$, 且$\varphi$ 和$\varphi^{-1}$都连续，则称曲面$S$与曲面$E$同胚</p><p>拓扑学的终极目标是分类互不同胚的空间，其中一个重要的方法就是拓扑不变量，其形式可以是一个数，一个群，一个环等</p><h3 id="拓扑不变量">拓扑不变量</h3><p>设对$\forall$空间$X$定义了一个量$\mu(x)$，若$\mu$满足：$X$与$Y$同胚，则$\mu(x)=\mu(y)$，则称$\mu$为一个拓扑不变量<br>一般用法：<br>若$\mu$为一个拓扑不变量，$X, Y$满足$\mu(X)\neq\mu(Y)$, 则$X$与$Y$不同胚<br>例子：$v-e+f$为欧拉示性数</p><h2 id="拓扑空间">拓扑空间</h2><h3 id="开集">开集</h3><p>定义$U\subset X$称为一个开集，若</p><ol><li>$U=\emptyset $</li><li>$U\neq \emptyset,\forall \delta&gt;0, 使得 p\in B(p,\delta)\subset U$</li></ol><h3 id="定义">定义</h3><p>设$X$为一个集合，F为X集中某些子集构成的集族，把F中的元素称为X中的开集，需要满足下列条件：</p><ol><li>$\emptyset\in F$, $X\in F$</li><li>$U, V$是开集，$U\cap V$开集。注：有限个开集的交为开集<br>$$<br>\begin{array}{l}<br>\forall p\in U\cap V\Rightarrow<br>\left{<br>\begin{array}{}<br>p\in U \Rightarrow \exists \delta_1&gt;0, p \in B(p, \delta_1)\subset U\<br>p\in V\Rightarrow \exists\delta_2&gt;0, p \in B(p, \delta_2) \subset V<br>\end{array}<br>\right.\<br>\begin{aligned}<br>&amp;取\delta=min{\delta_1, \delta_2}\<br>&amp;\Rightarrow B(p, \delta)\subset U, B(p, 1·\delta_2)\subset V\<br>&amp;\Rightarrow B(p, \delta)\subset U\cap V\<br>&amp;\Rightarrow U\cup V为开集<br>\end{aligned}<br>\end{array}<br>$$</li><li>$\cup_\alpha, \alpha\in I$都是开集，$\underset{\alpha\in I}{\bigcup} \cup_\alpha$为开集<br>$$<br>\begin{array}{l}<br>\begin{aligned}<br>&amp;\forall p\in \underset{\alpha \in I}{\bigcup}U_\alpha\<br>&amp;\Rightarrow\exists \alpha_0\in I, p\subset U_\alpha\<br>&amp;\Rightarrow \exists \delta &gt;0, 使得p\in B(p, \delta)\subset U_\alpha\subset\underset{\alpha \in I}{\bigcup}\<br>&amp;\Rightarrow \underset{\alpha in I}{\bigcup}U_\alpha 为开集<br>\end{aligned}<br>\end{array}<br>$$</li></ol><p>则称X为拓扑空间<br>有时候这样写：设X为拓扑空间$\Leftrightarrow X$为一个集合，且规定了$X$上的一个拓扑$\Leftrightarrow$指定了哪些集合为开集</p><h3 id="平凡拓扑">平凡拓扑</h3><p>设$X$为一个集合，$F={\emptyset, X}$</p><h3 id="离散拓扑">离散拓扑</h3><p>设$X$为一个集合，$F={U|U\subset X}$</p><p>同一个集合可以赋予不同的拓扑</p><h3 id="度量空间诱导的拓扑">度量空间诱导的拓扑</h3><p>设$X$为一个集合，$d$为$x, y$的距离。$d: X\times X\rightarrow\R, (x, y)\mapsto d(x, y)$，满足：</p><ol><li>$d(x, y)\geq0, “=”\Leftrightarrow “x=y”$</li><li>$d(x,y)=d(y,x)$</li><li>$d(x,y)\leq d(x,z)+d(z,y)$</li></ol><p>则$(X, d)$称为一个度量空间，诱导一个拓扑<br>$B(x_0, \delta)={x\in X|d(x, x_0)&lt;\delta}$</p><p>则${U|U\underset{open}{\subset}X}$定义了$X$上的一个拓扑（证明见定义），该拓扑称为$(X, d)所诱导的拓扑$</p><p>例子：$X=C[a, b]$, $X$是区间$[a, b]$上的连续函数全体，定义$d(f, g)=max|f(x)-g(x)|, x\in [a, b]$，则$(C[a, b], d)$为一个度量空间。<br>证明：</p><ol><li>$d(f,g)\geq 0,“=”\Leftrightarrow max|f(x)-g(x)|=0\Leftrightarrow f=g$</li><li>$d(f,g) = d(g,f)$</li><li>$d(f,g) \leq d(f,h)+d(h,g)$</li></ol><h3 id="子空间拓扑">子空间拓扑</h3><p>设$X$为一个拓扑空间，$Y\subset X$<br>$F:={U\cap Y|U\underset{open}{\subset}X}$<br>则$F$给出了$Y$上的一个拓扑，称为$X$在$Y$上诱导的拓扑，又称为子空间拓扑，$(Y, F)$称为$X$的拓扑子空间</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>几何优化培训</title>
      <link href="/2022/09/19/study-route/"/>
      <url>/2022/09/19/study-route/</url>
      
        <content type="html"><![CDATA[<h2 id="学习过程">学习过程</h2><p>第一步：看完书籍 <a href="https://zhelper.ga/book/537922/ffe8ac">Optimization Algorithms on Matrix Manifolds</a><br>第二步：跑通<a href="https://www.manopt.org/">manopt</a>的matlab和pytorch代码，点击<a href="https://www.manopt.org/tutorial.html">教程</a>开始学习<br>第三步：跑通orthogonal CNN, unitary RNN等代码<br>第四步：了解manifold opti在深度学习的应用现状</p><h2 id="理论部分">理论部分</h2><h3 id="资料">资料</h3><p><a href="https://pan.baidu.com/s/1ZKDTpYjgXvXeRsW7itDwDg">入门资料</a></p><h3 id="博客">博客</h3><p><a href="https://scholar.google.com.sg/citations?hl=en&amp;user=VYcCB1cAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Absil的博客</a><br><a href="https://zhuanlan.zhihu.com/p/153115983?utm_source=wechat_session&amp;utm_medium=social&amp;s_r=0">流形优化方法求解约束问题</a><br><a href="https://agustinus.kristia.de/techblog/2019/02/22/optimization-riemannian-manifolds/">Optimization and Gradient Descent on Riemannian Manifolds</a></p><h3 id="书籍">书籍</h3><p><a href="https://www.nicolasboumal.net/book/">An introduction to optimization on smooth manifolds</a></p><h3 id="会谈">会谈</h3><p><a href="https://simons.berkeley.edu/talks/tbd-337">An Introduction to Optimization on Smooth Manifolds</a></p><h3 id="视频">视频</h3><p><a href="https://www.youtube.com/watch?v=7NH_F49OI9o">Advanced Calculus: gradient on Riemannian manifold, metric construction</a><br><a href="https://www.youtube.com/watch?v=rtvQ6sgRT6k">Orthogonality-free Approaches for Optimization Problems on Stiefel Manifold</a><br><a href="https://pan.baidu.com/s/1Z9NHXNVlrobygPMOsp8JyQ?pwd=34xp">刘歆 几何优化</a> 提取码：34xp<br><a href="https://pan.baidu.com/s/1nK_bq_3hvNtlxa-EdTv4OA?pwd=fju7">黄文 几何优化</a> 提取码：fju7<br><a href="https://www.youtube.com/watch?v=2mnqWES2roQ&amp;t=1811s">Shape Analysis</a><br><a href="https://www.youtube.com/watch?v=c80b54bCAeY">Optimisation of Manifolds</a><br><a href="https://www.youtube.com/watch?v=tJHC2FVfhjA&amp;list=PL_P2yZtrrzqdqA6YPLuBb5PykRfR5ym3L">Tangent spaces and Riemannian manifolds</a><br><a href="https://www.youtube.com/watch?v=6S4ZASvCqAI">First-order Methods for Geodesically Convex Optimization</a></p><h3 id="软件">软件</h3><p>Geomstats: A Python Package for Riemannian Geometry in Machine Learning |SciPy 2020|<br><a href="https://www.youtube.com/watch?v=Ju-Wsd84uG0">Miolane</a><br>PyTorch is available at <a href="https://github.com/mctorch">https://github.com/mctorch</a></p><h2 id="代码部分">代码部分</h2><h3 id="seedbank-aihub"><a href="https://research.google.com/seedbank/seeds">seedbank</a>&amp;<a href="https://aihub.cloud.google.com/s?category=notebook">aihub</a></h3><p>全部例程(167个，每周10个), 详细标注至少20个程序是什么意思</p><h3 id="sklearn"><a href="https://scikit-learn.org/stable/inde">sklearn</a></h3><p><a href="https://scikit-learn.org/stable/user_guide.html">https://scikit-learn.org/stable/user_guide.html</a><br><a href="https://scikit-learn.org/stable/modules/classes.html">https://scikit-learn.org/stable/modules/classes.html</a></p><h3 id="Pytorch-tutorial"><a href="https://pytorch.org/tutorials/">Pytorch tutorial</a></h3><h3 id="Tensorflow-tutori"><a href="https://www.tensorflow.org/tutorials">Tensorflow tutori</a></h3><h3 id="keras实例"><a href="https://keras.io/">keras实例</a></h3><h3 id="colab例程">colab例程</h3><p><a href="https://drive.google.com/drive/folders/1I6oWNkDmLZp9xh5Zlk-InHMeUcfKQVi5?usp=sharing">https://drive.google.com/drive/folders/1I6oWNkDmLZp9xh5Zlk-InHMeUcfKQVi5?usp=sharing</a><br><a href="https://drive.google.com/file/d/1ATl9343JLk3ioaf6zXtQIQKbwKmW07Pi/view?usp=sharing">https://drive.google.com/file/d/1ATl9343JLk3ioaf6zXtQIQKbwKmW07Pi/view?usp=sharing</a><br><a href="https://colab.research.google.com/drive/15o9VaY-0RbdY3W5TP4tTU8OUttPg4BIm">https://colab.research.google.com/drive/15o9VaY-0RbdY3W5TP4tTU8OUttPg4BIm</a><br><a href="https://colab.research.google.com/drive/1TAmsmrk1wDvuVQtu1o9qizCeKUET20re">https://colab.research.google.com/drive/1TAmsmrk1wDvuVQtu1o9qizCeKUET20re</a><br><a href="https://colab.research.google.com/drive/1Psmw8tPNtGaicZzXAQ7Co5quOZFwKAKd">https://colab.research.google.com/drive/1Psmw8tPNtGaicZzXAQ7Co5quOZFwKAKd</a><br><a href="https://colab.research.google.com/drive/1wcSh35l-eu9JzXU02fvIhqI1UkaXz5TM#scrollTo=yPXuWTISTqAz">https://colab.research.google.com/drive/1wcSh35l-eu9JzXU02fvIhqI1UkaXz5TM#scrollTo=yPXuWTISTqAz</a></p><h3 id="攻防例程">攻防例程</h3><p><a href="https://drive.google.com/drive/folders/1WMNzG8jK9m6is0RVND_0wLMaYW4ttca0?usp=sharing">https://drive.google.com/drive/folders/1WMNzG8jK9m6is0RVND_0wLMaYW4ttca0?usp=sharing</a><br><a href="https://drive.google.com/drive/folders/15vNQMCkKL3rn9CBzrsANxDLpBWto_btj?usp=sharing">https://drive.google.com/drive/folders/15vNQMCkKL3rn9CBzrsANxDLpBWto_btj?usp=sharing</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>常微分方程求解</title>
      <link href="/2022/09/19/NumericalAnalysis/"/>
      <url>/2022/09/19/NumericalAnalysis/</url>
      
        <content type="html"><![CDATA[<p>令常微分方程<br>$$<br>\left{\begin{array}{l}<br>y^{\prime}=f(x, y)  \<br>y_0=y(x_0)<br>\end{array}<br>\right.<br>$$</p><h2 id="欧拉法">欧拉法</h2><p>用当前梯度作为函数的变化率进行迭代, 步长$h=x_{n+1}-x_n$, 精度不高<br><a href="./NumericalAnalysis%5Cadvance_Eular.jpg"><img src="./NumericalAnalysis%5Cadvance_Eular.jpg" alt="Eular"></a><br>$$y_{n+1}=y_n+hf(x_n, y_n)$$</p><h2 id="梯形法">梯形法</h2><p>用当前迭代点跟下一迭代点的平均值作为一次迭代的变化率<br>$$y_{n+1}=y_n+\frac h2(f(x_n, y_n)+f(x_{n+1}, y_{n+1}))$$</p><h2 id="改进欧拉法">改进欧拉法</h2><p>在梯形法中, $y_{n+1}$是未知的，无法直接求出来k, 因此可以先利用欧拉公式求出<br>$$\bar{y}<em>{n+1}=y_n+hf(x_n, y_n)$$<br>再带入梯形法中，得到$y</em>{n+1}=y_n+\frac h2(f(x_n, y_n)+f(x_{n+1}, \bar{y}_{n+1}))$<br><a href="./NumericalAnalysis/exEular.jpg"><img src="./NumericalAnalysis/exEular.jpg" alt="exEular"></a></p><h2 id="龙格库塔法">龙格库塔法</h2><p>根据拉格朗日中值定理，$\frac{y_{n+1}-y_n}h=y’(x_h+\theta h)$, 其中$0&lt;\theta&lt;1$，则可以得到$y_{n+1}$的精确解$y_{n+1}=y_n+hf(x_n+\theta h, y(x_n+\theta h))$, 之前的欧拉法梯形法都可以表示为这样的形式，只是$\theta$的取值不同<br>令平均斜率$\bar{K}=f(x_n+\theta h, y(x_n+\theta h))$, 在一次迭代中取$n$个点为$n$阶龙格库塔方法<br><a href="./NumericalAnalysis/RK.png"><img src="./NumericalAnalysis/RK.png" alt="RK"></a><br>这里以四阶龙格库塔法举例<br>$$<br>RK4=<br>\left{<br>\begin{array}{l}<br>K_1=f(x_n, y_n) \<br>K_2=f(x_{n+\frac12}, y_n+\frac h2K_1) \<br>K_3=f(x_{n+\frac12}, y_n+\frac h2K_2) \<br>K_4=f(x_{n+1}, y_n+hK_3) \<br>y_{n+1}=y_n+\frac h6 (K_1+2K_2+2K_3+K_4) \<br>\end{array} \<br>\right.<br>$$<br>其中<br>$$\begin{cases}<br>x_{n+1}=x_n+h \<br>x_{n+\frac12}=x_n+\frac<br>12h \<br>y’=f(x_n, y_n)<br>\end{cases} \<br>$$</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>赤之篇章</title>
      <link href="/2022/07/01/RedChapter/"/>
      <url>/2022/07/01/RedChapter/</url>
      
        <content type="html"><![CDATA[<h2 id="零散">零散</h2><p>赤: 即便我就这样死去也无所谓，但我的这条性命是粉她牺牲自己也要保下的，所以无论如何我都会往前走下去！</p><h2 id="石英联盟">石英联盟</h2><p>粉：喂！前面那位训练师！<br>赤：呃？！！<br>粉：我也来自真新镇，我叫樱粉红<br>赤：我叫赤红，让我们开始吧<br>粉：啊？什么？<br>赤：当然是训练师对战，这应该是最后一场吧</p><p>粉：啊呜呜…<br>赤：啊，抱歉，我并没有想惹哭你<br>粉：我被正棕和阿辉一路保护才能走到最后，结果这么轻易就被打败了吗<br>赤：怎么会，我也是打起十二分精神才能侥幸取胜的，正棕和阿辉是洞窟里那两个家伙吧<br>粉：是的，他们提前被打败了<br>赤：其实是跟我交手的啦，你们都是实力非常强劲的对手，你能走到这里不是因为他们两人的保护，而是你们三人一起努力的结果！<br>粉：噗，真的吗<br>赤：（她好像相信了…）<br>赤：当然啦！我还会骗你不成，我可是即将登上冠军宝座的人，冠军是不会说谎的！<br>粉：原来你都快获得冠军了吗！加油！小赤！你一定可以的！<br>赤：（她真的相信了…）<br>赤：放心吧，我一定会的<br>粉：等你好消息！再见！<br>赤：再见！</p><h2 id=""></h2>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
